[{"authors":null,"categories":["Game Networking"],"content":"Hi, I\u0026rsquo;m Glenn Fiedler and I\u0026rsquo;m the founder and CEO of Network Next.\nBefore starting Network Next, I worked in the game industry for 20 years as a software engineer. I was fortunate enough to have a good career and got to work on some games you\u0026rsquo;ve might have played: Freedom Force, Mercenaries 2, God of War, Journey and Titanfall 1 and 2. Some of my netcode is still active in Apex Legends even though I left Respawn Entertainment before it started development.\nEver since QTest I\u0026rsquo;ve found the idea that people in different physical locations could inhabit the same virtual space utterly fascinating. I wanted to understand how that worked and be part of it. So, after a few false starts in graphics programming and physics, I specialized in UDP protocol design for latency sensitive games.\nI\u0026rsquo;m writing this article to tell you something that you might find shocking.\nNetwork Neutrality, the thing that we hold so dear, the foundation of the internet as we know it, may in fact be harmful for latency sensitive applications like games.\nWhy? Please watch this video for an explanation (sound on):\nNow of course, saying the internet doesn\u0026rsquo;t care about your game is a pretty strong accusation, so I\u0026rsquo;m going to back it up with some evidence.\nFirst, anecdotal. Every multiplayer game I worked on, I spent years of my life working on the netcode, using every trick possible to hide packet loss and latency. Client side prediction to hide latency in player actions, lag compensation to avoid players needing to lead shots, bandwidth optimization to reduce the load on the network, redundancy to mitigate packet loss, custom UDP protocols to avoid head of line blocking\u0026hellip;\nAnd despite all this effort, a significant portion of players would play my game and get bad network performance and there was literally nothing I could do about it.\nJust how bad does it get? Let\u0026rsquo;s take a look at the data\u0026hellip;\nHere\u0026rsquo;s a screenshot from Thursday August 20th, 2020:\n1000+ players with 100ms or greater added latency courtesy of best effort delivery, one player with a whooping 730ms extra. Clearly, this is not a speed of light problem\u0026hellip;\nLater the same day we saw a packet loss event:\nNow you may look at this and think, oh, it\u0026rsquo;s just 0.8% packet loss at peak, it\u0026rsquo;s not that bad\u0026hellip; but this is an average across all players, and only some portion of players were affected. For these players, the effect was much more severe:\nDuring the mitigation the average packet loss reduction was an absolute reduction of 12 points of packet loss. No, not a reduction of 12%, a reduction of 12 points. In other words, a conversion from totally unplayable to playable.\nThis is not an isolated incident. These things literally happen all the time. Take a look for yourself, our portal is live and updated in real-time: https://portal.networknext.com\nFrom this point on it should be clear: the internet really doesn\u0026rsquo;t care about your game.\nSo\u0026hellip; what\u0026rsquo;s going on?\nIt\u0026rsquo;s this. The internet makes no guarantee of performance, but instead offers best effort delivery. By the orthodoxy of Network Neutrality it is hand-wavingly assumed that overall, the quality level is maintained via over provisioning. But while this may be true for browsing the web and reading email, looking at the results above we can clearly see it isn\u0026rsquo;t working for latency sensitive applications like games!\nAs a game developer what I want for my traffic is the lowest latency (within speed of light limits), with as little packet loss and jitter as possible. In other words, if I send 60 packets per-second, I want all of them to arrive as quickly as possible exactly 1/60th of a second apart. The internet as it exists today, is just not capable of this.\nSo now let\u0026rsquo;s take direct aim at one of the core tenets of Network Neutrality. That all traffic is the same. Clearly this is false. Latency sensitive traffic like game traffic is not the same as checking your email or browsing the web. It\u0026rsquo;s not even the same as watching YouTube or Netflix, which can be solved by simply buffering the stream. It\u0026rsquo;s something completely different.\nNow that we acknowledge that different classes of traffic exist, how can we reconcile this with a neutral network and avoid classically described dystopias where network providers throttle the performance of competing applications, and ISPs bill you for \u0026ldquo;acceleration plans\u0026rdquo; for your internet connection on a monthly basis.\nThis is why I created Network Next.\nNetwork Next is not just another network. We\u0026rsquo;re not building network infrastructure. We\u0026rsquo;re not lighting up dark fiber. We\u0026rsquo;re not creating yet another shadow internet with private interconnects. We don\u0026rsquo;t even have an ASN.\nInstead, we\u0026rsquo;re creating a neutral marketplace where networks compete to carry latency sensitive traffic. In this marketplace, networks cannot identify the application or even set a different price for different applications. They can only compete on performance and price.\nThe buyer on our marketplace is not the player, it\u0026rsquo;s the application developer who uses Network Next to communicate the quality of service they want to the network. Network Next then runs a bid on our marketplace every 10 seconds per-player and the result of this bid is the route players take across our supplier networks.\nThis creates a truly neutral network of networks - a new internet - with different classes of transit. An ethical, and technologically enforced alternative to the Network Neutrality Orthodoxy that covers its ears and yells \u0026ldquo;All traffic is the same!\u0026rdquo; in 2020 even though clearly it is not.\nI hope you agree with me, but if even if you don\u0026rsquo;t, we\u0026rsquo;ve created this marketplace over the past three years and it\u0026rsquo;s now live. Chances are pretty good over the next 3 months when you play a game, you\u0026rsquo;re playing it over Network Next and I look forward to improving the quality of your connection.\nBest wishes,\nGlenn Fiedler, CEO, Network Next networknext.com\n","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598227200,"objectID":"fd16efd31e7a145f67ffe1ebc7727c61","permalink":"https://gafferongames.com/post/network_neutrality_considered_harmful/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/post/network_neutrality_considered_harmful/","section":"post","summary":"Why I created Network Next","tags":["networking"],"title":"Network Neutrality Considered Harmful","type":"post"},{"authors":null,"categories":["Game Networking"],"content":"Hi, I\u0026rsquo;m Glenn Fiedler and this is my GDC 2019 talk called “Fixing the Internet for Games”.\nIt\u0026rsquo;s about what we are doing at my new startup Network Next.\nWhen you launch a multiplayer game, some percentage of your player base will complain they are getting a bad experience.\nYou only need to check your forums to see this is true.\nAnd as a player you’ve probably experienced it too.\nWhat’s going on?\nIs it your netcode, or maybe your matchmaker or hosting provider?\nCan you fix it by running more servers in additional locations, or by switching to another hosting company?\nOr maybe you’ve done all this already and now you have too many data centers, causing fragmentation in your player base?\nIt turns out that you can do all these things perfectly yet some % of your player base will still complain.\nThe real problem is that you don’t control the route from your player to your game server, and sometimes this route is bad.\nThis happens because the internet is not optimized for what we want (lowest latency, jitter and packet loss)\nNo amount of good netcode that you write can compensate for this.\nThe problem is the internet itself.\nThe internet doesn’t care about your game.\nThe internet thinks game traffic is the same as checking emails, visiting a website or watching netflix.\nBut game traffic is real-time and latency sensitive. It’s not the same.\nIt’s interactive so it can’t be cached at the edge and buffered like streamed video.\nNetworks that participate in the internet do hot potato routing, they just try to get your packets off their network as fast as possible so they don’t have to deal with it anymore. Nobody is coordinating centrally to ensure that packets are delivered with the lowest overall latency, jitter and packet loss.\nSometimes ISPs or transit providers make mistakes and packets are sent on ridiculous routes that can go to the other side of the country and back on their way to a game server just 5 miles away from the player… you can call up the ISP and ask them to fix this, but it can take days to resolve.\nEven from day to day, performance is not consistent. You can get a good route one day, and a terrible one the next.\nFor all of these problems, players tend to blame you, the developer. But it’s not actually your fault.\nWhat can you, the game developer, do about this?\nOne common approach is to try running as many servers in as many locations as possible, with as many different providers as you can.\nThis seems like a good idea at first, but there is no one data center or hosting company that is perfectly peered with every player of your game, so ultimately, it does not solve the problem.\nFlaws:\nPlayer fragmentation Logistics of so many suppliers Really difficult to find one data center suitable for party or team that wants to play together Another option is to host in public clouds. Google’s private network is pretty good, right?\nGame developers tend to assume that AWS, Azure and Google peering is perfect. But this is not true.\nFlaws:\nEgress bandwidth is expensive Locked into one provider Transit is not as good as you think You could also build your own internet for your game.\nThis is not a joke. It actually happened!\nRiot built their own private internet for League of Legends. When you play league of legends, your game traffic goes directly from your ISP onto this private network.\nCase study: Riot Direct\nFixing the internet for real-time applications (part 1) Fixing the internet for real-time applications (part 2) Fixing the internet for real-time applications (part 3) Flaws:\nCan you really afford this? How many internets do we really need?! :) Right now, if you are a game developer shipping a multiplayer game, you are competing against companies that have built their own private internet for their game.\nWhat’s the solution? Build your own too?\nDoes it really make sense to build an internet for each game? This is crazy\u0026hellip;\nThere has to be a way to do this without building your own infrastructure.\nNetwork Next was created to solve this problem.\nNetwork Next steers your game’s traffic across private networks that have already been built, so you don’t have to build your own private internet for your game.\nBut hold on. Aren\u0026rsquo;t all the problems at the edge?\nPeople seem to think that all the bad stuff on the internet occurs at the edge of the network, eg. shitty DSL connections, oversubscribed cable networks…\nBut this is not true.\nThe backbone itself is not as good as you think it is.\nAnd I’m going to prove it.\nJust two regular computers sitting in different data centers\u0026hellip; let’s send UDP ping and pong packets between them, so we can measure the quality of the network.\nWe need some way to measure this quality as a scalar value.\nDefine cost as the sum of round trip time (rtt) in milliseconds, jitter (3rd standard deviation), and packet loss %.\nLower cost is good. Higher cost is bad.\nNow let’s generalize to 4 nodes.\nWe measure cost between all nodes via pings, O(n^2).\nNow let’s go up to 10 nodes.\nStore the cost between all nodes in a triangular matrix.\nEach entry in the matrix is the cost between the node with index corresponding to the column, and node with index corresponding to the row.\nThe diagonal is -1, because nodes don’t ping themselves.\nNow spin up instances in all the providers you can think of and all locations they support in North America.\nFor example: Google, AWS, Azure, Bluemix, vultr.com, multiplay, i3d, gameservers.com, servers.com and so on.\nIf the internet backbone was perfectly efficient, A-B would be the lowest cost 100% of the time.\nInstead, for the worst provider it is only 5-10% of the time.\nAnd the best performing provider only 30% of the time\u0026hellip;\nOf course, we are not outperforming each provider on their own internal network, those are efficient.\nInstead we reveal that each node on the internet is not perfectly peered with every other node. There is some slack, and going through an intermediary node in the majority cases can fix this.\nMachines on the backbone are not talking to each other as efficiently as they can.\nTalking through an intermediary is often better, in terms of our cost function.\nWhy? Many reasons\u0026hellip; but overall, the public internet is optimized for throughput at lowest cost, not lowest latency and jitter.\nWhat other option is there, aside from the public internet?\nMany private networks have been built.\nThese include CDNs and any corporate entity that has realized the public internet is broken, and have built their own private networks (backhaul) and interconnections to compensate.\n(Not many people know this, but this “shadow” private internet is actually growing at a faster rate than the public internet\u0026hellip;)\nThis private internet is currently closed. Your game packets do not traverse it.\nHow can we open it up?\nWith a marketplace. Network Next is a marketplace where private networks resell excess capacity to applications that want better transit than the public internet.\nEvery 10 seconds, per-player we run a \u0026ldquo;route shader\u0026rdquo; as a bid on our marketplace, and find the best route across multiple suppliers that satisfies this request.\nSuppliers cannot identify buyers, and can only compete on performance and price.\nThus, Network Next discovers the market price for premium transit, while remaining neutral.\nNow let’s see how it works in practice, with real players.\nHere is a small sample of connections active at a specific time one night last week\u0026hellip;\nEach dot is a player. Green dots are taking Network Next, blue dots are taking the public internet, because Network Next does not provide any improvement for them (yet).\nAround 60% of player sessions are improved, fluctuating between 50% and 60% depending on the time of day. We believe that as we ramp up more suppliers, we can get the percentage of players improved up to 90%.\nOf the 60% that are currently improved, the improvement breaks down into the following buckets:\n49% of sessions had 0-5 cost unit improvement (cost being latency+jitter+packet loss). 25% had 5-10cu improvement. 12% had 10-15cu improvement. 6% had 15-20cu improvement. 7% had greater than 20cu improvement\u0026hellip;\nHere is a look at live sessions at a random time that night. Look at the rightmost column. Some players are getting an incredible improvement\u0026hellip; there are always some players getting improvements like this at all times of the day\u0026hellip;\n(My apologies for the black censoring, it is necessary for GDPR compliance).\nDrilling into the session getting the most improvement, we can see that they are not in the middle of nowhere, they are in Monterey, California\u0026hellip;\nHere we can see that the improvement is in both latency and jitter. Notice how flat the blue latency line is (Network Next), vs. the red line (public internet) that is going all over the place.\nNetwork Next not only has lower latency, it is also more consistent.\nHere is a look at different data centers where our customers run game servers.\nOn the right is the % of players taking Network Next (getting improvement) to servers in that data center (green), and those not getting improvement and going direct over the public internet (blue).\nOn the left is the distribution of cost unit improvements for players that are getting improvement on Network Next to servers in that data center. The improvement depends on the peering arrangements of that data center, and on internet weather. It fluctuates somewhat from day to day.\nWe are able to fix this fluctuation due to internet weather and get the best result at all times.\nIf you\u0026rsquo;d like to learn more, please visit us at networknext.com\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1553385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385600,"objectID":"49ef240e579f84b5260bf278064d622c","permalink":"https://gafferongames.com/post/fixing_the_internet_for_games/","publishdate":"2019-03-24T00:00:00Z","relpermalink":"/post/fixing_the_internet_for_games/","section":"post","summary":"What we're doing at my new startup [Network Next](https://networknext.com) :rocket:","tags":["networking"],"title":"Fixing the Internet for Games","type":"post"},{"authors":null,"categories":["Networked Physics"],"content":"Introduction About a year ago, Oculus approached me and offered to sponsor my research. They asked me, effectively: \u0026ldquo;Hey Glenn, there\u0026rsquo;s a lot of interest in networked physics in VR. You did a cool talk at GDC. Do you think could come up with a networked physics sample in VR that we could share with devs? Maybe you could use the touch controllers?\u0026rdquo;\nI replied \u0026ldquo;F*** yes!\u0026quot; cough \u0026ldquo;Sure. This could be a lot of fun!\u0026rdquo;. But to keep it real, I insisted on two conditions. One: the source code I developed would be published under a permissive open source licence (for example, BSD) so it would create the most good. Two: when I was finished, I would be able to write an article describing the steps I took to develop the sample.\nOculus agreed. Welcome to that article! Also, the source for the networked physics sample is here, wherein the code that I wrote is released under a BSD licence. I hope the next generation of programmers can learn from my research into networked physics and create some really cool things. Good luck!\nWhat are we building? When I first started discussions with Oculus, we imagined creating something like a table where four players could sit around and interact with physically simulated cubes on the table. For example, throwing, catching and stacking cubes, maybe knocking each other\u0026rsquo;s stacks over with a swipe of their hand.\nBut after a few days spent learning Unity and C#, I found myself actually inside the Rift. In VR, scale is so important. When the cubes were small, everything felt much less interesting, but when the cubes were scaled up to around a meter cubed, everything had this really cool sense of scale. You could make these huge stacks of cubes, up to 20 or 30 meters high. This felt really cool!\nIt\u0026rsquo;s impossible to communicate visually what this feels like outside of VR, but it looks something like this\u0026hellip;\n\u0026hellip; where you can select, grab and throw cubes using the touch controller, and any cubes you release from your hand interact with the other cubes in the simulation. You can throw a cube at a stack of cubes and knock them over. You can pick up a cube in each hand and juggle them. You can build a stack of cubes and see how high you can make it go.\nEven though this was a lot of fun, it\u0026rsquo;s not all rainbows and unicorns. Working with Oculus as a client, I had to define tasks and deliverables before I could actually start the work.\nI suggested the following criteria we would use to define success:\nPlayers should be able to pick up, throw and catch cubes without latency.\nPlayers should be able to stack cubes, and these stacks should be stable (come to rest) and be without visible jitter.\nWhen cubes thrown by any player interact with the simulation, wherever possible, these interactions should be without latency.\nAt the same time I created a set of tasks to work in order of greatest risk to least, since this was R\u0026amp;D, there was no guarantee we would actually succeed at what we were trying to do.\nNetwork Models First up, we had to pick a network model. A network model is basically a strategy, exactly how we are going to hide latency and keep the simulation in sync.\nThere are three main network models to choose from:\nDeterministic lockstep Client/server with client-side prediction Distributed simulation with authority scheme I was instantly confident of the correct network model: a distributed simulation model where players take over authority of cubes they interact with. But let me share with you my reasoning behind this.\nFirst, I could trivially rule out a deterministic lockstep network model, since the physics engine inside Unity (PhysX) is not deterministic. Furthermore, even if PhysX was deterministic I could still rule it out because of the requirement that player interactions with the simulation be without latency.\nThe reason for this is that to hide latency with deterministic lockstep I needed to maintain two copies of the simulation and predict the authoritative simulation ahead with local inputs prior to render (GGPO style). At 90HZ simulation rate and with up to 250ms of latency to hide, this meant 25 physics simulation steps for each visual render frame. 25X cost is simply not realistic for a CPU intensive physics simulation.\nThis leaves two options: a client/server network model with client-side prediction (perhaps with dedicated server) and a less secure distributed simulation network model.\nSince this was a non-competitive sample, there was little justification to incur the cost of running dedicated servers. Therefore, whether I implemented a client/server model with client-side prediction or distributed simulation model, the security would be effectively the same. The only difference would be if only one of the players in the game could theoretically cheat, or all of them could.\nFor this reason, a distributed simulation model made the most sense. It had effectively the same amount of security, and would not require any expensive rollback and resimulation, since players simply take authority over cubes they interact with and send the state for those cubes to other players.\nAuthority Scheme While it makes intuitive sense that taking authority (acting like the server) for objects you interact can hide latency \u0026ndash; since, well if you\u0026rsquo;re the server, you don\u0026rsquo;t experience any lag, right? \u0026ndash; what\u0026rsquo;s not immediately obvious is how to resolve conflicts.\nWhat if two players interact with the same stack? What if two players, masked by latency, grab the same cube? In the case of conflict: who wins, who gets corrected, and how is this decided?\nMy intuition at this point was that because I would be exchanging state for objects rapidly (up to 60 times per-second), that it would be best to implement this as an encoding in the state exchanged between players over my network protocol, rather than as events.\nI thought about this for a while and came up with two key concepts:\nAuthority Ownership Each cube would have authority, either set to default (white), or to whatever color of the player that last interacted with it. If another player interacted with an object, authority would switch and update to that player. I planned to use authority for interactions of thrown objects with the scene. I imagined that a cube thrown by player 2 could take authority over any objects it interacted with, and in turn any objects those objects interacted with, recursively.\nOwnership was a bit different. Once a cube is owned by a player, no other player could take ownership until that player reliquished ownership. I planned to use ownership for players grabbing cubes, because I didn\u0026rsquo;t want to make it possible for players to grab cubes out of other player\u0026rsquo;s hands after they picked them up.\nI had an intuition that I could represent and communicate authority and ownership as state by including two different sequence numbers per-cube as I sent them: an authority sequence, and an ownership sequence number. This intuition ultimately proved correct, but turned out to be much more complicated in implementation than I expected. More on this later.\nState Synchronization Trusting I could implement the authority rules described above, my first task was to prove that synchronizing physics in one direction of flow could actually work with Unity and PhysX. In previous work I had networked simulations built with ODE, so really, I had no idea if it was really possible.\nTo find out, I setup a loopback scene in Unity where cubes fall into a pile in front of the player. There are two sets of cubes. The cubes on the left represent the authority side. The cubes on the right represent the non-authority side, which we want to be in sync with the cubes on the left.\nAt the start, without anything in place to keep the cubes in sync, even though both sets of cubes start from the same initial state, they give slightly different end results. You can see this most easily from top-down:\nThis happens because PhysX is non-deterministic. Rather than tilting at non-determinstic windmills, I fight non-determinism by grabbing state from the left side (authority) and applying it to the right side (non-authority) 10 times per-second:\nThe state I grab from each cube looks like this:\nstruct CubeState { Vector3 position; Quaternion rotation; Vector3 linear_velocity; Vector3 angular_velocity; }; And when I apply this state to the simulation on the right side, I simply snap the position, rotation, linear and angular velocity of each cube to the state captured from the left side.\nThis simple change is enough to keep the left and right simulations in sync. PhysX doesn\u0026rsquo;t even diverge enough in the 1/10th of a second between updates to show any noticeable pops.\nThis proves that a state synchronization based approach for networking can work with PhysX. (Sigh of relief). The only problem of course, is that sending uncompressed physics state uses way too much bandwidth\u0026hellip;\nBandwidth Optimization To make sure the networked physics sample is playable over the internet, I needed to get bandwidth under control.\nThe easiest gain I found was to simply encode the state for at rest cubes more efficiently. For example, instead of repeatedly sending (0,0,0) for linear velocity and (0,0,0) for angular velocity for at rest cubes, I send just one bit:\n[position] (vector3) [rotation] (quaternion) [at rest] (bool) \u0026lt;if not at rest\u0026gt; { [linear_velocity] (vector3) [angular_velocity] (vector3) } This is lossless technique because it doesn\u0026rsquo;t change the state sent over the network in any way. It\u0026rsquo;s also extremely effective, since statistically speaking, most of the time the majority of cubes are at rest.\nTo optimize bandwidth further we need to use lossy techniques. For example, we can reduce the precision of the physics state sent over the network by bounding position in some min/max range and quantizing it to a resolution of 1/1000th of a centimeter and sending that quantized position as an integer value in some known range. The same basic approach can be used for linear and angular velocity. For rotation I used the smallest three representation of a quaternion.\nBut while this saves bandwidth, it also adds risk. My concern was that if we are networking a stack of cubes (for example, 10 or 20 cubes placed on top of each other), maybe the quantization would create errors that add jitter to that stack. Perhaps it would even cause the stack to become unstable, but in a particularly annoying and hard to debug way, where the stack looks fine for you, and is only unstable in the remote view (eg. the non-authority simulation), where another player is watching what you do.\nThe best solution to this problem that I found was to quantize the state on both sides. This means that before each physics simulation step, I capture and quantize the physics state exactly the same way as when it\u0026rsquo;s sent over the network, then I apply this quantized state back to the local simulation.\nNow the extrapolation from quantized state on the non-authority side exactly matches the authority simulation, minimizing jitter in large stacks. At least, in theory.\nComing To America Rest But quantizing the physics state created some very interesting side-effects!\nPhysX doesn\u0026rsquo;t really like you forcing the state of each rigid body at the start of every frame and makes sure you know by taking up a bunch of CPU.\nQuantization adds error to position which PhysX tries very hard to correct, snapping cubes immediately out of penetration with huge pops!\nRotations can\u0026rsquo;t be represented exactly either, again causing penetration. Interestingly in this case, cubes can get stuck in a feedback loop where they slide across the floor!\nAlthough cubes in large stacks seem to be at rest, close inspection in the editor reveals that they are actually jittering by tiny amounts, as cubes are quantized just above surface and falling towards it.\nThere\u0026rsquo;s not much I could do about the PhysX CPU usage, but the solution I found for the depenetration was to set maxDepenetrationVelocity on each rigid body, limiting the velocity that cubes are pushed apart with. I found that one meter per-second works very well.\nGetting cubes to come to rest reliably was much harder. The solution I found was to disable the PhysX at rest calculation entirely and replace it with a ring-buffer of positions and rotations per-cube. If a cube has not moved or rotated significantly in the last 16 frames, I force it to rest. Boom. Perfectly stable stacks with quantization.\nNow this might seem like a hack, but short of actually getting in the PhysX source code and rewriting the PhysX solver and at rest calculations, which I\u0026rsquo;m certainly not qualified to do, I didn\u0026rsquo;t see any other option. I\u0026rsquo;m happy to be proven wrong though, so if you find a better way to do this, please let me know :)\nPriority Accumulator The next big bandwidth optimization I did was to send only a subset of cubes in each packet. This gave me fine control over the amount of bandwidth sent, by setting a maximum packet size and sending only the set of updates that fit in each packet.\nHere\u0026rsquo;s how it works in practice:\nEach cube has a priority factor which is calculated each frame. Higher values are more likely to be sent. Negative values mean \u0026ldquo;don\u0026rsquo;t send this cube\u0026rdquo;.\nIf the priority factor is positive, it\u0026rsquo;s added to the priority accumulator value for that cube. This value persists between simulation updates such that the priority accumulator increases each frame, so cubes with higher priority rise faster than cubes with low priority.\nNegative priority factors clear the priority accumulator to -1.0.\nWhen a packet is sent, cubes are sorted in order of highest priority accumulator to lowest. The first n cubes become the set of cubes to potentially include in the packet. Objects with negative priority accumulator values are excluded.\nThe packet is written and cubes are serialized to the packet in order of importance. Not all state updates will necessarily fit in the packet, since cube updates have a variable encoding depending on their current state (at rest vs. not at rest and so on). Therefore, packet serialization returns a flag per-cube indicating whether it was included in the packet.\nPriority accumulator values for cubes sent in the packet are cleared to 0.0, giving other cubes a fair chance to be included in the next packet.\nFor this demo I found some value in boosting priority for cubes recently involved in high energy collisions, since high energy collision was the largest source of divergence due to non-deterministic results. I also boosted priority for cubes recently thrown by players.\nSomewhat counter-intuitively, reducing priority for at rest cubes gave bad results. My theory is that since the simulation runs on both sides, at rest cubes would get slightly out of sync and not be corrected quickly enough, causing divergence when other cubes collided with them.\nDelta Compression Even with all the techniques so far, it still wasn\u0026rsquo;t optimized enough. With four players I really wanted to get the cost per-player down under 256kbps, so the entire simulation could fit into 1mbps for the host.\nI had one last trick remaining: delta compression.\nFirst person shooters often implement delta compression by compressing the entire state of the world relative to a previous state. In this technique, a previous complete world state or \u0026lsquo;snapshot\u0026rsquo; acts as the baseline, and a set of differences, or delta, between the baseline and the current snapshot is generated and sent down to the client.\nThis technique is (relatively) easy to implement because the state for all objects are included in each snapshot, thus all the server needs to do is track the most recent snapshot received by each client, and generate deltas from that snapshot to the current.\nHowever, when a priority accumulator is used, packets don\u0026rsquo;t contain updates for all objects and delta encoding becomes more complicated. Now the server (or authority-side) can\u0026rsquo;t simply encode cubes relative to a previous snapshot number. Instead, the baseline must be specified per-cube, so the receiver knows which state each cube is encoded relative to.\nThe supporting systems and data structures are also much more complicated:\nA reliability system is required that can report back to the sender which packets were received, not just the most recently received snapshot #.\nThe sender needs to track the states included in each packet sent, so it can map packet level acks to sent states and update the most recently acked state per-cube. The next time a cube is sent, its delta is encoded relative to this state as a baseline.\nThe receiver needs to store a ring-buffer of received states per-cube, so it can reconstruct the current cube state from a delta by looking up the baseline in this ring-buffer.\nBut ultimately, it\u0026rsquo;s worth the extra complexity, because this system combines the flexibility of being able to dynamically adjust bandwidth usage, with the orders of magnitude bandwidth improvement you get from delta encoding.\nDelta Encoding Now that I have the supporting structures in place, I actually have to encode the difference of a cube relative to a previous baseline state. How is this done?\nThe simplest way is to encode cubes that haven\u0026rsquo;t changed from the baseline value as just one bit: not changed. This is also the easiest gain you\u0026rsquo;ll ever see, because at any time most cubes are at rest, and therefore aren\u0026rsquo;t changing state.\nA more advanced strategy is to encode the difference between the current and baseline values, aiming to encode small differences with fewer bits. For example, delta position could be (-1,+2,+5) from baseline. I found this works well for linear values, but breaks down for deltas of the smallest three quaternion representation, as the largest component of a quaternion is often different between the baseline and current rotation.\nFurthermore, while encoding the difference gives some gains, it didn\u0026rsquo;t provide the order of magnitude improvement I was hoping for. In a desperate, last hope, I came up with a delta encoding strategy that included prediction. In this approach, I predict the current state from the baseline assuming the cube is moving ballistically under acceleration due to gravity.\nPrediction was complicated by the fact that the predictor must be written in fixed point, because floating point calculations are not necessarily guaranteed to be deterministic. But after a few days of tweaking and experimentation, I was able to write a ballistic predictor for position, linear and angular velocity that matched the PhysX integrator within quantize resolution about 90% of the time.\nThese lucky cubes get encoded with another bit: perfect prediction, leading to another order of magnitude improvement. For cases where the prediction doesn\u0026rsquo;t match exactly, I encoded small error offset relative to the prediction.\nIn the time I had to spend, I not able to get a good predictor for rotation. I blame this on the smallest three representation, which is highly numerically unstable, especially in fixed point. In the future, I would not use the smallest three representation for quantized rotations.\nIt was also painfully obvious while encoding differences and error offsets that using a bitpacker was not the best way to read and write these quantities. I\u0026rsquo;m certain that something like a range coder or arithmetic compressor that can represent fractional bits, and dynamically adjust its model to the differences would give much better results, but I was already within my bandwidth budget at this point and couldn\u0026rsquo;t justify any further noodling :)\nSynchronizing Avatars After several months of work, I had made the following progress:\nProof that state synchronization works with Unity and PhysX Stable stacks in the remote view while quantizing state on both sides Bandwidth reduced to the point where all four players can fit in 1mbps The next thing I needed to implement was interaction with the simulation via the touch controllers. This part was a lot of fun, and was my favorite part of the project :)\nI hope you enjoy these interactions. There was a lot of experimentation and tuning to make simple things like picking up, throwing, passing from hand to hand feel good, even crazy adjustments to ensure throwing worked great, while placing objects on top of high stacks could still be done with high accuracy.\nBut when it comes to networking, in this case the game code doesn\u0026rsquo;t count. All the networking cares about is that avatars are represented by a head and two hands driven by the tracked headset and touch controller positions and orientations.\nTo synchronize this I captured the position and orientation of the avatar components in FixedUpdate along the rest of the physics state, and applied this state to the avatar components in the remote view.\nBut when I first tried this it looked absolutely awful. Why?\nAfter a bunch of debugging I worked out that the avatar state was sampled from the touch hardware at render framerate in Update, and was applied on the other machine at FixedUpdate, causing jitter because the avatar sample time didn\u0026rsquo;t line up with the current time in the remote view.\nTo fix this I stored the difference between physics and render time when sampling avatar state, and included this in the avatar state in each packet. Then I added a jitter buffer with 100ms delay to received packets, solving network jitter from time variance in packet delivery and enabling interpolation between avatar states to reconstruct a sample at the correct time.\nTo synchronize cubes held by avatars, while a cube is parented to an avatar\u0026rsquo;s hand, I set the cube\u0026rsquo;s priority factor to -1, stopping it from being sent with regular physics state updates. While a cube is attached to a hand, I include its id and relative position and rotation as part of the avatar state. In the remote view, cubes are attached to the avatar hand when the first avatar state arrives with that cube parented to it, and detached when regular physics state updates resume, corresponding to the cube being thrown or released.\nBidirectional Flow Now that I had player interaction with the scene working with the touch controllers, it was time to start thinking about how the second player can interact with the scene as well.\nTo do this without going insane switching between two headsets all the time (!!!), I extended my Unity test scene to be able to switch between the context of player one (left) and player two (right).\nI called the first player the \u0026ldquo;host\u0026rdquo; and the second player the \u0026ldquo;guest\u0026rdquo;. In this model, the host is the \u0026ldquo;real\u0026rdquo; simulation, and by default synchronizes all cubes to the guest player, but as the guest interacts with the world, it takes authority over these objects and sends state for them back to the host player.\nTo make this work without inducing obvious conflicts the host and guest both check the local state of cubes before taking authority and ownership. For example, the host won\u0026rsquo;t take ownership over a cube already under ownership of the guest, and vice versa, while authority is allowed to be taken, to let players throw cubes at somebody else\u0026rsquo;s stack and knock it over while it\u0026rsquo;s being built.\nGeneralizing further to four players, in the networked physics sample, all packets flow through the host player, making the host the arbiter. In effect, rather than being truly peer-to-peer, a topology is chosen that all guests in the game communicate only with the host player. This lets the host decide which updates to accept, and which updates to ignore and subsequently correct.\nTo apply these corrections I needed some way for the host to override guests and say, no, you don\u0026rsquo;t have authority/ownership over this cube, and you should accept this update. I also needed some way for the host to determine ordering for guest interactions with the world, so if one client experiences a burst of lag and delivers a bunch of packets late, these packets won\u0026rsquo;t take precedence over more recent actions from other guests.\nAs per my hunch earlier, this was achieved with two sequence numbers per-cube:\nAuthority sequence Ownership sequence These sequence numbers are sent along with each state update and included in avatar state when cubes are held by players. They are used by the host to determine if it should accept an update from guests, and by guests to determine if the state update from the server is more recent and should be accepted, even when that guest thinks it has authority or ownership over a cube.\nAuthority sequence increments each time a player takes authority over a cube and when a cube under authority of a player comes to rest. When a cube has authority on a guest machine, it holds authority on that machine until it receives confirmation from the host before returning to default authority. This ensures that the final at rest state for cubes under guest authority are committed back to the host, even under significant packet loss.\nOwnership sequence increments each time a player grabs a cube. Ownership is stronger than authority, such that an increase in ownership sequence wins over an increase in authority sequence number. For example, if a player interacts with a cube just before another player grabs it, the player who grabbed it wins.\nIn my experience working on this demo I found these rules to be sufficient to resolve conflicts, while letting host and guest players interact with the world lag free. Conflicts requiring corrections are rare in practice even under significant latency, and when they do occur, the simulation quickly converges to a consistent state.\nConclusion High quality networked physics with stable stacks of cubes is possible with Unity and PhysX using a distributed simulation network model.\nThis approach is best used for cooperative experiences only, as it does not provide the security of a server-authoritative network model with dedicated servers and client-side prediction.\nThanks to Oculus for sponsoring my work and making this research possible!\nThe source code for the networked physics sample can be downloaded here.\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1519257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519257600,"objectID":"8072fbc6e6bd49296218533ed1bd9c99","permalink":"https://gafferongames.com/post/networked_physics_in_virtual_reality/","publishdate":"2018-02-22T00:00:00Z","relpermalink":"/post/networked_physics_in_virtual_reality/","section":"post","summary":"Introduction About a year ago, Oculus approached me and offered to sponsor my research. They asked me, effectively: \u0026ldquo;Hey Glenn, there\u0026rsquo;s a lot of interest in networked physics in VR. You did a cool talk at GDC. Do you think could come up with a networked physics sample in VR that we could share with devs? Maybe you could use the touch controllers?\u0026rdquo;\nI replied \u0026ldquo;F*** yes!\u0026quot; cough \u0026ldquo;Sure. This could be a lot of fun!","tags":["physics","networking","vr"],"title":"Networked Physics in Virtual Reality","type":"post"},{"authors":null,"categories":["White papers"],"content":"Premise In 2017 the most popular web games like agar.io are networked via WebSockets over TCP. If a UDP equivalent of WebSockets could be incorporated into browsers, it would greatly improve the networking of these games.\nBackground Web browsers are built on top of HTTP, which is a stateless request/response protocol initially designed for serving static web pages. HTTP is built on top of TCP, a low-level protocol which guarantees data sent over the internet arrives reliably, and in the same order it was sent.\nThis has worked well for many years, but recently websites have become more interactive and poorly suited to the HTTP request/response paradigm. Rising to this challenge are modern web protocols like WebSockets, WebRTC, HTTP 2.0 and QUIC, which hold the potential to greatly improve the interactivity of the web.\nUnfortunately, this new set of standards for web development don\u0026rsquo;t provide what multiplayer games need, or, provide it in a form that is too complicated for game developers to use.\nThis leads to frustration from game developers, who just want to be able to send and receive UDP packets in the browser.\nThe Problem The web is built on top of TCP, which is a reliable-ordered protocol.\nTo deliver data reliably and in order under packet loss, it is necessary for TCP to hold more recent data in a queue while waiting for dropped packets to be resent. Otherwise, data would be delivered out of order.\nThis is called head of line blocking and it creates a frustrating and almost comedically tragic problem for game developers. The most recent data they want is delayed while waiting for old data to be resent, but by the time the resent data arrives, it\u0026rsquo;s too old to be used.\nUnfortunately, there is no way to fix this behavior under TCP. All data must be received reliably and in order. Therefore, the standard solution in the game industry for the past 20 years has been to send game data over UDP instead.\nHow this works in practice is that each game develops their own custom protocol on top of UDP, implementing basic reliability as required, while sending the majority of data as unreliable-unordered. This ensures that time series data arrives as quickly as possible without waiting for dropped packets to be resent.\nSo, what does this have to do with web games?\nThe main problem for web games today is that game developers have no way to follow this industry best practice in the browser. Instead, web games send their game data over TCP, causing hitches and non-responsiveness due to head of line blocking.\nThis is completely unnecessary and could be fixed overnight if web games had some way to send and receive UDP packets.\nWhat about WebSockets? WebSockets are an extension to the HTTP protocol which upgrade a HTTP connection so that data can be exchanged bidirectionally, rather than in the traditional request/response pattern.\nThis elegantly solves the problem of websites that need to display dynamically changing content, because once a web socket connection is established, the server can push data to the browser without a corresponding request.\nUnfortunately, since WebSockets are implemented on top of TCP, data is still subject to head of line blocking.\nWhat about QUIC? QUIC is an experimental protocol built on top of UDP that is designed as replacement transport layer for HTTP. It\u0026rsquo;s currently supported in Google Chrome only.\nA key feature of QUIC is support for multiple data streams. New data streams can be created implicitly by the client or server by increasing the channel id.\nThe channel concept provide two key benefits:\nAvoids a connection handshake each time a new request is made.\nEliminates head of line blocking between unrelated streams of data.\nUnfortunately, while head of line blocking is eliminated across unrelated data streams, it still exists within each stream.\nWhat about WebRTC? WebRTC is a collection of protocols that enable peer-to-peer communication between browsers for applications like audio and video streaming.\nAlmost as a footnote, WebRTC supports a data channel which can be configured in unreliable mode, providing a way to send and receive unreliable-unordered data from the browser.\nSo why are browser games still using WebSockets in 2017?\nThe reason is that there is a trend away from peer-to-peer towards client/server for multiplayer games and while WebRTC makes it easy to send unreliable-unordered data from one browser to another, it falls down when data needs to be sent between a browser and a dedicated server.\nIt falls down because WebRTC is extremely complex. This complexity is understandable, being designed primarily to support peer-to-peer communication between browsers, WebRTC needs STUN, ICE and TURN support for NAT traversal and packet forwarding in the worst case.\nBut from a game developer point of view, all this complexity seems like dead weight, when STUN, ICE and TURN are completely completely unnecessary to communicate with dedicated servers, which have public IPs.\n\u0026ldquo;I feel what is needed is a UDP version of WebSockets. That\u0026rsquo;s all I wish we had.\u0026quot; Matheus Valadares, creator of agar.io\nIn short, game developers appreciate simplicity and desire a \u0026ldquo;WebSockets for UDP\u0026rdquo;-like approach over the complexity of WebRTC.\nWhy not just let people send UDP? The final option to consider is to just let users send and receive UDP packets directly from the browser. Of course, this is an absolutely terrible idea and there are good reasons why it should never be allowed.\nWebsites would be able to launch DDoS attacks by coordinating UDP packet floods from browsers.\nNew security holes would be created as JavaScript running in web pages could craft malicious UDP packets to probe the internals of corporate networks and report back over HTTPS.\nUDP packets are not encrypted, so any data sent over these packets could be sniffed and read by an attacker, or even modified in transmit. It would be a massive step back for web security to create a new way for browsers to send unencrypted packets.\nThere is no authentication, so a dedicated server reading packets sent from a browser would have to implement its own method to ensure that only valid clients are allowed to connect to it, which is well outside the amount of effort most game developers would be willing to apply to this problem.\nSo clearly, just letting JavaScript create UDP sockets in the browser is a no go.\nWhat could a solution look like? But what if we approach it from the other side. What if, instead of trying to bridge from the web world to games, we started with what games need and worked back to something that could work well on the web?\nI\u0026rsquo;m Glenn Fiedler and I\u0026rsquo;ve been a game developer for the last 15 years. For most of this time I\u0026rsquo;ve specialized as a network programmer. I\u0026rsquo;ve got a lot of experience working on fast-paced action games. The last game I worked on was Titanfall 2\nAbout a month ago, I read this thread on Hacker News:\nWebRTC: the future of web games\nWhere the creator of agar.io, Matheus Valadares, explained that WebRTC was too complex for him to use, and that he\u0026rsquo;s still using WebSockets for his games.\nI got to thinking, surely a solution must exist that\u0026rsquo;s simpler than WebRTC?\nI wondered what exactly this solution would look like?\nMy conclusion was that any solution must have these properties:\nConnection based so it could not be used in DDoS attacks or to probe security holes.\nEncrypted because no game or web application would want to send unencrypted packets in 2017.\nAuthenticated because dedicated servers only want to accept connections from clients who are authenticated on the web backend.\nI would now like to present the solution. I\u0026rsquo;m not holding my breath that this would be accepted as a standard in browsers as-is, I\u0026rsquo;m a game guy, not a web guy. But I do hope at least that it will help browser creators and web developers see what client/server games actually need, and in some small way, do its part to help bridge the gap.\nHopefully the result will be multiplayer games playing better in a browser in the near future.\nnetcode.io The solution I came up with is netcode.io\nnetcode.io is a simple network protocol that lets clients securely connect to dedicated servers and communicate over UDP. It\u0026rsquo;s connection oriented and encrypts and signs packets, and provides authentication support so only authenticated clients can connect to dedicated servers.\nIt\u0026rsquo;s designed for games like agar.io that need to shunt players off from the main website to a number of dedicated server instances, each with some maximum number of players (up to 256 players per-instance in the reference implementation).\nThe basic idea is that the web backend performs authentication and when a client wants to play, it makes a REST call to obtain a connect token which is passed to the dedicated server as part of the connection handshake over UDP.\nConnect tokens are short lived and rely on a shared private key between the web backend and the dedicated server instances. The benefit of this approach is that only authenticated clients are able to connect to the dedicated servers.\nWhere netcode.io wins out over WebRTC is simplicity. By focusing only on the dedicated server case, it removes the need for ICE, STUN and TURN. By implementing encryption, signing and authentication with libsodium it avoids the complexity of a full implementation of DTLS, while still providing the same level of security.\nOver the past month I\u0026rsquo;ve created a reference implementation of netcode.io in C. It\u0026rsquo;s licenced under the BSD 3-Clause open source licence. Over the next few months, I hope to continue refining this implementation, spend time writing a spec, and work with people to port netcode.io to different languages.\nYour feedback on the reference implementation is appreciated.\nHow it works A client authenticates with the web backend using standard authentication techniques (eg. OAuth). Once a client is authenticated they request to play a game by making a REST call. The REST call returns a connect token to that client encoded as base64 over HTTPS.\nA connect token has two parts:\nA private portion, encrypted and signed by the shared private key using an AEAD primitive from libsodium. This cannot be read, modified or forged by the client.\nA public portion, which provides information the client needs to connect, like encryption keys for UDP packets and the list of server addresses to connect to, along with some other information corresponding to the \u0026lsquo;associated data\u0026rsquo; portion of the AEAD.\nThe client reads the connect token and has a list of n IP addresses to connect to in order. While n can be 1, it\u0026rsquo;s best to give the client multiple servers in case the first server is full by the time the client attempts to connect to it.\nWhen connecting to a dedicated server the client sends a connection request packet repeatedly over UDP. This packet contains the private connect token data, plus some additional data for the AEAD such as the netcode.io version info, protocol id (a 64bit number unique to this particular game), expiry timestamp for the connnect token and the sequence number for the AEAD primitive.\nWhen the dedicated server receives a connection request over UDP it first checks that the contents of the packet are valid using the AEAD primitive. If any of the public data in the connection request packet is modified, the signature check will fail. This stops clients from modifying the expiry timestamp for a connect token, while also making rejection of expired tokens very fast.\nProvided the connect token is valid, it is decrypted. Internally it contains a list of dedicated server addresses that the connect token is valid for, stopping malicious clients going wide with one connect token and using it to connect to all available dedicated servers.\nThe server also checks if the connect token has already been used by searching a short history of connect token HMACs, and ignores the connection request if a match is found. This prevents one connect token from being used to connect multiple clients.\nAdditionally, the server enforces that only one client with a given IP address and port may be connected at any time, and only one client by unique client id may be connected at any time, where client id is a 64 bit integer that uniquely identifies a client that has been authenticated by the web backend.\nProvided the connect token has not expired, it decrypts successfully, and the dedicated server\u0026rsquo;s public IP is in the list of server addresses, and all other checks pass, the dedicated server sets up a mapping between the client IP address and the encryption keys contained in the private connect token data.\nAll packets exchanged between the client and server from this point are encrypted using these keys. This encryption mapping expires if no UDP packets are received from the address for a short amount of time like 5 seconds.\nNext, the server checks if there is room for the client on the server. Each server supports some maximum number of clients, for example a 64 player game has 64 slots for clients to connect to. If the server is full, it responds with a connection request denied packet. This lets clients quickly know to move on to the next server in the list when a server is full.\nIf there is room for the client, the server doesn\u0026rsquo;t yet assign the client to that slot, but instead stores the address + HMAC for the connect token for that client as a potential client. The server then responds with a connection challenge packet, which contains a challenge token which is a block of data encrypted with a random key rolled when the server is started.\nThis key randomization ensures there is not a security problem when the same sequence number is used to encrypt challenge tokens across multiple servers (the servers do not coordinate). Also, the connection challenge packet is significantly smaller than the connection request packet by design, to eliminate the possibility of the protocol being used as part of a DDoS amplification attack.\nThe client receives the connection challenge packet over UDP and switches to a state where it sends connection response packets to the server. Connection response packets simply reflect the challenge token back to the dedicated server, establishing that the client is actually able to receive packets on the source IP address they claim they are sending packets from. This stops clients with spoofed packet source addresses from connecting.\nWhen the server receives a connection response packet it looks for a matching pending client entry, and if one exists, it searches once again for a free slot for the client to connect to. If there isn\u0026rsquo;t one, it replies with a connection request denied packet since there may have been a slot free when the connection request was first received that is no longer available.\nAlternatively, the server assigns the client to a free slot and replies back with a connection keep-alive packet, which tells the client which slot it was assigned on the server. This is known as a client index. In multiplayer games, this is typically used to identify clients connected to a server. For example, clients 0,1,2,3 in a 4 player game correspond to players 1,2,3 and 4.\nThe server now considers the client connected and is able to send connection payload packets down to that client. These packets wrap game specific data and are delivered unreliable-ordered. The only caveat is that since the client needs to first receive a connection keep-alive before it knows its client index and considers itself to be fully connected, the server tracks on a per-client slot basis whether that client is confirmed.\nThe confirmed flag per-client is initially set to false, and flips true once the server has received a keep-alive or payload packet from that client. Until a client is confirmed, each time a payload packet is sent from the server to that client, it is prefixed with a keep-alive packet. This ensures the client is statistically likely to know its client index and be fully connected prior to receiving the first payload packet sent from the server, minimizing the number of connection establishment round-trips.\nNow that the client and server are fully connected they can exchange UDP packets bidirectionally. Typical game protocols sent player inputs from client to server at a high rate like 60 times per-second, and world state from the server to client at a slightly lower rate, like 20 times per-second. However more recent AAA games are increasing the server update rate.\nIf the server or client don\u0026rsquo;t exchange a steady stream of packets, keep-alive packets are automatically generated so the connection doesn\u0026rsquo;t time out. If no packets are received from either side of the connection for a short amount of time like 5 seconds, the connection times out.\nIf either side of the connection wishes to cleanly disconnect, a number of connection disconnect packets are fired across redundantly, so that statistically these packets are likely to get through even under packet loss. This ensures that clean disconnects happen quickly, without the other side waiting for time out.\nConclusion Popular web games like agar.io are networked via WebSockets over TCP, because WebRTC is difficult to use in a client/server context with dedicated servers.\nOne solution would be for Google to make it significantly easier for game developers to integrate WebRTC data channel support in their dedicated servers.\nAlternatively, netcode.io provides a much simpler \u0026lsquo;WebSockets for UDP\u0026rsquo;-like approach, which would also solve the problem, if it were standardized and incorporated into browsers.\nUPDATE: netcode.io is now available in browsers!\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1488067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488067200,"objectID":"615d96691fe362081f04f7155b29836b","permalink":"https://gafferongames.com/post/why_cant_i_send_udp_packets_from_a_browser/","publishdate":"2017-02-26T00:00:00Z","relpermalink":"/post/why_cant_i_send_udp_packets_from_a_browser/","section":"post","summary":"Premise In 2017 the most popular web games like agar.io are networked via WebSockets over TCP. If a UDP equivalent of WebSockets could be incorporated into browsers, it would greatly improve the networking of these games.\nBackground Web browsers are built on top of HTTP, which is a stateless request/response protocol initially designed for serving static web pages. HTTP is built on top of TCP, a low-level protocol which guarantees data sent over the internet arrives reliably, and in the same order it was sent.","tags":["networking"],"title":"Why can't I send UDP packets from a browser?","type":"post"},{"authors":null,"categories":["Building a Game Network Protocol"],"content":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nSo far in this article series we\u0026rsquo;ve discussed how games read and write packets, how to unify packet read and write into a single function, how to fragment and re-assemble packets, and how to send large blocks of data over UDP.\nNow in this article we\u0026rsquo;re going to bring everything together and build a client/server connection on top of UDP.\nBackground Developers from a web background often wonder why games go to such effort to build a client/server connection on top of UDP, when for many applications, TCP is good enough. *\nThe reason is that games send time critical data.\nWhy don\u0026rsquo;t games use TCP for time critical data? The answer is that TCP delivers data reliably and in-order, and to do this on top of IP (which is unreliable, unordered) it holds more recent packets hostage in a queue while older packets are resent over the network.\nThis is known as head of line blocking and it\u0026rsquo;s a huuuuuge problem for games. To understand why, consider a game server broadcasting the state of the world to clients 10 times per-second. Each client advances time forward and wants to display the most recent state it receives from the server.\nBut if the packet containing state for time t = 10.0 is lost, under TCP we must wait for it to be resent before we can access t = 10.1 and 10.2, even though those packets have already arrived and contain the state the client wants to display.\nWorse still, by the time the resent packet arrives, it\u0026rsquo;s far too late for the client to actually do anything useful with it. The client has already advanced past 10.0 and wants to display something around 10.3 or 10.4!\nSo why resend dropped packets at all? BINGO! What we\u0026rsquo;d really like is an option to tell TCP: \u0026ldquo;Hey, I don\u0026rsquo;t care about old packets being resent, by they time they arrive I can\u0026rsquo;t use them anyway, so just let me skip over them and access the most recent data\u0026rdquo;.\nUnfortunately, TCP simply does not give us this option :(\nAll data must be delivered reliably and in-order.\nThis creates terrible problems for time critical data where packet loss and latency exist. Situations like, you know, The Internet, where people play FPS games.\nLarge hitches corresponding to multiples of round trip time are added to the stream of data as TCP waits for dropped packets to be resent, which means additional buffering to smooth out these hitches, or long pauses where the game freezes and is non-responsive.\nNeither option is acceptable for first person shooters, which is why virtually all first person shooters are networked using UDP. UDP doesn\u0026rsquo;t provide any reliability or ordering, so protocols built on top it can access the most recent data without waiting for lost packets to be resent, implementing whatever reliability they need in radically different ways to TCP.\nBut, using UDP comes at a cost:\nUDP doesn\u0026rsquo;t provide any concept of connection.\nWe have to build that ourselves. This is a lot of work! So strap in, get ready, because we\u0026rsquo;re going to build it all up from scratch using the same basic techniques first person shooters use when creating their protocols over UDP. You can use this client/server protocol for games or non-gaming applications and, provided the data you send is time critical, I promise you, it\u0026rsquo;s well worth the effort.\n* These days even web servers are transitioning to UDP via Google\u0026rsquo;s QUIC. If you still think TCP is good enough for time critical data in 2016, I encourage you to put that in your pipe and smoke it :)\nClient/Server Abstraction The goal is to create an abstraction on top of a UDP socket where our server presents a number of virtual slots for clients to connect to:\nWhen a client requests a connection, it gets assigned to one of these slots:\nIf a client requests connection, but no slots are available, the server is full and the connection request is denied:\nOnce a client is connected, packets are exchanged in both directions. These packets form the basis for the custom protocol between the client and server which is game specific.\nIn a first person shooter, packets are sent continuously in both directions. Clients send input to the server as quickly as possible, often 30 or 60 times per-second, and the server broadcasts the state of the world to clients 10, 20 or even 60 times per-second.\nBecause of this steady flow of packets in both directions there is no need for keep-alive packets. If at any point packets stop being received from the other side, the connection simply times out. No packets for 5 seconds is a good timeout value in my opinion, but you can be more aggressive if you want.\nWhen a client slot times out on the server, it becomes available for other clients to connect. When the client times out, it transitions to an error state.\nSimple Connection Protocol Let\u0026rsquo;s get started with the implementation of a simple protocol. It\u0026rsquo;s a bit basic and more than a bit naive, but it\u0026rsquo;s a good starting point and we\u0026rsquo;ll build on it during the rest of this article, and the next few articles in this series.\nFirst up we have the client state machine.\nThe client is in one of three states:\nDisconnected Connecting Connected Initially the client starts in disconnected.\nWhen a client connects to a server, it transitions to the connecting state and sends connection request packets to the server:\nThe CRC32 and implicit protocol id in the packet header allow the server to trivially reject UDP packets not belonging to this protocol or from a different version of it.\nSince connection request packets are sent over UDP, they may be lost, received out of order or in duplicate.\nBecause of this we do two things: 1) we keep sending packets for the client state until we get a response from the server or the client times out, and 2) on both client and server we ignore any packets that don\u0026rsquo;t correspond to what we are expecting, since a lot of redundant packets are flying over the network.\nOn the server, we have the following data structure:\nconst int MaxClients = 64; class Server { int m_maxClients; int m_numConnectedClients; bool m_clientConnected[MaxClients]; Address m_clientAddress[MaxClients]; }; Which lets the server lookup a free slot for a client to join (if any are free):\nint Server::FindFreeClientIndex() const { for ( int i = 0; i \u0026lt; m_maxClients; ++i ) { if ( !m_clientConnected[i] ) return i; } return -1; } Find the client index corresponding to an IP address and port:\nint Server::FindExistingClientIndex( const Address \u0026amp; address ) const { for ( int i = 0; i \u0026lt; m_maxClients; ++i ) { if ( m_clientConnected[i] \u0026amp;\u0026amp; m_clientAddress[i] == address ) return i; } return -1; } Check if a client is connected to a given slot:\nbool Server::IsClientConnected( int clientIndex ) const { return m_clientConnected[clientIndex]; } \u0026hellip; and retrieve a client’s IP address and port by client index:\nconst Address \u0026amp; Server::GetClientAddress( int clientIndex ) const { return m_clientAddress[clientIndex]; } Using these queries we implement the following logic when the server processes a connection request packet:\nIf the server is full, reply with connection denied.\nIf the connection request is from a new client and we have a slot free, assign the client to a free slot and respond with connection accepted.\nIf the sender corresponds to the address of a client that is already connected, also reply with connection accepted. This is necessary because the first response packet may not have gotten through due to packet loss. If we don\u0026rsquo;t resend this response, the client gets stuck in the connecting state until it times out.\nThe connection accepted packet tells the client which client index it was assigned, which the client needs to know which player it is in the game:\nOnce the server sends a connection accepted packet, from its point of view it considers that client connected. As the server ticks forward, it watches connected client slots, and if no packets have been received from a client for 5 seconds, the slot times out and is reset, ready for another client to connect.\nBack to the client. While the client is in the connecting state the client listens for connection denied and connection accepted packets from the server. Any other packets are ignored.\nIf the client receives connection accepted, it transitions to connected. If it receives connection denied, or after 5 seconds hasn\u0026rsquo;t received any response from the server, it transitions to disconnected.\nOnce the client hits connected it starts sending connection payload packets to the server. If no packets are received from the server in 5 seconds, the client times out and transitions to disconnected.\nNaive Protocol is Naive While this protocol is easy to implement, we can\u0026rsquo;t use a protocol like this in production. It\u0026rsquo;s way too naive. It simply has too many weaknesses to be taken seriously:\nSpoofed packet source addresses can be used to redirect connection accepted responses to a target (victim) address. If the connection accepted packet is larger than the connection request packet, attackers can use this protocol as part of a DDoS amplification attack.\nSpoofed packet source addresses can be used to trivially fill all client slots on a server by sending connection request packets from n different IP addresses, where n is the number of clients allowed per-server. This is a real problem for dedicated servers. Obviously you want to make sure that only real clients are filling slots on servers you are paying for.\nAn attacker can trivially fill all slots on a server by varying the client UDP port number on each client connection. This is because clients are considered unique on an address + port basis. This isn\u0026rsquo;t easy to fix because due to NAT (network address translation), different players behind the same router collapse to the same IP address with only the port being different, so we can\u0026rsquo;t just consider clients to be unique at the IP address level sans port.\nTraffic between the client and server can be read and modified in transit by a third party. While the CRC32 protects against packet corruption, an attacker would simply recalculate the CRC32 to match the modified packet.\nIf an attacker knows the client and server IP addresses and ports, they can impersonate the client or server. This gives an attacker the power to completely a hijack a client’s connection and perform actions on their behalf.\nOnce a client is connected to a server there is no way for them to disconnect cleanly, they can only time out. This creates a delay before the server realizes a client has disconnected, or before a client realizes the server has shut down. It would be nice if both the client and server could indicate a clean disconnect, so the other side doesn’t need to wait for timeout in the common case.\nClean disconnection is usually implemented with a disconnect packet, however because an attacker can impersonate the client and server with spoofed packets, doing so would give the attacker the ability to disconnect a client from the server whenever they like, provided they know the client and server IP addresses and the structure of the disconnect packet.\nIf a client disconnects dirty and attempts to reconnect before their slot times out on the server, the server still thinks that client is connected and replies with connection accepted to handle packet loss. The client processes this response and thinks it\u0026rsquo;s connected to the server, but it\u0026rsquo;s actually in an undefined state.\nWhile some of these problems require authentication and encryption before they can be fully solved, we can make some small steps forward to improve the protocol before we get to that. These changes are instructive.\nImproving The Connection Protocol The first thing we want to do is only allow clients to connect if they can prove they are actually at the IP address and port they say they are.\nTo do this, we no longer accept client connections immediately on connection request, instead we send back a challenge packet, and only complete connection when a client replies with information that can only be obtained by receiving the challenge packet.\nThe sequence of operations in a typical connect now looks like this:\nTo implement this we need an additional data structure on the server. Somewhere to store the challenge data for pending connections, so when a challenge response comes in from a client we can check against the corresponding entry in the data structure and make sure it\u0026rsquo;s a valid response to the challenge sent to that address.\nWhile the pending connect data structure can be made larger than the maximum number of connected clients, it\u0026rsquo;s still ultimately finite and is therefore subject to attack. We\u0026rsquo;ll cover some defenses against this in the next article. But for the moment, be happy at least that attackers can\u0026rsquo;t progress to the connected state with spoofed packet source addresses.\nNext, to guard against our protocol being used in a DDoS amplification attack, we\u0026rsquo;ll inflate client to server packets so they\u0026rsquo;re large relative to the response packet sent from the server. This means we add padding to both connection request and challenge response packets and enforce this padding on the server, ignoring any packets without it. Now our protocol effectively has DDoS minification for requests -\u0026gt; responses, making it highly unattractive for anyone thinking of launching this kind of attack.\nFinally, we\u0026rsquo;ll do one last small thing to improve the robustness and security of the protocol. It\u0026rsquo;s not perfect, we need authentication and encryption for that, but it at least it ups the ante, requiring attackers to actually sniff traffic in order to impersonate the client or server. We\u0026rsquo;ll add some unique random identifiers, or \u0026lsquo;salts\u0026rsquo;, to make each client connection unique from previous ones coming from the same IP address and port.\nThe connection request packet now looks like this:\nThe client salt in the packet is a random 64 bit integer rolled each time the client starts a new connect. Connection requests are now uniquely identified by the IP address and port combined with this client salt value. This distinguishes packets from the current connection from any packets belonging to a previous connection, which makes connection and reconnection to the server much more robust.\nNow when a connection request arrives and a pending connection entry can\u0026rsquo;t be found in the data structure (according to IP, port and client salt) the server rolls a server salt and stores it with the rest of the data for the pending connection before sending a challange packet back to the client. If a pending connection is found, the salt value stored in the data structure is used for the challenge. This way there is always a consistent pair of client and server salt values corresponding to each client session.\nThe client state machine has been expanded so connecting is replaced with two new states: sending connection request and sending challenge response, but it\u0026rsquo;s the same idea as before. Client states repeatedly send the packet corresponding to that state to the server while listening for the response that moves it forward to the next state, or back to an error state. If no response is received, the client times out and transitions to disconnected.\nThe challenge response sent from the client to the server looks like this:\nThe utility of this being that once the client and server have established connection, we prefix all payload packets with the xor of the client and server salt values and discard any packets with the incorrect salt values. This neatly filters out packets from previous sessions and requires an attacker to sniff packets in order to impersonate a client or server.\nNow that we have at least a basic level of security, it\u0026rsquo;s not much, but at least it\u0026rsquo;s something, we can implement a disconnect packet:\nAnd when the client or server want to disconnect clean, they simply fire 10 of these over the network to the other side, in the hope that some of them get through, and the other side disconnects cleanly instead of waiting for timeout.\nConclusion We now have a much more robust protocol. It\u0026rsquo;s secure against spoofed IP packet headers. It\u0026rsquo;s no longer able to be used as port of DDoS amplification attacks, and with a trivial xor based authentication, we are protected against casual attackers while client reconnects are much more robust.\nBut it\u0026rsquo;s still vulnerable to a sophisticated actors who can sniff packets:\nThis attacker can read and modify packets in flight.\nThis breaks the trivial identification based around salt values\u0026hellip;\n\u0026hellip; giving an attacker the power to disconnect any client at will.\nTo solve this, we need to get serious with cryptography to encrypt and sign packets so they can\u0026rsquo;t be read or modified by a third party.\nNEXT ARTICLE: Securing Dedicated Servers.\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1475020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475020800,"objectID":"053124fe85545bf081268d188fd745d7","permalink":"https://gafferongames.com/post/client_server_connection/","publishdate":"2016-09-28T00:00:00Z","relpermalink":"/post/client_server_connection/","section":"post","summary":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nSo far in this article series we\u0026rsquo;ve discussed how games read and write packets, how to unify packet read and write into a single function, how to fragment and re-assemble packets, and how to send large blocks of data over UDP.\nNow in this article we\u0026rsquo;re going to bring everything together and build a client/server connection on top of UDP.","tags":["networking"],"title":"Client Server Connection","type":"post"},{"authors":null,"categories":["Building a Game Network Protocol"],"content":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nMany people will tell you that implementing your own reliable message system on top of UDP is foolish. After all, why reimplement TCP?\nBut why limit ourselves to how TCP works? But there are so many different ways to implement reliable-messages and most of them work nothing like TCP!\nSo let\u0026rsquo;s get creative and work out how we can implement a reliable message system that\u0026rsquo;s better and more flexible than TCP for real-time games.\nDifferent Approaches A common approach to reliability in games is to have two packet types: reliable-ordered and unreliable. You\u0026rsquo;ll see this approach in many network libraries.\nThe basic idea is that the library resends reliable packets until they are received by the other side. This is the option that usually ends up looking a bit like TCP-lite for the reliable-packets. It\u0026rsquo;s not that bad, but you can do much better.\nThe way I prefer to think of it is that messages are smaller bitpacked elements that know how to serialize themselves. This makes the most sense when the overhead of length prefixing and padding bitpacked messages up to the next byte is undesirable (eg. lots of small messages included in each packet). Sent messages are placed in a queue and each time a packet is sent some of the messages in the send queue are included in the outgoing packet. This way there are no reliable packets that need to be resent. Reliable messages are simply included in outgoing packets until they are received.\nThe easiest way to do this is to include all unacked messages in each packet sent. It goes something like this: each message sent has an id that increments each time a message is sent. Each outgoing packet includes the start message id followed by the data for n messages. The receiver continually sends back the most recent received message id to the sender as an ack and only messages newer than the most recent acked message id are included in packets.\nThis is simple and easy to implement but if a large burst of packet loss occurs while you are sending messages you get a spike in packet size due to unacked messages.\nYou can avoid this by extending the system to have an upper bound on the number of messages included per-packet n. But now if you have a high packet send rate (like 60 packets per-second) you are sending the same message multiple times until you get an ack for that message.\nIf your round trip time is 100ms each message will be sent 6 times redundantly before being acked on average. Maybe you really need this amount of redundancy because your messages are extremely time critical, but in most cases, your bandwidth would be better spent on other things.\nThe approach I prefer combines packet level acks with a prioritization system that picks the n most important messages to include in each packet. This combines time critical delivery and the ability to send only n messages per-packet, while distributing sends across all messages in the send queue.\nPacket Level Acks To implement packet level acks, we add the following packet header:\nstruct Header { uint16_t sequence; uint16_t ack; uint32_t ack_bits; }; These header elements combine to create the ack system: sequence is a number that increases with each packet sent, ack is the most recent packet sequence number received, and ack_bits is a bitfield encoding the set of acked packets.\nIf bit n is set in ack_bits, then ack - n is acked. Not only is ack_bits a smart encoding that saves bandwidth, it also adds redundancy to combat packet loss. Each ack is sent 32 times. If one packet is lost, there\u0026rsquo;s 31 other packets with the same ack. Statistically speaking, acks are very likely to get through.\nBut bursts of packet loss do happen, so it\u0026rsquo;s important to note that:\nIf you receive an ack for packet n then that packet was definitely received.\nIf you don\u0026rsquo;t receive an ack, the packet was most likely not received. But, it might have been, and the ack just didn\u0026rsquo;t get through. This is extremely rare.\nIn my experience it\u0026rsquo;s not necessary to send perfect acks. Building a reliability system on top of a system that very rarely drops acks adds no significant problems. But it does create a challenge for testing this system works under all situations because of the edge cases when acks are dropped.\nSo please if you do implement this system yourself, setup a soak test with terrible network conditions to make sure your ack system is working correctly. You\u0026rsquo;ll find such a soak test in the example source code for this article, and the open source network libraries reliable.io and yojimbo which also implement this technique.\nSequence Buffers To implement this ack system we need a data structure on the sender side to track whether a packet has been acked so we can ignore redundant acks (each packet is acked multiple times via ack_bits. We also need a data structure on the receiver side to keep track of which packets have been received so we can fill in the ack_bits value in the packet header.\nThe data structure should have the following properties:\nConstant time insertion (inserts may be random, for example out of order packets\u0026hellip;) Constant time query if an entry exists given a packet sequence number Constant time access for the data stored for a given packet sequence number Constant time removal of entries You might be thinking. Oh of course, hash table. But there\u0026rsquo;s a much simpler way:\nconst int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData { bool acked; }; PacketData packet_data[BufferSize]; PacketData * GetPacketData( uint16_t sequence ) { const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return \u0026amp;packet_data[index]; else return NULL; } As you can see the trick here is a rolling buffer indexed by sequence number:\nconst int index = sequence % BufferSize; This works because we don\u0026rsquo;t care about being destructive to old entries. As the sequence number increases older entries are naturally overwritten as we insert new ones. The sequence_buffer[index] value is used to test if the entry at that index actually corresponds to the sequence number you\u0026rsquo;re looking for. A sequence buffer value of 0xFFFFFFFF indicates an empty entry and naturally returns NULL for any sequence number query without an extra branch.\nWhen entries are added in order like a send queue, all that needs to be done on insert is to update the sequence buffer value to the new sequence number and overwrite the data at that index:\nPacketData \u0026amp; InsertPacketData( uint16_t sequence ) { const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index]; } Unfortunately, on the receive side packets arrive out of order and some are lost. Under ridiculously high packet loss (99%) I\u0026rsquo;ve seen old sequence buffer entries stick around from before the previous sequence number wrap at 65535 and break my ack logic (leading to false acks and broken reliability where the sender thinks the other side has received something they haven\u0026rsquo;t\u0026hellip;).\nThe solution to this problem is to walk between the previous highest insert sequence and the new insert sequence (if it is more recent) and clear those entries in the sequence buffer to 0xFFFFFFFF. Now in the common case, insert is very close to constant time, but worst case is linear where n is the number of sequence entries between the previous highest insert sequence and the current insert sequence.\nBefore we move on I would like to note that you can do much more with this data structure than just acks. For example, you could extend the per-packet data to include time sent:\nstruct PacketData { bool acked; double send_time; }; With this information you can create your own estimate of round trip time by comparing send time to current time when packets are acked and taking an exponentially smoothed moving average. You can even look at packets in the sent packet sequence buffer older than your RTT estimate (you should have received an ack for them by now\u0026hellip;) to create your own packet loss estimate.\nAck Algorithm Now that we have the data structures and packet header, here is the algorithm for implementing packet level acks:\nOn packet send:\nInsert an entry for for the current send packet sequence number in the sent packet sequence buffer with data indicating that it hasn\u0026rsquo;t been acked yet\nGenerate ack and ack_bits from the contents of the local received packet sequence buffer and the most recent received packet sequence number\nFill the packet header with sequence, ack and ack_bits\nSend the packet and increment the send packet sequence number\nOn packet receive:\nRead in sequence from the packet header\nIf sequence is more recent than the previous most recent received packet sequence number, update the most recent received packet sequence number\nInsert an entry for this packet in the received packet sequence buffer\nDecode the set of acked packet sequence numbers from ack and ack_bits in the packet header.\nIterate across all acked packet sequence numbers and for any packet that is not already acked call OnPacketAcked( uint16_t sequence ) and mark that packet as acked in the sent packet sequence buffer.\nImportantly this algorithm is done on both sides so if you have a client and a server then each side of the connection runs the same logic, maintaining its own sequence number for sent packets, tracking most recent received packet sequence # from the other side and a sequence buffer of received packets from which it generates sequence, ack and ack_bits to send to the other side.\nAnd that\u0026rsquo;s really all there is to it. Now you have a callback when a packet is received by the other side: OnPacketAcked. The main benefit of this ack system is now that you know which packets were received, you can build any reliability system you want on top. It\u0026rsquo;s not limited to just reliable-ordered messages. For example, you could use it to implement delta encoding on a per-object basis.\nMessage Objects Messages are small objects (smaller than packet size, so that many will fit in a typical packet) that know how to serialize themselves. In my system they perform serialization using a unified serialize functionunified serialize function.\nThe serialize function is templated so you write it once and it handles read, write and measure.\nYes. Measure. One of my favorite tricks is to have a dummy stream class called MeasureStream that doesn\u0026rsquo;t do any actual serialization but just measures the number of bits that would be written if you called the serialize function. This is particularly useful for working out which messages are going to fit into your packet, especially when messages themselves can have arbitrarily complex serialize functions.\nstruct TestMessage : public Message { uint32_t a,b,c; TestMessage() { a = 0; b = 0; c = 0; } template \u0026lt;typename Stream\u0026gt; bool Serialize( Stream \u0026amp; stream ) { serialize_bits( stream, a, 32 ); serialize_bits( stream, b, 32 ); serialize_bits( stream, c, 32 ); return true; } virtual SerializeInternal( WriteStream \u0026amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( ReadStream \u0026amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( MeasureStream \u0026amp; stream ) { return Serialize( stream ); } }; The trick here is to bridge the unified templated serialize function (so you only have to write it once) to virtual serialize methods by calling into it from virtual functions per-stream type. I usually wrap this boilerplate with a macro, but it\u0026rsquo;s expanded in the code above so you can see what\u0026rsquo;s going on.\nNow when you have a base message pointer you can do this and it just works:\nMessage * message = CreateSomeMessage(); message-\u0026gt;SerializeInternal( stream ); An alternative if you know the full set of messages at compile time is to implement a big switch statement on message type casting to the correct message type before calling into the serialize function for each type. I\u0026rsquo;ve done this in the past on console platform implementations of this message system (eg. PS3 SPUs) but for applications today (2016) the overhead of virtual functions is neglible.\nMessages derive from a base class that provides a common interface such as serialization, querying the type of a message and reference counting. Reference counting is necessary because messages are passed around by pointer and stored not only in the message send queue until acked, but also in outgoing packets which are themselves C++ structs.\nThis is a strategy to avoid copying data by passing both messages and packets around by pointer. Somewhere else (ideally on a separate thread) packets and the messages inside them are serialized to a buffer. Eventually, when no references to a message exist in the message send queue (the message is acked) and no packets including that message remain in the packet send queue, the message is destroyed.\nWe also need a way to create messages. I do this with a message factory class with a virtual function overriden to create a message by type. It\u0026rsquo;s good if the packet factory also knows the total number of message types, so we can serialize a message type over the network with tight bounds and discard malicious packets with message type values outside of the valid range:\nenum TestMessageTypes { TEST_MESSAGE_A, TEST_MESSAGE_B, TEST_MESSAGE_C, TEST_MESSAGE_NUM_TYPES }; // message definitions omitted class TestMessageFactory : public MessageFactory { public: Message * Create( int type ) { switch ( type ) { case TEST_MESSAGE_A: return new TestMessageA(); case TEST_MESSAGE_B: return new TestMessageB(); case TEST_MESSAGE_C: return new TestMessageC(); } } virtual int GetNumTypes() const { return TEST_MESSAGE_NUM_TYPES; } }; Again, this is boilerplate and is usually wrapped by macros, but underneath this is what\u0026rsquo;s going on.\nReliable Ordered Message Algorithm The algorithm for sending reliable-ordered messages is as follows:\nOn message send:\nMeasure how many bits the message serializes to using the measure stream\nInsert the message pointer and the # of bits it serializes to into a sequence buffer indexed by message id. Set the time that message has last been sent to -1\nIncrement the send message id\nOn packet send:\nWalk across the set of messages in the send message sequence buffer between the oldest unacked message id and the most recent inserted message id from left -\u0026gt; right (increasing message id order).\nNever send a message id that the receiver can\u0026rsquo;t buffer or you\u0026rsquo;ll break message acks (since that message won\u0026rsquo;t be buffered, but the packet containing it will be acked, the sender thinks the message has been received, and will not resend it). This means you must never send a message id equal to or more recent than the oldest unacked message id plus the size of the message receive buffer.\nFor any message that hasn\u0026rsquo;t been sent in the last 0.1 seconds and fits in the available space we have left in the packet, add it to the list of messages to send. Messages on the left (older messages) naturally have priority due to the iteration order.\nInclude the messages in the outgoing packet and add a reference to each message. Make sure the packet destructor decrements the ref count for each message.\nStore the number of messages in the packet n and the array of message ids included in the packet in a sequence buffer indexed by the outgoing packet sequence number so they can be used to map packet level acks to the set of messages included in that packet.\nAdd the packet to the packet send queue.\nOn packet receive:\nWalk across the set of messages included in the packet and insert them in the receive message sequence buffer indexed by their message id.\nThe ack system automatically acks the packet sequence number we just received.\nOn packet ack:\nLook up the set of messages ids included in the packet by sequence number.\nRemove those messages from the message send queue if they exist and decrease their ref count.\nUpdate the last unacked message id by walking forward from the previous unacked message id in the send message sequence buffer until a valid message entry is found, or you reach the current send message id. Whichever comes first.\nOn message receive:\nCheck the receive message sequence buffer to see if a message exists for the current receive message id.\nIf the message exists, remove it from the receive message sequence buffer, increment the receive message id and return a pointer to the message.\nOtherwise, no message is available to receive. Return NULL.\nIn short, messages keep getting included in packets until a packet containing that message is acked. We use a data structure on the sender side to map packet sequence numbers to the set of message ids to ack. Messages are removed from the send queue when they are acked. On the receive side, messages arriving out of order are stored in a sequence buffer indexed by message id, which lets us receive them in the order they were sent.\nThe End Result This provides the user with an interface that looks something like this on send:\nTestMessage * message = (TestMessage*) factory.Create( TEST_MESSAGE ); if ( message ) { message-\u0026gt;a = 1; message-\u0026gt;b = 2; message-\u0026gt;c = 3; connection.SendMessage( message ); } And on the receive side:\nwhile ( true ) { Message * message = connection.ReceiveMessage(); if ( !message ) break; if ( message-\u0026gt;GetType() == TEST_MESSAGE ) { TestMessage * testMessage = (TestMessage*) message; // process test message } factory.Release( message ); } Which is flexible enough to implement whatever you like on top of it.\nNEXT ARTICLE: Client Server Connection\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1473897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473897600,"objectID":"7e7fdb6a826e70b80b732f97d98d4818","permalink":"https://gafferongames.com/post/reliable_ordered_messages/","publishdate":"2016-09-15T00:00:00Z","relpermalink":"/post/reliable_ordered_messages/","section":"post","summary":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nMany people will tell you that implementing your own reliable message system on top of UDP is foolish. After all, why reimplement TCP?\nBut why limit ourselves to how TCP works? But there are so many different ways to implement reliable-messages and most of them work nothing like TCP!\nSo let\u0026rsquo;s get creative and work out how we can implement a reliable message system that\u0026rsquo;s better and more flexible than TCP for real-time games.","tags":["networking"],"title":"Reliable Ordered Messages","type":"post"},{"authors":null,"categories":["Building a Game Network Protocol"],"content":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn the previous article we implemented packet fragmentation and reassembly so we can send packets larger than MTU.\nThis approach works great when the data block you\u0026rsquo;re sending is time critical and can be dropped, but in other cases you need to send large blocks of quickly and reliably over packet loss, and you need the data to get through.\nIn this situation, a different technique gives much better results.\nBackground It\u0026rsquo;s common for servers to send large block of data to the client on connect, for example, the initial state of the game world for late join.\nLet\u0026rsquo;s assume this data is 256k in size and the client needs to receive it before they can join the game. The client is stuck behind a load screen waiting for the data, so obviously we want it to be transmitted as quickly as possible.\nIf we send the data with the technique from the previous article, we get packet loss amplification because a single dropped fragment results in the whole packet being lost. The effect of this is actually quite severe. Our example block split into 256 fragments and sent over 1% packet loss now has a whopping 92.4% chance of being dropped!\nSince we just need the data to get across, we have no choice but to keep sending it until it gets through. On average, we have to send the block 10 times before it\u0026rsquo;s received. You may laugh but this actually happened on a AAA game I worked on!\nTo fix this, I implemented a new system for sending large blocks, one that handles packet loss by resends fragments until they are acked. Then I took the problematic large blocks and piped them through this system, fixing a bunch of players stalling out on connect, while continuing to send time critical data (snapshots) via packet fragmentation and reassembly.\nChunks and Slices In this new system blocks of data are called chunks. Chunks are split up into slices. This name change keeps the chunk system terminology (chunks/slices) distinct from packet fragmentation and reassembly (packets/fragments).\nThe basic idea is that slices are sent over the network repeatedly until they all get through. Since we are implementing this over UDP, simple in concept becomes a little more complicated in implementation because have to build in our own basic reliability system so the sender knows which slices have been received.\nThis reliability gets quite tricky if we have a bunch of different chunks in flight, so we\u0026rsquo;re going to make a simplifying assumption up front: we\u0026rsquo;re only going to send one chunk over the network at a time. This doesn\u0026rsquo;t mean the sender can\u0026rsquo;t have a local send queue for chunks, just that in terms of network traffic there\u0026rsquo;s only ever one chunk in flight at any time.\nThis makes intuitive sense because the whole point of the chunk system is to send chunks reliably and in-order. If you are for some reason sending chunk 0 and chunk 1 at the same time, what\u0026rsquo;s the point? You can\u0026rsquo;t process chunk 1 until chunk 0 comes through, because otherwise it wouldn\u0026rsquo;t be reliable-ordered.\nThat said, if you dig a bit deeper you\u0026rsquo;ll see that sending one chunk at a time does introduce a small trade-off, and that is that it adds a delay of RTT between chunk n being received and the send starting for chunk n+1 from the receiver\u0026rsquo;s point of view.\nThis trade-off is totally acceptable for the occasional sending of large chunks like data sent once on client connect, but it\u0026rsquo;s definitely not acceptable for data sent 10 or 20 times per-second like snapshots. So remember, this system is useful for large, infrequently sent blocks of data, not for time critical data.\nPacket Structure There are two sides to the chunk system, the sender and the receiver.\nThe sender is the side that queues up the chunk and sends slices over the network. The receiver is what reads those slice packets and reassembles the chunk on the other side. The receiver is also responsible for communicating back to the sender which slices have been received via acks.\nThe netcode I work on is usually client/server, and in this case I usually want to be able to send blocks of data from the server to the client and from the client to the server. In that case, there are two senders and two receivers, a sender on the client corresponding to a receiver on the server and vice-versa.\nThink of the sender and receiver as end points for this chunk transmission protocol that define the direction of flow. If you want to send chunks in a different direction, or even extend the chunk sender to support peer-to-peer, just add sender and receiver end points for each direction you need to send chunks.\nTraffic over the network for this system is sent via two packet types:\nSlice packet - contains a slice of a chunk up to 1k in size. Ack packet - a bitfield indicating which slices have been received so far. The slice packet is sent from the sender to the receiver. It is the payload packet that gets the chunk data across the network and is designed so each packet fits neatly under a conservative MTU of 1200 bytes. Each slice is a maximum of 1k and there is a maximum of 256 slices per-chunk, therefore the largest data you can send over the network with this system is 256k.\nconst int SliceSize = 1024; const int MaxSlicesPerChunk = 256; const int MaxChunkSize = SliceSize * MaxSlicesPerChunk; struct SlicePacket : public protocol2::Packet { uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template \u0026amp;lt;typename Stream\u0026amp;gt; bool Serialize( Stream \u0026amp;amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) { serialize_int( stream, sliceBytes, 1, SliceSize ); } else if ( Stream::IsReading ) { sliceBytes = SliceSize; } serialize_bytes( stream, data, sliceBytes ); return true; } }; There are two points I\u0026rsquo;d like to make about the slice packet. The first is that even though there is only ever one chunk in flight over the network, it\u0026rsquo;s still necessary to include the chunk id (0,1,2,3, etc\u0026hellip;) because packets sent over UDP can be received out of order.\nSecond point. Due to the way chunks are sliced up we know that all slices except the last one must be SliceSize (1024 bytes). We take advantage of this to save a small bit of bandwidth sending the slice size only in the last slice, but there is a trade-off: the receiver doesn\u0026rsquo;t know the exact size of a chunk until it receives the last slice.\nThe other packet sent by this system is the ack packet. This packet is sent in the opposite direction, from the receiver back to the sender. This is the reliability part of the chunk network protocol. Its purpose is to lets the sender know which slices have been received.\nstruct AckPacket : public protocol2::Packet { uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream \u0026amp;amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i \u0026amp;lt; numSlices; ++i ) { serialize_bool( stream, acked[i] ); return true; } }; } } }; Acks are short for \u0026lsquo;acknowledgments\u0026rsquo;. So an ack for slice 100 means the receiver is acknowledging that it has received slice 100. This is critical information for the sender because not only does it let the sender determine when all slices have been received so it knows when to stop, it also allows the sender to use bandwidth more efficiently by only sending slices that haven\u0026rsquo;t been acked.\nLooking a bit deeper into the ack packet, at first glance it seems a bit redundant. Why are we sending acks for all slices in every packet? Well, ack packets are sent over UDP so there is no guarantee that all ack packets are going to get through. You certainly don\u0026rsquo;t want a desync between the sender and the receiver regarding which slices are acked.\nSo we need some reliability for acks, but we don\u0026rsquo;t want to implement an ack system for acks because that would be a huge pain in the ass. Since the worst case ack bitfield is just 256 bits or 32 bytes, we just send the entire state of all acked slices in each ack packet. When the ack packet is received, we consider a slice to be acked the instant an ack packet comes in with that slice marked as acked and locally that slice is not seen as acked yet.\nThis last step, biasing in the direction of non-acked to ack, like a fuse getting blown, means we can handle out of order delivery of ack packets.\nSender Implementation Let\u0026rsquo;s get started with the implementation of the sender.\nThe strategy for the sender is:\nKeep sending slices until all slices are acked Don\u0026rsquo;t resend slices that have already been acked We use the following data structure for the sender:\nclass ChunkSender { bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk]; }; As mentioned before, only one chunk is sent at a time, so there is a \u0026lsquo;sending\u0026rsquo; state which is true if we are currently sending a chunk, false if we are in an idle state ready for the user to send a chunk. In this implementation, you can\u0026rsquo;t send another chunk while the current chunk is still being sent over the network. If you don\u0026rsquo;t like this, stick a queue in front of the sender.\nNext, we have the id of the chunk we are currently sending, or, if we are not sending a chunk, the id of the next chunk to be sent, followed by the size of the chunk and the number of slices it has been split into. We also track, per-slice, whether that slice has been acked, which lets us count the number of slices that have been acked so far while ignoring redundant acks. A chunk is considered fully received from the sender\u0026rsquo;s point of view when numAckedSlices == numSlices.\nWe also keep track of the current slice id for the algorithm that determines which slices to send, which works like this. At the start of a chunk send, start at slice id 0 and work from left to right and wrap back around to 0 again when you go past the last slice. Eventually, you stop iterating across because you\u0026rsquo;ve run out of bandwidth to send slices. At this point, remember our current slice index via current slice id so you can pick up from where you left off next time. This last part is important because it distributes sends across all slices, not just the first few.\nNow let\u0026rsquo;s discuss bandwidth limiting. Obviously you don\u0026rsquo;t just blast slices out continuously as you\u0026rsquo;d flood the connection in no time, so how do we limit the sender bandwidth? My implementation works something like this: as you walk across slices and consider each slice you want to send, estimate roughly how many bytes the slice packet will take eg: roughly slice bytes + some overhead for your protocol and UDP/IP header. Then compare the amount of bytes required vs. the available bytes you have to send in your bandwidth budget. If you don\u0026rsquo;t have enough bytes accumulated, stop. Otherwise, subtract the bytes required to send the slice and repeat the process for the next slice.\nWhere does the available bytes in the send budget come from? Each frame before you update the chunk sender, take your target bandwidth (eg. 256kbps), convert it to bytes per-second, and add it multiplied by delta time (dt) to an accumulator.\nA conservative send rate of 256kbps means you can send 32000 bytes per-second, so add 32000 * dt to the accumulator. A middle ground of 512kbit/sec is 64000 bytes per-second. A more aggressive 1mbit is 125000 bytes per-second. This way each update you accumulate a number of bytes you are allowed to send, and when you\u0026rsquo;ve sent all the slices you can given that budget, any bytes left over stick around for the next time you try to send a slice.\nOne subtle point with the chunk sender and is that it\u0026rsquo;s a good idea to implement some minimum resend delay per-slice, otherwise you get situations where for small chunks, or the last few slices of a chunk that the same few slices get spammed over the network.\nFor this reason we maintain an array of last send time per-slice. One option for this resend delay is to maintain an estimate of RTT and to only resend a slice if it hasn\u0026rsquo;t been acked within RTT * 1.25 of its last send time. Or, you could just resend the slice it if it hasn\u0026rsquo;t been sent in the last 100ms. Works for me!\nKicking it up a notch Do the math you\u0026rsquo;ll notice it still takes a long time for a 256k chunk to get across:\n1mbps = 2 seconds 512kbps = 4 seconds 256kbps = 8 seconds :( Which kinda sucks. The whole point here is quickly and reliably. Emphasis on quickly. Wouldn\u0026rsquo;t it be nice to be able to get the chunk across faster? The typical use case of the chunk system supports this. For example, a large block of data sent down to the client immediately on connect or a block of data that has to get through before the client exits a load screen and starts to play. You want this to be over as quickly as possible and in both cases the user really doesn\u0026rsquo;t have anything better to do with their bandwidth, so why not use as much of it as possible?\nOne thing I\u0026rsquo;ve tried in the past with excellent results is an initial burst. Assuming your chunk size isn\u0026rsquo;t so large, and your chunk sends are infrequent, I can see no reason why you can\u0026rsquo;t just fire across the entire chunk, all slices of it, in separate packets in one glorious burst of bandwidth, wait 100ms, and then resume the regular bandwidth limited slice sending strategy.\nWhy does this work? In the case where the user has a good internet connection (some multiple of 10mbps or greater\u0026hellip;), the slices get through very quickly indeed. In the situation where the connection is not so great, the burst gets buffered up and most slices will be delivered as quickly as possible limited only by the amount bandwidth available. After this point switching to the regular strategy at a lower rate picks up any slices that didn\u0026rsquo;t get through the first time.\nThis seems a bit risky so let me explain. In the case where the user can\u0026rsquo;t quite support this bandwidth what you\u0026rsquo;re relying on here is that routers on the Internet strongly prefer to buffer packets rather than discard them at almost any cost. It\u0026rsquo;s a TCP thing. Normally, I hate this because it induces latency in packet delivery and messes up your game packets which you want delivered as quickly as possible, but in this case it\u0026rsquo;s good behavior because the player really has nothing else to do but wait for your chunk to get through.\nJust don\u0026rsquo;t go too overboard with the spam or the congestion will persist after your chunk send completes and it will affect your game for the first few seconds. Also, make sure you increase the size of your OS socket buffers on both ends so they are larger than your maximum chunk size (I recommend at least double), otherwise you\u0026rsquo;ll be dropping slices packets before they even hit the wire.\nFinally, I want to be a responsible network citizen here so although I recommend sending all slices once in an initial burst, it\u0026rsquo;s important for me to mention that I think this really is only appropriate, and only really borderline appropriate behavior for small chunks in the few 100s of k range in 2016, and only when your game isn\u0026rsquo;t sending anything else that is time-critical.\nPlease don\u0026rsquo;t use this burst strategy if your chunk is really large, eg: megabytes of data, because that\u0026rsquo;s way too big to be relying on the kindness of strangers, AKA. the buffers in the routers between you and your packet\u0026rsquo;s destination. For this it\u0026rsquo;s necessary to implement something much smarter. Something adaptive that tries to send data as quickly as it can, but backs off when it detects too much latency and/or packet loss as a result of flooding the connection. Such a system is outside of the scope of this article.\nReceiver Implementation Now that we have the sender all sorted out let\u0026rsquo;s move on to the reciever. As mentioned previously, unlike the packet fragmentation and reassembly system from the previous article, the chunk system only ever has one chunk in flight.\nThis makes the reciever side of the chunk system much simpler:\nclass ChunkReceiver { bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; }; We have a state whether we are currently \u0026lsquo;receiving\u0026rsquo; a chunk over the network, plus a \u0026lsquo;readyToRead\u0026rsquo; state which indicates that a chunk has received all slices and is ready to be popped off by the user. This is effectively a minimal receive queue of length 1. If you don\u0026rsquo;t like this, of course you are free to add a queue.\nIn this data structure we also keep track of chunk size (although it is not known with complete accuracy until the last slice arrives), num slices and num received slices, as well as a received flag per-slice. This per-slice received flag lets us discard packets containing slices we have already received, and count the number of slices received so far (since we may receive the slice multiple times, we only increase this count the first time we receive a particular slice). It\u0026rsquo;s also used when generating ack packets. The chunk receive is completed from the receiver\u0026rsquo;s point of view when numReceivedSlices == numSlices.\nSo what does it look like end-to-end receiving a chunk?\nFirst, the receiver sets up set to start at chunk 0. When the a slice packet comes in over the network matching the chunk id 0, \u0026lsquo;receiving\u0026rsquo; flips from false to true, data for that first slice is inserted into \u0026lsquo;chunkData\u0026rsquo; at the correct position, numSlices is set to the value in that packet, numReceivedSlices is incremented from 0 -\u0026gt; 1, and the received flag in the array entry corresponding to that slice is set to true.\nAs the remaining slice packets for the chunk come in, each of them are checked that they match the current chunk id and numSlices that are being received and are ignored if they don\u0026rsquo;t match. Packets are also ignored if they contain a slice that has already been received. Otherwise, the slice data is copied into the correct place in the chunkData array, numReceivedSlices is incremented and received flag for that slice is set to true.\nThis process continues until all slices of the chunk are received, at which point the receiver sets receiving to \u0026lsquo;false\u0026rsquo; and \u0026lsquo;readyToRead\u0026rsquo; to true. While \u0026lsquo;readyToRead\u0026rsquo; is true, incoming slice packets are discarded. At this point, the chunk receive packet processing is performed, typically on the same frame. The caller checks \u0026lsquo;do I have a chunk to read?\u0026rsquo; and processes the chunk data. All chunk receive data is cleared back to defaults, except chunk id which is incremented from 0 -\u0026gt; 1, and we are ready to receive the next chunk.\nConclusion The chunk system is simple in concept, but the implementation is certainly not. I encourage you to take a close look at the source code for this article for further details.\nNEXT ARTICLE: Reliable Ordered Messages\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1473638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473638400,"objectID":"6f74022e949bef18091eb34bdbba0259","permalink":"https://gafferongames.com/post/sending_large_blocks_of_data/","publishdate":"2016-09-12T00:00:00Z","relpermalink":"/post/sending_large_blocks_of_data/","section":"post","summary":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn the previous article we implemented packet fragmentation and reassembly so we can send packets larger than MTU.\nThis approach works great when the data block you\u0026rsquo;re sending is time critical and can be dropped, but in other cases you need to send large blocks of quickly and reliably over packet loss, and you need the data to get through.","tags":["networking"],"title":"Sending Large Blocks of Data","type":"post"},{"authors":null,"categories":["Building a Game Network Protocol"],"content":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn the previous article we discussed how to unify packet read and write into a single serialize function and added a bunch of safety features to packet read.\nNow we are ready to start putting interesting things in our packets and sending them over the network, but immediately we run into an interesting question: how big should our packets be?\nTo answer this question properly we need a bit of background about how packets are actually sent over the Internet.\nBackground Perhaps the most important thing to understand about the internet is that there\u0026rsquo;s no direct connection between the source and destination IP address. What actually happens is that packets hop from one computer to another to reach their destination.\nEach computer along this route enforces a maximum packet size called the maximum transmission unit, or MTU. According to the IP standard, if any computer recieves a packet larger than its MTU, it has the option of a) fragmenting that packet, or b) dropping the packet.\nSo here\u0026rsquo;s how this usually goes down. People write a multiplayer game where the average packet size is quite small, lets say a few hundred bytes, but every now and then when a lot of stuff is happening in their game and a burst of packet loss occurs, packets get a lot larger than usual, going above MTU for the route, and suddenly all packets start getting dropped!\nJust last year (2015) I was talking with Alex Austin at Indiecade about networking in his game Sub Rosa. He had this strange networking bug he couldn\u0026rsquo;t reproduce. For some reason, players would randomly get disconnected from the game, but only when a bunch of stuff was going on. It was extremely rare and he was unable to reproduce it. Alex told me looking at the logs it seemed like packets just stopped getting through.\nThis sounded exactly like an MTU issue to me, and sure enough, when Alex limited his maximum packet size to a reasonable value the bug went away.\nMTU in the real world So what\u0026rsquo;s a reasonable maximum packet size?\nOn the Internet today (2016, IPv4) the real-world MTU is 1500 bytes.\nGive or take a few bytes for UDP/IP packet header and you\u0026rsquo;ll find that the typical number before packets start to get dropped or fragmented is somewhere around 1472.\nYou can try this out for yourself by running this command on MacOS X:\nping -g 56 -G 1500 -h 10 -D 8.8.4.4 On my machine it conks out around just below 1500 bytes as expected:\n1404 bytes from 8.8.4.4: icmp_seq=134 ttl=56 time=11.945 ms 1414 bytes from 8.8.4.4: icmp_seq=135 ttl=56 time=11.964 ms 1424 bytes from 8.8.4.4: icmp_seq=136 ttl=56 time=13.492 ms 1434 bytes from 8.8.4.4: icmp_seq=137 ttl=56 time=13.652 ms 1444 bytes from 8.8.4.4: icmp_seq=138 ttl=56 time=133.241 ms 1454 bytes from 8.8.4.4: icmp_seq=139 ttl=56 time=17.463 ms 1464 bytes from 8.8.4.4: icmp_seq=140 ttl=56 time=12.307 ms 1474 bytes from 8.8.4.4: icmp_seq=141 ttl=56 time=11.987 ms ping: sendto: Message too long ping: sendto: Message too long Request timeout for icmp_seq 142 Why 1500? That\u0026rsquo;s the default MTU for MacOS X. It\u0026rsquo;s also the default MTU on Windows. So now we have an upper bound for your packet size assuming you actually care about packets getting through to Windows and Mac boxes without IP level fragmentation or a chance of being dropped: 1472 bytes.\nSo what\u0026rsquo;s the lower bound? Unfortunately for the routers in between your computer and the destination the IPv4 standard says 576. Does this mean we have to limit our packets to 400 bytes or less? In practice, not really.\nMacOS X lets me set MTU values in range 1280 to 1500 so considering packet header overhead, my first guess for a conservative lower bound on the IPv4 Internet today would be 1200 bytes. Moving forward, in IPv6 this is also a good value, as any packet of 1280 bytes or less is guaranteed to get passed on without IP level fragmentation.\nThis lines up with numbers that I\u0026rsquo;ve seen throughout my career. In my experience games rarely try anything complicated like attempting to discover path MTU, they just assume a reasonably conservative MTU and roll with that, something like 1000 to 1200 bytes of payload data. If a packet larger than this needs to be sent, it\u0026rsquo;s split up into fragments by the game protocol and re-assembled on the other side.\nAnd that\u0026rsquo;s exactly what I\u0026rsquo;m going to show you how to do in this article.\nFragment Packet Structure Let\u0026rsquo;s get started with implementation.\nThe first thing we need to decide is how we\u0026rsquo;re going to represent fragment packets over the network so they are distinct from non-fragmented packets.\nIdeally, we would like fragmented and non-fragmented packets to be compatible with the existing packet structure we\u0026rsquo;ve already built, with as little overhead as possible in the common case when we are sending packets smaller than MTU.\nHere\u0026rsquo;s the packet structure from the previous article:\n[protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [packet type] (2 bits for 3 distinct packet types) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) In our protocol we have three packet types: A, B and C.\nLet\u0026rsquo;s make one of these packet types generate really large packets:\nstatic const int MaxItems = 4096 * 4; struct TestPacketB : public Packet { int numItems; int items[MaxItems]; TestPacketB() : Packet( TEST_PACKET_B ) { numItems = random_int( 0, MaxItems ); for ( int i = 0; i \u0026lt; numItems; ++i ) items[i] = random_int( -100, +100 ); } template \u0026lt;typename Stream\u0026gt; bool Serialize( Stream \u0026amp; stream ) { serialize_int( stream, numItems, 0, MaxItems ); for ( int i = 0; i \u0026lt; numItems; ++i ) { serialize_int( stream, items[i], -100, +100 ); } return true; } }; This may seem somewhat contrived but these situations really do occur. For example, if you have a strategy where you send all un-acked events from server to client and you hit a burst of packet loss, you can easily end up with packets larger than MTU, even though your average packet size is quite small.\nAnother common case is delta encoded snapshots in a first person shooter. Here packet size is proportional to the amount of state changed between the baseline and current snapshots for each client. If there are a lot of differences between the snapshots the delta packet is large and there\u0026rsquo;s nothing you can do about it except break it up into fragments and re-assemble them on the other side.\nGetting back to packet structure. It\u0026rsquo;s fairly common to add a sequence number at the header of each packet. This is just a packet number that increases with each packet sent. I like to use 16 bits for sequence numbers even though they wrap around in about 15 minutes @ 60 packets-per-second, because it\u0026rsquo;s extremely unlikely that a packet will be delivered 15 minutes late.\nSequence numbers are useful for a bunch of things like acks, reliability and detecting and discarding out of order packets. In our case, we\u0026rsquo;re going to use the sequence number to identify which packet a fragment belongs to:\n[protocol id] (64 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type] (2 bits) (variable length packet data according to packet type) [end of packet serialize check] (32 bits) Here\u0026rsquo;s the interesting part. Sure we could just add a bit is_fragment to the header, but then in the common case of non-fragmented packets you\u0026rsquo;re wasting one bit that is always set to zero.\nWhat I do instead is add a special fragment packet type:\nenum TestPacketTypes { PACKET_FRAGMENT = 0, // RESERVED TEST_PACKET_A, TEST_PACKET_B, TEST_PACKET_C, TEST_PACKET_NUM_TYPES }; And it just happens to be free because four packet types fit into 2 bits. Now when a packet is read, if the packet type is zero we know it\u0026rsquo;s a fragment packet, otherwise we run through the ordinary, non-fragmented read packet codepath.\nLets design what this fragment packet looks like. We\u0026rsquo;ll allow a maximum of 256 fragments per-packet and have a fragment size of 1024 bytes. This gives a maximum packet size of 256k that we can send through this system, which should be enough for anybody, but please don\u0026rsquo;t quote me on this.\nWith a small fixed size header, UDP header and IP header a fragment packet be well under the conservative MTU value of 1200. Plus, with 256 max fragments per-packet we can represent a fragment id in the range [0,255] and the total number of fragments per-packet [1,256] with 8 bits.\n[protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) [packet type = 0] (2 bits) [fragment id] (8 bits) [num fragments] (8 bits) [pad zero bits to nearest byte index] \u0026lt;fragment data\u0026gt; Notice that we pad bits up to the next byte before writing out the fragment data. Why do this? Two reasons: 1) it\u0026rsquo;s faster to copy fragment data into the packet via memcpy than bitpacking each byte, and 2) we can now save a small amount of bandwidth by inferring the fragment size by subtracting the start of the fragment data from the total size of the packet.\nSending Packet Fragments Sending packet fragments is easy. For any packet larger than conservative MTU, simply calculate how many 1024 byte fragments it needs to be split into, and send those fragment packets over the network. Fire and forget!\nOne consequence of this is that if any fragment of that packet is lost then the entire packet is lost. It follows that if you have packet loss then sending a 256k packet as 256 fragments is not a very good idea, because the probability of dropping a packet increases significantly as the number of fragments increases. Not quite linearly, but in an interesting way that you can read more about here.\nIn short, to calculate the probability of losing a packet, you must calculate the probability of all fragments being delivered successfully and subtract that from one, giving you the probability that at least one fragment was dropped.\n1 - probability_of_fragment_being_delivered ^ num_fragments For example, if we send a non-fragmented packet over the network with 1% packet loss, there is naturally a 1/100 chance the packet will be dropped.\nAs the number of fragments increase, packet loss is amplified:\nTwo fragments: 1 - (99/100) ^ 2 = 2% Ten fragments: 1 - (99/100) ^ 10 = 9.5% 100 fragments: 1 - (99/100) ^ 100 = 63.4% 256 fragments: 1 - (99/100) ^ 256 = 92.4% So I recommend you take it easy with the number of fragments. It\u0026rsquo;s best to use this strategy only for packets in the 2-4 fragment range, and only for time critical data that doesn\u0026rsquo;t matter too much if it gets dropped. It\u0026rsquo;s definitely not a good idea to fire down a bunch of reliable-ordered events in a huge packet and rely on packet fragmentation and reassembly to save your ass.\nAnother typical use case for large packets is when a client initially joins a game. Here you usually want to send a large block of data down reliably to that client, for example, representing the initial state of the world for late join. Whatever you do, don\u0026rsquo;t send that block of data down using the fragmentation and re-assembly technique in this article.\nInstead, check out the technique in next article which handles packet loss by resending fragments until they are all received.\nReceiving Packet Fragments It\u0026rsquo;s time to implement the code that receives and processed packet fragments. This is a bit tricky because we have to be particularly careful of somebody trying to attack us with malicious packets.\nHere\u0026rsquo;s a list of all the ways I can think of to attack the protocol:\nTry to send out of bound fragments ids trying to get you to crash memory. eg: send fragments [0,255] in a packet that has just two fragments.\nSend packet n with some maximum fragment count of say 2, and then send more fragment packets belonging to the same packet n but with maximum fragments of 256 hoping that you didn\u0026rsquo;t realize I widened the maximum number of fragments in the packet after the first one you received, and you trash memory.\nSend really large fragment packets with fragment data larger than 1k hoping to get you to trash memory as you try to copy that fragment data into the data structure, or blow memory budget trying to allocate fragments\nContinually send fragments of maximum size (256/256 fragments) in hope that it I could make you allocate a bunch of memory and crash you out. Lets say you have a sliding window of 256 packets, 256 fragments per-packet max, and each fragment is 1k. That\u0026rsquo;s 64 mb per-client.\nCan I fragment the heap with a bunch of funny sized fragment packets sent over and over? Perhaps the server shares a common allocator across clients and I can make allocations fail for other clients in the game because the heap becomes fragmented.\nAside from these concerns, implementation is reasonably straightforward: store received fragments somewhere and when all fragments arrive for a packet, reassemble them into the original packet and return that to the user.\nData Structure on Receiver Side The first thing we need is some way to store fragments before they are reassembled. My favorite data structure is something I call a sequence buffer:\nconst int MaxEntries = 256; struct SequenceBuffer { uint32_t sequence[MaxEntries]; Entry entries[MaxEntries]; }; Indexing into the arrays is performed with modulo arithmetic, giving us a fast O(1) lookup of entries by sequence number:\nconst int index = sequence % MaxEntries; A sentinel value of 0xFFFFFFFF is used to represent empty entries. This value cannot possibly occur with 16 bit sequence numbers, thus providing us with a fast test to see if an entry exists for a given sequence number, without an additional branch to test if that entry exists.\nThis data structure is used as follows. When the first fragment of a new packet comes in, the sequence number is mapped to an entry in the sequence buffer. If an entry doesn\u0026rsquo;t exist, it\u0026rsquo;s added and the fragment data is stored in there, along with information about the fragment, eg. how many fragments there are, how many fragments have been received so far, and so on.\nEach time a new fragment arrives, it looks up the entry by the packet sequence number. When an entry already exists, the fragment data is stored and number of fragments received is incremented. Eventually, once the number of fragments received matches the number of fragments in the packet, the packet is reassembled and delivered to the user.\nSince it\u0026rsquo;s possible for old entries to stick around (potentially with allocated blocks), great care must be taken to clean up any stale entries when inserting new entries in the sequence buffer. These stale entries correspond to packets that didn\u0026rsquo;t receive all fragments.\nAnd that\u0026rsquo;s basically it at a high level. For further details on this approach please refer to the example source code for this article. Click here to get the example source code for this article series.\nTest Driven Development One thing I\u0026rsquo;d like to close this article out on.\nWriting a custom UDP network protocol is hard. It\u0026rsquo;s so hard that even though I\u0026rsquo;ve done this from scratch at least 10 times, each time I still manage to fuck it up in a new and exciting ways. You\u0026rsquo;d think eventually I\u0026rsquo;d learn, but this stuff is complicated. You can\u0026rsquo;t just write low-level netcode and expect it to just work.\nYou have to test it!\nMy strategy when testing low-level netcode is as follows:\nCode defensively. Assert everywhere. These asserts will fire and they\u0026rsquo;ll be important clues you need when something goes wrong.\nAdd functional tests and make sure stuff is working as you are writing it. Put your code through its paces at a basic level as you write it and make sure it\u0026rsquo;s working as you build it up. Think hard about the essential cases that need to be property handled and add tests that cover them.\nBut just adding a bunch of functional tests is not enough. There are of course cases you didn\u0026rsquo;t think of! Now you have to get really mean. I call this soak testing and I\u0026rsquo;ve never, not even once, have coded a network protocol that hasn\u0026rsquo;t subsequently had problems found in it by soak testing.\nWhen soak testing just loop forever and just do a mix of random stuff that puts your system through its paces, eg. random length packets in this case with a huge amount of packet loss, out of order and duplicates through a packet simulator. Your soak test passes when it runs overnight and doesn\u0026rsquo;t hang or assert.\nIf you find anything wrong with soak testing. You may need to go back and add detailed logs to the soak test to work out how you got to the failure case. Once you know what\u0026rsquo;s going on, stop. Don\u0026rsquo;t fix it immediately and just run the soak test again.\nInstead, add a unit test that reproduces that problem you are trying to fix, verify your test reproduces the problem, and that it problem goes away with your fix. Only after this, go back to the soak test and make sure they run overnight. This way the unit tests document the correct behavior of your system and can quickly be run in future to make sure you don\u0026rsquo;t break this thing moving forward when you make other changes.\nAdd a bunch of logs. High level errors, info asserts showing an overview of what is going on, but also low-level warnings and debug logs that show what went wrong after the fact. You\u0026rsquo;re going to need these logs to diagnose issues that don\u0026rsquo;t occur on your machine. Make sure the log level can be adjusted dynamically.\nImplement network simulators and make sure code handles the worst possible network conditions imaginable. 99% packet loss, 10 seconds of latency and +/- several seconds of jitter. Again, you\u0026rsquo;ll be surprised how much this uncovers. Testing is the time where you want to uncover and fix issues with bad network conditions, not the night before your open beta.\nImplement fuzz tests where appropriate to make sure your protocol doesn\u0026rsquo;t crash when processing random packets. Leave fuzz tests running overnight to feel confident that your code is reasonably secure against malicious packets and doesn\u0026rsquo;t crash.\nSurprisingly, I\u0026rsquo;ve consistently found issues that only show up when I loop the set of unit tests over and over, perhaps these issues are caused by different random numbers in tests, especially with the network simulator being driven by random numbers. This is a great way to take a rare test that fails once every few days and make it fail every time. So before you congratulate yourself on your tests passing 100%, add a mode where your unit tests can be looped easily, to uncover such errors.\nTest simultaneously on multiple platforms. I\u0026rsquo;ve never written a low-level library that worked first time on MacOS, Windows and Linux. There are always interesting compiler specific issues and crashes. Test on multiple platforms as you develop, otherwise it\u0026rsquo;s pretty painful fixing all these at the end.\nThink about how people can attack the protocol. Implement code to defend against these attacks. Add functional tests that mimic these attacks and make sure that your code handles them correctly.\nThis is my process and it seems to work pretty well. If you are writing a low-level network protocol, the rest of your game depends on this code working correctly. You need to be absolutely sure it works before you build on it, otherwise it\u0026rsquo;s basically a stack of cards.\nIn my experience, game neworking is hard enough without having suspicions that that your low-level network protocol has bugs that only show up under extreme network conditions. That\u0026rsquo;s exactly where you need to be able to trust your code works correctly. So test it!\nNEXT ARTICLE: Sending Large Blocks of Data\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1473120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473120000,"objectID":"bf104dc01c948a9b86ac65dfd4d3946e","permalink":"https://gafferongames.com/post/packet_fragmentation_and_reassembly/","publishdate":"2016-09-06T00:00:00Z","relpermalink":"/post/packet_fragmentation_and_reassembly/","section":"post","summary":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn the previous article we discussed how to unify packet read and write into a single serialize function and added a bunch of safety features to packet read.\nNow we are ready to start putting interesting things in our packets and sending them over the network, but immediately we run into an interesting question: how big should our packets be?","tags":["networking"],"title":"Packet Fragmentation and Reassembly","type":"post"},{"authors":null,"categories":["Building a Game Network Protocol"],"content":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn the previous article, we created a bitpacker but it required manual checking to make sure reading a packet from the network is safe. This is a real problem because the stakes are particularly high - a single missed check creates a vulnerability that an attacker can use to crash your server.\nIn this article, we\u0026rsquo;re going to transform the bitpacker into a system where this checking is automatic. We\u0026rsquo;re going to do this with minimal runtime overhead, and in such a way that we don\u0026rsquo;t have to code separate read and write functions, performing both read and write with a single function.\nThis is called a serialize function.\nSerializing Bits Let\u0026rsquo;s start with the goal. Here\u0026rsquo;s where we want to end up:\nstruct PacketA { int x,y,z; template \u0026lt;typename Stream\u0026gt; bool Serialize( Stream \u0026amp; stream ) { serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; } }; Above you can see a simple serialize function. We serialize three integer variables x,y,z with 32 bits each.\nstruct PacketB { int numElements; int elements[MaxElements]; template \u0026lt;typename Stream\u0026gt; bool Serialize( Stream \u0026amp; stream ) { serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i \u0026lt; numElements; ++i ) { serialize_bits( buffer, elements[i], 32 ); } return true; } }; And now something more complicated. We serialize a variable length array, making sure that the array length is in the range [0,MaxElements].\nNext, we serialize a rigid body with an simple optimization while it\u0026rsquo;s at rest, serializing only one bit in place of linear and angular velocity:\nstruct RigidBody { vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity; template \u0026lt;typename Stream\u0026gt; bool Serialize( Stream \u0026amp; stream ) { serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? ( velocity.length() == 0 ) : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); } else if ( Stream::IsReading ) { linear_velocity = vec3f(0,0,0); angular_velocity = vec3f(0,0,0); } return true; } }; Notice how we\u0026rsquo;re able to branch on Stream::IsWriting and Stream::IsReading to write code for each case. These branches are removed by the compiler when the specialized read and write serialize functions are generated.\nAs you can see, serialize functions are flexible and expressive. They\u0026rsquo;re also safe, with each serialize*_ call performing checks and aborting read if anything is wrong (eg. a value out of range, going past the end of the buffer). Most importantly, this checking is automatic, so you can\u0026rsquo;t forget to do it!\nImplementation in C++ The trick to making this all work is to create two stream classes that share the same interface: ReadStream and WriteStream.\nThe write stream implementation writes values using the bitpacker:\nclass WriteStream { public: enum { IsWriting = 1 }; enum { IsReading = 0 }; WriteStream( uint8_t * buffer, int bytes ) : m_writer( buffer, bytes ) {} bool SerializeInteger( int32_t value, int32_t min, int32_t max ) { assert( min \u003c max ); assert( value \u003e= min ); assert( value \u003c= max ); const int bits = bits_required( min, max ); uint32_t unsigned_value = value - min; m_writer.WriteBits( unsigned_value, bits ); return true; } // ... private: BitWriter m_writer; }; And the read stream implementation reads values in:\nclass ReadStream { public: enum { IsWriting = 0 }; enum { IsReading = 1 }; ReadStream( const uint8_t * buffer, int bytes ) : m_reader( buffer, bytes ) {} bool SerializeInteger( int32_t \u0026 value, int32_t min, int32_t max ) { assert( min \u003c max ); const int bits = bits_required( min, max ); if ( m_reader.WouldReadPastEnd( bits ) ) { return false; } uint32_t unsigned_value = m_reader.ReadBits( bits ); value = (int32_t) unsigned_value + min; return true; } // ... private: BitReader m_reader; }; With the magic of C++ templates, we leave it up to the compiler to specialize the serialize function to the stream class passed in, producing optimized read and write functions.\nTo handle safety serialize*_ calls are not actually functions at all. They\u0026rsquo;re actually macros that return false on error, thus unwinding the stack in case of error, without the need for exceptions.\nFor example, this macro serializes an integer in a given range:\n#define serialize_int( stream, value, min, max ) \\ do \\ { \\ assert( min \u0026lt; max ); \\ int32_t int32_value; \\ if ( Stream::IsWriting ) \\ { \\ assert( value \u0026gt;= min ); \\ assert( value \u0026lt;= max ); \\ int32_value = (int32_t) value; \\ } \\ if ( !stream.SerializeInteger( int32_value, min, max ) ) \\ { \\ return false; \\ } \\ if ( Stream::IsReading ) \\ { \\ value = int32_value; \\ if ( value \u0026lt; min || value \u0026gt; max ) \\ { \\ return false; \\ } \\ } \\ } while (0) If a value read in from the network is outside the expected range, or we read past the end of the buffer, the packet read is aborted.\nSerializing Floating Point Values We\u0026rsquo;re used to thinking about floating point numbers as being different to integers, but in memory they\u0026rsquo;re just a 32 bit value like any other.\nThe C++ language lets us work with this fundamental property, allowing us to directly access the bits of a float value as if it were an integer:\nunion FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; tmp.float_value = 10.0f; printf( \"float value as an integer: %x\\n\", tmp.int_value ); You may prefer to do this with an aliased uint32_t* pointer, but this breaks with GCC -O2. Friends of mine point out that the only truly standard way to get the float as an integer is to cast a pointer to the float value to char* and reconstruct the integer from the bytes values accessed through the char pointer.\nMeanwhile in the past 5 years I\u0026rsquo;ve had no problems in the field with the union trick. Here\u0026rsquo;s how I use it to serialize an uncompressed float value:\ntemplate \u0026lt;typename Stream\u0026gt; bool serialize_float_internal( Stream \u0026amp; stream, float \u0026amp; value ) { union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; if ( Stream::IsWriting ) { tmp.float_value = value; } bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) { value = tmp.float_value; } return result; } This is of course wrapped with a serialize_float macro for error checking:\n#define serialize_float( stream, value ) \\ do \\ { \\ if ( !serialize_float_internal( stream, value ) ) \\ { \\ return false; \\ } } while (0) We can now transmit full precision floating point values over the network.\nBut what about situations where you don\u0026rsquo;t need full precision? What about a floating point value in the range [0,10] with an acceptable precision of 0.01? Is there a way to send this over the network using less bits?\nYes there is. The trick is to simply divide by 0.01 to get an integer in the range [0,1000] and send that value over the network. On the other side, convert back to a float by multiplying by 0.01.\nHere\u0026rsquo;s a general purpose implementation of this basic idea:\ntemplate \u0026lt;typename Stream\u0026gt; bool serialize_compressed_float_internal( Stream \u0026amp; stream, float \u0026amp; value, float min, float max, float res ) { const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) { float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue * maxIntegerValue + 0.5f ); } if ( !stream.SerializeBits( integerValue, bits ) ) { return false; } if ( Stream::IsReading ) { const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue * delta + min; } return true; } Of course we need error checking, so we wrap this with a macro:\n#define serialize_compressed_float( stream, value, min, max ) \\ do \\ { \\ if ( !serialize_float_internal( stream, value, min, max ) ) \\ { \\ return false; \\ } \\ } while (0) And now the basic interface is complete. We can serialize both compressed and uncompressed floating point values over the network.\nSerializing Vectors and Quaternions Once you can serialize float values it\u0026rsquo;s trivial to serialize vectors over the network. I use a modified version of the vectorial library in my projects and implement serialization for its vector type like this:\ntemplate \u0026lt;typename Stream\u0026gt; bool serialize_vector_internal( Stream \u0026amp; stream, vec3f \u0026amp; vector ) { float values[3]; if ( Stream::IsWriting ) { vector.store( values ); } serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) { vector.load( values ); } return true; } #define serialize_vector( stream, value ) \\ do \\ { \\ if ( !serialize_vector_internal( stream, value ) ) \\ { \\ return false; \\ } \\ } \\ while(0) If your vector is bounded in some range, then you can compress it:\ntemplate \u0026lt;typename Stream\u0026gt; bool serialize_compressed_vector_internal( Stream \u0026amp; stream, vec3f \u0026amp; vector, float min, float max, float res ) { float values[3]; if ( Stream::IsWriting ) { vector.store( values ); } serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) { vector.load( values ); } return true; } Notice how we are able to build more complex serialization using the primitives we\u0026rsquo;re already created. Using this approach you can easily extend the serialization to support anything you need.\nSerializing Strings and Arrays What if you need to serialize a string over the network?\nIs it a good idea to send a string over the network with null termination? Not really. You\u0026rsquo;re just asking for trouble! Instead, serialize the string as an array of bytes with the string length in front. Therefore, in order to send a string over the network, we have to work out how to send an array of bytes.\nFirst observation. Why waste effort bitpacking an array of bytes into your bit stream just so they are randomly shifted by [0,7] bits? Why not align to byte so you can memcpy the array of bytes directly into the packet?\nTo align a bitstream just work out your current bit index in the stream and how many bits of padding are needed until the current bit index divides evenly into 8, then insert that number of padding bits. For bonus points, pad up with zero bits to add entropy so that on read you can verify that yes, you are reading a byte align and yes, it is indeed padded up with zero bits to the next whole byte bit index. If a non-zero bit is discovered in the padding, abort serialize read and discard the packet.\nHere\u0026rsquo;s my code to align a bit stream to byte:\nvoid BitWriter::WriteAlign() { const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) { uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); } } bool BitReader::ReadAlign() { const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) { uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; } return true; } #define serialize_align( stream ) \\ do \\ { \\ if ( !stream.SerializeAlign() ) \\ return false; \\ } while (0) Now we can align to byte prior to writing an array of bytes, letting us use memcpy for the bulk of the array data. The only wrinkle is because the bitpacker works at the word level, it\u0026rsquo;s necessary to have special handling for the head and tail portions. Because of this, the code is quite complex and is omitted for brevity. You can find it in the sample code for this article.\nThe end result of all this is a serialize_bytes primitive that we can use to serialize a string as a length followed by the string data, like so:\ntemplate \u0026lt;typename Stream\u0026gt; bool serialize_string_internal( Stream \u0026amp; stream, char * string, int buffer_size ) { uint32_t length; if ( Stream::IsWriting ) { length = strlen( string ); assert( length \u0026lt; buffer_size - 1 ); } serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t*)string, length ); if ( Stream::IsReading ) { string[length] = '\\0'; } } #define serialize_string( stream, string, buffer_size ) \\ do \\ { \\ if ( !serialize_string_internal( stream, \\ string, \\ buffer_size ) ) \\ { \\ return false; \\ } \\ } while (0) This is an ideal string format because it lets us quickly reject malicious data, vs. having to scan through to the end of the packet searching for \u0026rsquo;\\0\u0026rsquo; before giving up. This is important because otherwise protocol level attacks could be crafted to degrade your server\u0026rsquo;s performance by making it do extra work.\nSerializing Array Subsets When implemeting a game network protocol, sooner or later you need to serialize an array of objects over the network. Perhaps the server needs to send object state down to the client, or there is an array of messages to be sent.\nThis is straightforward if you are sending all objects in the array - just iterate across the array and serialize each object in turn. But what if you want to send a subset of the array?\nThe simplest approach is to iterate across all objects in the array and serialize a bit per-object if that object is to be sent. If the value of the bit is 1 then the object data follows in the bit stream, otherwise it\u0026rsquo;s ommitted:\ntemplate \u0026lt;typename Stream\u0026gt; bool serialize_scene_a( Stream \u0026amp; stream, Scene \u0026amp; scene ) { for ( int i = 0; i \u0026lt; MaxObjects; ++i ) { serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) { if ( Stream::IsReading ) { memset( \u0026amp;scene.objects[i], 0, sizeof( Object ) ); } continue; } serialize_object( stream, scene.objects[i] ); } return true; } This approach breaks down as the size of the array gets larger. For example, for an array size of size 4096, then 4096 / 8 = 512 bytes spent on skip bits. That\u0026rsquo;s not good. Can we switch it around so we take overhead propertional to the number of objects sent instead of the total number of objects in the array?\nWe can but now, we\u0026rsquo;ve done something interesting. We\u0026rsquo;re walking one set of objects in the serialize write (all objects in the array) and are walking over a different set of objects in the serialize read (subset of objects sent).\nAt this point the unified serialize function concept starts to breaks down, and in my opinion, it\u0026rsquo;s best to separate the read and write back into separate functions, because they have so little in common:\nbool write_scene_b( WriteStream \u0026amp; stream, Scene \u0026amp; scene ) { int num_objects_sent = 0; for ( int i = 0; i \u0026lt; MaxObjects; ++i ) { if ( scene.objects[i].send ) num_objects_sent++; } write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i \u0026lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); } return true; } bool read_scene_b( ReadStream \u0026amp; stream, Scene \u0026amp; scene ) { memset( \u0026amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i \u0026lt; num_objects_sent; ++i ) { int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); } return true; } One more point. The code above walks over the set of objects twice on serialize write. Once to determine the number of changed objects and a second time to actually serialize the set of changed objects. Can we do it in one pass instead? Absolutely! You can use another trick, rather than serializing the # of objects in the array up front, use a sentinel value to indicate the end of the array:\nbool write_scene_c( WriteStream \u0026amp; stream, Scene \u0026amp; scene ) { for ( int i = 0; i \u0026lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); } write_int( stream, MaxObjects, 0, MaxObjects ); return true; } bool read_scene_c( ReadStream \u0026amp; stream, Scene \u0026amp; scene ) { memset( \u0026amp;scene, 0, sizeof( scene ) ); while ( true ) { int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) { break; } read_object( stream, scene.objects[index] ); } return true; } The above technique works great if the objects sent are a small percentage of total objects. But what if a large number of objects are sent, lets say half of the 4000 objects in the scene. That\u0026rsquo;s 2000 object indices with each index costing 12 bits\u0026hellip; that\u0026rsquo;s 24000 bits or 3000 bytes (almost 3k!) in your packet wasted on indexing.\nYou can reduce this overhead by encoding each object index relative to the previous object index. Think about it, you\u0026rsquo;re walking from left to right along an array, so object indices start at 0 and go up to MaxObjects - 1. Statistically speaking, you\u0026rsquo;re quite likely to have objects that are close to each other and if the next index is +1 or even +10 or +30 from the previous one, on average, you\u0026rsquo;ll need quite a few less bits to represent that difference than an absolute index.\nHere\u0026rsquo;s one way to encode the object index as an integer relative to the previous object index, while spending less bits on statistically more likely values:\ntemplate \u0026lt;typename Stream\u0026gt; bool serialize_object_index_internal( Stream \u0026amp; stream, int \u0026amp; previous, int \u0026amp; current ) { uint32_t difference; if ( Stream::IsWriting ) { assert( previous \u0026lt; current ); difference = current - previous; assert( difference \u0026gt; 0 ); } // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) { plusOne = difference == 1; } serialize_bool( stream, plusOne ); if ( plusOne ) { if ( Stream::IsReading ) { current = previous + 1; } previous = current; return true; } // [+2,5] -\u0026gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) { twoBits = difference \u0026lt;= 5; } serialize_bool( stream, twoBits ); if ( twoBits ) { serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [6,13] -\u0026gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) { threeBits = difference \u0026lt;= 13; } serialize_bool( stream, threeBits ); if ( threeBits ) { serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [14,29] -\u0026gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) { fourBits = difference \u0026lt;= 29; } serialize_bool( stream, fourBits ); if ( fourBits ) { serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [30,61] -\u0026gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) { fiveBits = difference \u0026lt;= 61; } serialize_bool( stream, fiveBits ); if ( fiveBits ) { serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [62,125] -\u0026gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) { sixBits = difference \u0026lt;= 125; } serialize_bool( stream, sixBits ); if ( sixBits ) { serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } template \u0026lt;typename Stream\u0026gt; bool serialize_scene_d( Stream \u0026amp; stream, Scene \u0026amp; scene ) { int previous_index = -1; if ( Stream::IsWriting ) { for ( int i = 0; i \u0026lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); } write_object_index( stream, previous_index, MaxObjects ); } else { while ( true ) { int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) { break; } read_object( stream, scene.objects[index] ); } } return true; } But what about the worst case? Won\u0026rsquo;t we spent more bits when indices are \u0026gt;= +126 apart than on an absolute index? Yes we do, but how many of these worst case indices fit in an array of size 4096? Just 32. It\u0026rsquo;s nothing to worry about.\nProtocol IDs, CRC32 and Serialization Checks We are nearly at the end of this article, and you can see by now that we are sending a completely unattributed binary stream. It\u0026rsquo;s essential that read and write match perfectly, which is of course why the serialize functions are so great, it\u0026rsquo;s hard to desync something when you unify read and write.\nBut accidents happen, and when they do this system can seem like a stack of cards. What if you somehow desync read and write? How can you debug this? What if somebody tries to connect to your latest server code with an old version of your client?\nOne technique to protect against this is to include a protocol id in your packet. For example, it could be a combination of a unique number for your game, plus the hash of your protocol version and a hash of your game data. Now if a packet comes in from an incompatible game version, it\u0026rsquo;s automatically discarded because the protocol ids don\u0026rsquo;t match:\n[protocol id] (64bits) (packet data) The next level of protection is to pass a CRC32 over your packet and include that in the header. This lets you pick up corrupt packets (these do happen, remember that the IP checksum is just 16 bits\u0026hellip;). Now your packet header looks like this:\n[protocol id] (64bits) [crc32] (32bits) (packet data) At this point you may be wincing. Wait. I have to take 8+4 = 12 bytes of overhead per-packet just to implement my own checksum and protocol id? Well actually, you don\u0026rsquo;t. You can take a leaf out of how IPv4 does their checksum, and make the protocol id a magical prefix.\nThis means you don\u0026rsquo;t actually send it, and rely on the fact that if the CRC32 is calculated as if the packet were prefixed by the protocol id, then the CRC32 will be incorrect if the sender does not have the same protocol id as the receiver, thus saving 8 bytes per-packet:\n[protocol id] (64bits) // not actually sent, but used to calc crc32 [crc32] (32bits) (packet data) One final technique, perhaps as much a check against programmer error on your part and malicious senders (although redundant once you encrypt and sign your packet) is the serialization check. Basically, somewhere mid-packet, either before or after a complicated serialization section, just write out a known 32 bit integer value, and check that it reads back in on the other side with the same value. If the serialize check value is incorrect abort read and discard the packet.\nI like to do this between sections of my packet as I write them, so at least I know which part of my packet serialization has desynced read and write as I\u0026rsquo;m developing my protocol. Another cool trick I like to use is to always serialize a protocol check at the very end of the packet, to detect accidental packet truncation (which happens more often than you would think).\nNow the packet looks something like this:\n[protocol id] (64bits) // not actually sent, but used to calc crc32 [crc32] (32bits) (packet data) [end of packet serialize check] (32 bits) This is great packet structure to use during development.\nNEXT ARTICLE: Packet Fragmentation and Reassembly\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1472947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472947200,"objectID":"f8632416570c05d021c7efcce435f4b9","permalink":"https://gafferongames.com/post/serialization_strategies/","publishdate":"2016-09-04T00:00:00Z","relpermalink":"/post/serialization_strategies/","section":"post","summary":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn the previous article, we created a bitpacker but it required manual checking to make sure reading a packet from the network is safe. This is a real problem because the stakes are particularly high - a single missed check creates a vulnerability that an attacker can use to crash your server.\nIn this article, we\u0026rsquo;re going to transform the bitpacker into a system where this checking is automatic.","tags":["networking"],"title":"Serialization Strategies","type":"post"},{"authors":null,"categories":["Building a Game Network Protocol"],"content":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn this article we\u0026rsquo;re going to explore how AAA multiplayer games like first person shooters read and write packets. We\u0026rsquo;ll start with text based formats then move into binary hand-coded binary formats and bitpacking.\nAt the end of this article and the next, you should understand exactly how to implement your own packet read and write the same way the pros do it.\nBackground Consider a web server. It listens for requests, does some work asynchronously and sends responses back to clients. It’s stateless and generally not real-time, although a fast response time is great. Web servers are most often IO bound.\nGame server are different. They\u0026rsquo;re a headless version of the game running in the cloud. As such they are stateful and CPU bound. The traffic patterns are different too. Instead of infrequent request/response from tens of thousands of clients, a game server has far fewer clients, but processes a continuous stream of input packets sent from each client 60 times per-second, and broadcasts out the state of the world to clients 10, 20 or even 60 times per-second.\nAnd this state is huge. Thousands of objects with hundreds of properties each. Game network programmers spend a lot of their time optimizing exactly how this state is sent over the network with crazy bit-packing tricks, hand-coded binary formats and delta encoding.\nWhat would happen if we just encoded this world state as XML?\n\u0026lt;world_update world_time=\"0.0\"\u0026gt; \u0026lt;object id=\"1\" class=\"player\"\u0026gt; \u0026lt;property name=\"position\" value=\"(0,0,0)\"\u0026lt;/property\u0026gt; \u0026lt;property name=\"orientation\" value=\"(1,0,0,0)\"\u0026lt;/property\u0026gt; \u0026lt;property name=\"velocity\" value=\"(10,0,0)\"\u0026lt;/property\u0026gt; \u0026lt;property name=\"health\" value=\"100\"\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property name=\"weapon\" value=\"110\"\u0026gt;\u0026lt;/property\u0026gt; ... 100s more properties per-object ... \u0026lt;/object\u0026gt; \u0026lt;object id=\"100\" class=\"grunt\"\u0026gt; \u0026lt;property name=\"position\" value=\"(100,100,0)\"\u0026lt;/property\u0026gt; \u0026lt;property name=\"health\" value=\"10\"\u0026lt;/property\u0026gt; \u0026lt;/object\u0026gt; \u0026lt;object id=\"110\" class=\"weapon\"\u0026gt; \u0026lt;property type=\"semi-automatic\"\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property ammo_in_clip=\"8\"\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;property round_in_chamber=\"true\"\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;/object\u0026gt; ... 1000s more objects ... \u0026lt;/world_update\u0026gt; Pretty verbose\u0026hellip; it\u0026rsquo;s hard to see how this would be practical for a large world.\nJSON is a bit more compact:\n{ \"world_time\": 0.0, \"objects\": { 1: { \"class\": \"player\", \"position\": \"(0,0,0)\", \"orientation\": \"(1,0,0,0)\", \"velocity\": \"(10,0,0)\", \"health\": 100, \"weapon\": 110 } 100: { \"class\": \"grunt\", \"position\": \"(100,100,0)\", \"health\": 10 } 110: { \"class\": \"weapon\", \"type: \"semi-automatic\" \"ammo_in_clip\": 8, \"round_in_chamber\": 1 } // etc... } } But it still suffers from the same problem: the description of the data is larger than the data itself. What if instead of fully describing the world state in each packet, we split it up into two parts?\nA schema that describes the set of object classes and properties per-class, sent only once when a client connects to the server.\nData sent rapidly from server to client, which is encoded relative to the schema.\nThe schema could look something like this:\n{ \"classes\": { 0: \"player\" { \"properties\": { 0: { \"name\": \"position\", \"type\": \"vec3f\" } 1: { \"name\": \"orientation\", \"type\": \"quat4f\" } 2: { \"name\": \"velocity\", \"type\": \"vec3f\" } 3: { \"name\": \"health\", \"type\": \"float\" } 4: { \"name\": \"weapon\", \"type\": \"object\", } } } 1: \"grunt\": { \"properties\": { 0: { \"name\": \"position\", \"type\": \"vec3f\" } 1: { \"name\": \"health\", \"type\": \"float\" } } } 2: \"weapon\": { \"properties\": { 0: { \"name\": \"type\", \"type\": \"enum\", \"enum_values\": [ \"revolver\", \"semi-automatic\" ] } 1: { \"name\": \"ammo_in_clip\", \"type\": \"integer\", \"range\": \"0..9\", } 2: { \"name\": \"round_in_chamber\", \"type\": \"integer\", \"range\": \"0..1\" } } } } } The schema is quite big, but that\u0026rsquo;s beside the point. It\u0026rsquo;s sent only once, and now the client knows the set of classes in the game world and the number, name, type and range of properties per-class.\nWith this knowledge we can make the rapidly sent portion of the world state much more compact:\n{ \"world_time\": 0.0, \"objects\": { 1: [0,\"(0,0,0)\",\"(1,0,0,0)\",\"(10,0,0)\",100,110], 100: [1,\"(100,100,0)\",10], 110: [2,1,8,1] } } And we can compress it even further by switching to a custom text format:\n0.0 1:0,0,0,0,1,0,0,0,10,0,0,100,110 100:1,100,100,0,10 110:2,1,8,1 As you can see, it’s much more about what you don’t send than what you do.\nThe Inefficiencies of Text We’ve made good progress on our text format so far, moving from a highly attributed stream that fully describes the data (more description than actual data) to an unattributed text format that\u0026rsquo;s an order of magnitude more efficient.\nBut there are inherent inefficiencies when using text format for packets:\nWe are most often sending data in the range A-Z, a-z and 0-1, plus a few other symbols. This wastes the remainder of the 0-255 range for each character sent. From an information theory standpoint, this is an inefficient encoding.\nThe text representation of integer values are in the general case much less efficient than the binary format. For example, in text format the worst case unsigned 32 bit integer 4294967295 takes 10 bytes, but in binary format it takes just four.\nIn text, even the smallest numbers in 0-9 range require at least one byte, but in binary, smaller values like 0, 11, 31, 100 can be sent with fewer than 8 bits if we know their range ahead of time.\nIf an integer value is negative, you have to spend a whole byte on \u0026rsquo;-\u0026rsquo; to indicate that.\nFloating point numbers waste one byte specifying the decimal point.\nThe text representation of numerical values are variable length: “5”, “12345”, “3.141593”. Because of this we need to spend one byte on a separator after each value so we know when it ends.\nNewlines \u0026rsquo;\\n\u0026rsquo; or some other separator are required to distinguish between the set of variables belonging to one object and the next. When you have thousands of objects, this really adds up.\nIn short, if we wish to optimize any further, it\u0026rsquo;s necessary to switch to a binary format.\nSwitching to a Binary Format In the web world there are some really great libraries that read and write binary formats like BJSON, Protocol Buffers, Flatbuffers, Thrift, Cap’n Proto and MsgPack.\nIn manay cases, these libraries are great fit for building your game network protocol. But in the fast-paced world of first person shooters where efficiency is paramount, a hand-tuned binary protocol is still the gold standard.\nThere are a few reasons for this. Web binary formats are designed for situations where versioning of data is extremely important. If you upgrade your backend, older clients should be able to keep talking to it with the old format. Data formats are also expected to be language agnostic. A backend written in Golang should be able to talk with a web client written in JavaScript and other server-side components written in Python or Java.\nGame servers are completely different beasts. The client and server are almost always written in the same language (C++), and versioning is much simpler. If a client with an incompatible version tries to connect, that connection is simply rejected. There\u0026rsquo;s simply no need for compatibility across different versions.\nSo if you don’t need versioning and you don’t need cross-language support what are the benefits for these libraries? Convenience. Ease of use. Not needing to worry about creating, testing and debugging your own binary format.\nBut this convenience is offset by the fact that these libraries are less efficient and less flexible than a binary protocol we can roll ourselves. So while I encourage you to evaluate these libraries and see if they suit your needs, for the rest of this article, we\u0026rsquo;re going to move forward with a custom binary protocol.\nGetting Started with a Binary Format One option for creating a custom binary protocol is to use the in-memory format of your data structures in C/C++ as the over-the-wire format. People often start here, so although I don’t recommend this approach, lets explore it for a while before we poke holes in it.\nFirst define the set of packets, typically as a union of structs:\nstruct Packet { enum PacketTypeEnum { PACKET_A, PACKET_B, PACKET_C }; uint8_t packetType; union { struct PacketA { int x,y,z; } a; struct PacketB { int numElements; int elements[MaxElements]; } b; struct PacketC { bool x; short y; int z; } c; }; }; When writing the packet, set the first byte in the packet to the packet type number (0, 1 or 2). Then depending on the packet type, memcpy the appropriate union struct into the packet. On read do the reverse: read in the first byte, then according to the packet type, copy the packet data to the corresponding struct.\nIt couldn’t get simpler. So why do most games avoid this approach?\nThe first reason is that different compilers and platforms provide different packing of structs. If you go this route you’ll spend a lot of time with #pragma pack trying to make sure that different compilers and different platforms lay out the structures in memory exactly the same way.\nThe next one is endianness. Most computers are mostly little endian these days but historically some architectures like PowerPC were big endian. If you need to support communication between little endian and big endian machines, the memcpy the struct in and out of the packet approach simply won’t work. At minimum you need to write a function to swap bytes between host and network byte order on read and write for each variable in your struct.\nThere are other issues as well. If a struct contains pointers you can’t just serialize that value over the network and expect a valid pointer on the other side. Also, if you have variable sized structures, such as an array of 32 elements, but most of the time it’s empty or only has a few elements, it\u0026rsquo;s wasteful to always send the array at worst case size. A better approach would support a variable length encoding that only sends the actual number of elements in the array.\nBut ultimately, what really drives a stake into the heart of this approach is security. It’s a massive security risk to take data coming in over the network and trust it, and that\u0026rsquo;s exactly what you do if you just copy a block of memory sent over the network into your struct. Wheee! What if somebody constructs a malicious PacketB and sends it to you with numElements = 0xFFFFFFFF?\nYou should, no you must, at minimum do some sort of per-field checking that values are in range vs. blindly accepting what is sent to you. This is why the memcpy struct approach is rarely used in professional games.\nRead and Write Functions The next level of sophistication is read and write functions per-packet.\nStart with the following simple operations:\nvoid WriteInteger( Buffer \u0026amp; buffer, uint32_t value ); void WriteShort( Buffer \u0026amp; buffer, uint16_t value ); void WriteChar( Buffer \u0026amp; buffer, uint8_t value ); uint32_t ReadInteger( Buffer \u0026amp; buffer ); uint16_t ReadShort( Buffer \u0026amp; buffer ); uint8_t ReadByte( Buffer \u0026amp; buffer ); These operate on a structure which keeps track of the current position:\nstruct Buffer { uint8_t * data; // pointer to buffer data int size; // size of buffer data (bytes) int index; // index of next byte to be read/written }; The write integer function looks something like this:\nvoid WriteInteger( Buffer \u0026amp; buffer, uint32_t value ) { assert( buffer.index + 4 \u0026lt;= size ); #ifdef BIG_ENDIAN *((uint32_t*)(buffer.data+buffer.index)) = bswap( value ); #else // #ifdef BIG_ENDIAN *((uint32_t*)(buffer.data+buffer.index)) = value; #endif // #ifdef BIG_ENDIAN buffer.index += 4; } And the read integer function looks like this:\nuint32_t ReadInteger( Buffer \u0026amp; buffer ) { assert( buffer.index + 4 \u0026lt;= size ); uint32_t value; #ifdef BIG_ENDIAN value = bswap( *((uint32_t*)(buffer.data+buffer.index)) ); #else // #ifdef BIG_ENDIAN value = *((uint32_t*)(buffer.data+buffer.index)); #endif // #ifdef BIG_ENDIAN buffer.index += 4; return value; } Now, instead of copying across packet data in and out of structs, we implement read and write functions for each packet type:\nstruct PacketA { int x,y,z; void Write( Buffer \u0026amp; buffer ) { WriteInteger( buffer, x ); WriteInteger( buffer, y ); WriteInteger( buffer, z ); } void Read( Buffer \u0026amp; buffer ) { ReadInteger( buffer, x ); ReadInteger( buffer, y ); ReadInteger( buffer, z ); } }; struct PacketB { int numElements; int elements[MaxElements]; void Write( Buffer \u0026amp; buffer ) { WriteInteger( buffer, numElements ); for ( int i = 0; i \u0026lt; numElements; ++i ) WriteInteger( buffer, elements[i] ); } void Read( Buffer \u0026amp; buffer ) { ReadInteger( buffer, numElements ); for ( int i = 0; i \u0026lt; numElements; ++i ) ReadInteger( buffer, elements[i] ); } }; struct PacketC { bool x; short y; int z; void Write( Buffer \u0026amp; buffer ) { WriteByte( buffer, x ); WriteShort( buffer, y ); WriteInt( buffer, z ); } void Read( Buffer \u0026amp; buffer ) { ReadByte( buffer, x ); ReadShort( buffer, y ); ReadInt( buffer, z ); } }; When reading and writing packets, start the packet with a byte specifying the packet type via ReadByte/WriteByte, then according to the packet type, call the read/write on the corresponding packet struct in the union.\nNow we have a system that allows machines with different endianness to communicate and supports variable length encoding of elements.\nBitpacking What if we have a value in the range [0,1000] we really only need 10 bits to represent all possible values. Wouldn\u0026rsquo;t it be nice if we could write just 10 bits, instead of rounding up to 16? What about boolean values? It would be nice to send these as one bit instead of 8!\nOne way to implement this is to manually organize your C++ structures into packed integers with bitfields and union tricks, such as grouping all bools together into one integer type via bitfield and serializing them as a group. But this is tedious and error prone and there’s no guarantee that different C++ compilers pack bitfields in memory exactly the same way.\nA much more flexible way that trades a small amount of CPU on packet read and write for convenience is a bitpacker. This is code that reads and writes non-multiples of 8 bits to a buffer.\nWriting Bits Many people write bitpackers that work at the byte level. This means they flush bytes to memory as they are filled. This is simpler to code, but the ideal is to read and write words at a time, because modern machines are optimized to work this way instead of farting across a buffer at byte level like it’s 1985.\nIf you want to write 32 bits at a time, you\u0026rsquo;ll need a scratch word twice that size, eg. uint64_t. The reason is that you need the top half for overflow. For example, if you have just written a value 30 bits long into the scratch buffer, then write another value that is 10 bits long you need somewhere to store 30 + 10 = 40 bits.\nuint64_t scratch; int scratch_bits; int word_index; uint32_t * buffer; When we start writing with the bitpacker, all these variables are cleared to zero except buffer which points to the start of the packet we are writing to. Because we\u0026rsquo;re accessing this packet data at a word level, not byte level, make sure packet buffers lengths are a multiple of 4 bytes.\nLet’s say we want to write 3 bits followed by 10 bits, then 24. Our goal is to pack this tightly in the scratch buffer and flush that out to memory, 32 bits at a time. Note that 3 + 10 + 24 = 37. We have to handle this case where the total number of bits don’t evenly divide into 32. This is actually the common case.\nAt the first step, write the 3 bits to scratch like this:\nxxx scratch_bits is now 3.\nNext, write 10 bits:\nyyyyyyyyyyxxx scratch_bits is now 13 (3+10).\nNext write 24 bits:\nzzzzzzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx scratch_bits is now 37 (3+10+24). We’re straddling the 32 bit word boundary in our 64 bit scratch variable and have 5 bits in the upper 32 bits (overflow). Flush the lower 32 bits of scratch to memory, advance word_index by one, shift scratch right by 32 and subtract 32 from scratch_bits.\nscratch now looks like this:\nzzzzz We\u0026rsquo;ve finished writing bits but we still have data in scratch that\u0026rsquo;s not flushed to memory. For this data to be included in the packet we need to make sure to flush any remaining bits in scratch to memory at the end of writing.\nWhen we flush a word to memory it is converted to little endian byte order. To see why this is important consider what happens if we flush bytes to memory in big endian order:\nDCBA000E Since we fill bits in the word from right to left, the last byte in the packet E is actually on the right. If we try to send this buffer in a packet of 5 bytes (the actual amount of data we have to send) the packet catches 0 for the last byte instead of E. Ouch!\nBut when we write to memory in little endian order, bytes are reversed back out in memory like this:\nABCDE000 And we can write 5 bytes to the network and catch E at the end. Et voilà!\nReading Bits To read the bitpacked data, start with the buffer sent over the network:\nABCDE The bit reader has the following state:\nuint64_t scratch; int scratch_bits; int total_bits; int num_bits_read; int word_index; uint32_t * buffer; To start all variables are cleared to zero except total_bits which is set to the size of the packet as bytes * 8, and buffer which points to the start of the packet.\nThe user requests a read of 3 bits. Since scratch_bits is zero, it’s time to read in the first word. Read in the word to scratch, shifted left by scratch_bits (0). Add 32 to scratch_bits.\nThe value of scratch is now:\nzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx Read off the low 3 bits, giving the expected value of:\nxxx Shift scratch to the right 3 bits and subtract 3 from scratch_bits:\nzzzzzzzzzzzzzzzzzzzyyyyyyyyyy Read off another 10 bits in the same way, giving the expected value of:\nyyyyyyyyyy Scratch now looks like:\nzzzzzzzzzzzzzzzzzzz The next read asks for 24 bits but scratch_bits is only 19 (=32-10-3).\nIt’s time to read in the next word. Shifting the word in memory left by scratch_bits (19) and or it on top of scratch.\nNow we have all the bits necessary for z in scratch:\nzzzzzzzzzzzzzzzzzzzzzzzz Read off 24 bits and shift scratch right by 24. scratch is now all zeros.\nWe\u0026rsquo;re done!\nBeyond Bitpacking Reading and writing integer values into a packet by specifying the number of bits to read/write is not the most user friendly option.\nConsider this example:\nconst int MaxElements = 32; struct PacketB { int numElements; int elements[MaxElements]; void Write( BitWriter \u0026amp; writer ) { WriteBits( writer, numElements, 6 ); for ( int i = 0; i \u0026lt; numElements; ++i ) WriteBits( writer, elements[i] ); } void Read( BitReader \u0026amp; reader ) { ReadBits( reader, numElements, 6 ); for ( int i = 0; i \u0026lt; numElements; ++i ) ReadBits( reader, elements[i] ); } }; This code looks fine at first glance, but let’s assume that some time later you, or somebody else on your team, increases MaxElements from 32 to 200 but forget to update the number of bits required to 7. Now the high bit of numElements are being silently truncated on send. It\u0026rsquo;s pretty hard to track something like this down after the fact.\nThe simplest option is to just turn it around and define the maximum number of elements in terms of the number of bits sent:\nconst int MaxElementBits = 7; const int MaxElements = ( 1 \u0026lt;\u0026lt; MaxElementBits ) - 1; Another option is to get fancy and work out the number of bits required at compile time:\ntemplate \u0026lt;uint32_t x\u0026gt; struct PopCount { enum { a = x - ( ( x \u0026gt;\u0026gt; 1 ) \u0026amp; 0x55555555 ), b = ( ( ( a \u0026gt;\u0026gt; 2 ) \u0026amp; 0x33333333 ) + ( a \u0026amp; 0x33333333 ) ), c = ( ( ( b \u0026gt;\u0026gt; 4 ) + b ) \u0026amp; 0x0f0f0f0f ), d = c + ( c \u0026gt;\u0026gt; 8 ), e = d + ( d \u0026gt;\u0026gt; 16 ), result = e \u0026amp; 0x0000003f }; }; template \u0026lt;uint32_t x\u0026gt; struct Log2 { enum { a = x | ( x \u0026gt;\u0026gt; 1 ), b = a | ( a \u0026gt;\u0026gt; 2 ), c = b | ( b \u0026gt;\u0026gt; 4 ), d = c | ( c \u0026gt;\u0026gt; 8 ), e = d | ( d \u0026gt;\u0026gt; 16 ), f = e \u0026gt;\u0026gt; 1, result = PopCount\u0026lt;f\u0026gt;::result }; }; template \u0026lt;int64_t min, int64_t max\u0026gt; struct BitsRequired { static const uint32_t result = ( min == max ) ? 0 : ( Log2\u0026lt;uint32_t(max-min)\u0026gt;::result + 1 ); }; #define BITS_REQUIRED( min, max ) BitsRequired\u0026lt;min,max\u0026gt;::result Now you can’t mess up the number of bits, and you can specify non-power of two maximum values and it everything works out.\nconst int MaxElements = 32; const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); But be careful when array sizes aren\u0026rsquo;t a power of two! In the example above MaxElements is 32, so MaxElementBits is 6. This seems fine because all values in [0,32] fit in 6 bits. The problem is that there are additional values within 6 bits that are outside our array bounds: [33,63]. An attacker can use this to construct a malicious packet that corrupts memory!\nThis leads to the inescapable conclusion that it’s not enough to just specify the number of bits required when reading and writing a value, we must also check that it is within the valid range: [min,max]. This way if a value is outside of the expected range we can detect that and abort read.\nI used to implement this using C++ exceptions, but when I profiled, I found it to be incredibly slow. In my experience, it’s much faster to take one of two approaches: set a flag on the bit reader that it should abort, or return false from read functions on failure. But now, in order to be completely safe on read you must to check for error on every read operation.\nconst int MaxElements = 32; const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB { int numElements; int elements[MaxElements]; void Write( BitWriter \u0026amp; writer ) { WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i \u0026lt; numElements; ++i ) { WriteBits( writer, elements[i], 32 ); } } void Read( BitReader \u0026amp; reader ) { ReadBits( reader, numElements, MaxElementBits ); if ( numElements \u0026gt; MaxElements ) { reader.Abort(); return; } for ( int i = 0; i \u0026lt; numElements; ++i ) { if ( reader.IsOverflow() ) break; ReadBits( buffer, elements[i], 32 ); } } }; If you miss any of these checks, you expose yourself to buffer overflows and infinite loops when reading packets. Clearly you don’t want this to be a manual step when writing a packet read function. You want it to be automatic.\nNEXT ARTICLE: Serialization Strategies\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"e4e0ae2409551f2a58982112ee64e1d5","permalink":"https://gafferongames.com/post/reading_and_writing_packets/","publishdate":"2016-09-01T00:00:00Z","relpermalink":"/post/reading_and_writing_packets/","section":"post","summary":"Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.\nIn this article we\u0026rsquo;re going to explore how AAA multiplayer games like first person shooters read and write packets. We\u0026rsquo;ll start with text based formats then move into binary hand-coded binary formats and bitpacking.\nAt the end of this article and the next, you should understand exactly how to implement your own packet read and write the same way the pros do it.","tags":["networking"],"title":"Reading and Writing Packets","type":"post"},{"authors":null,"categories":["Networked Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we discussed techniques for compressing snapshots.\nIn this article we round out our discussion of networked physics strategies with state synchronization, the third and final strategy in this article series.\nState Synchronization What is state synchronization? The basic idea is that, somewhat like deterministic lockstep, we run the simulation on both sides but, unlike deterministic lockstep, we don\u0026rsquo;t just send input, we send both input and state.\nThis gives state synchronization interesting properties. Because we send state, we don\u0026rsquo;t need perfect determinism to stay in sync, and because the simulation runs on both sides, objects continue moving forward between updates.\nThis lets us approach state synchronization differently to snapshot interpolation. Instead of sending state updates for every object in each packet, we can now send updates for only a few, and if we\u0026rsquo;re smart about how we select the objects for each packet, we can save bandwidth by concentrating updates on the most important objects.\nSo what\u0026rsquo;s the catch? State synchronization is an approximate and lossy synchronization strategy. In practice, this means you\u0026rsquo;ll spend a lot of time tracking down sources of extrapolation divergence and pops. But other than that, it\u0026rsquo;s a quick and easy strategy to get started with.\nImplementation Here\u0026rsquo;s the state sent over the network per-object:\nstruct StateUpdate { int index; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity; }; Unlike snapshot interpolation, we\u0026rsquo;re not just sending visual quantities like position and orientation, we\u0026rsquo;re also sending non-visual state such as linear and angular velocity. Why is this?\nThe reason is that state synchronization runs the simulation on both sides, so it\u0026rsquo;s always extrapolating from the last state update applied to each object. If linear and angular velocity aren\u0026rsquo;t synchronized, this extrapolation is done with incorrect velocities, leading to pops when objects are updated.\nWhile we must send the velocities, there\u0026rsquo;s no point wasting bandwidth sending (0,0,0) over and over while an object is at rest. We can fix this with a trivial optimization, like so:\nvoid serialize_state_update( Stream \u0026amp; stream, int \u0026amp; index, StateUpdate \u0026amp; state_update ) { serialize_int( stream, index, 0, NumCubes - 1 ); serialize_vector( stream, state_update.position ); serialize_quaternion( stream, state_update.orientation ); bool at_rest = stream.IsWriting() ? state_update.AtRest() : false; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, state_update.linear_velocity ); serialize_vector( stream, state_update.angular_velocity ); } else if ( stream.IsReading() ) { state_update.linear_velocity = vec3f(0,0,0); state_update.angular_velocity = vec3f(0,0,0); } } What you see above is a serialize function. It\u0026rsquo;s a trick I like to use to unify packet read and write. I like it because it\u0026rsquo;s expressive while at the same time it\u0026rsquo;s difficult to desync read and write. You can read more about them here.\nPacket Structure Now let\u0026rsquo;s look at the overall structure of packets being sent:\nconst int MaxInputsPerPacket = 32; const int MaxStateUpdatesPerPacket = 64; struct Packet { uint32_t sequence; Input inputs[MaxInputsPerPacket]; int num_object_updates; StateUpdate state_updates[MaxStateUpdatesPerPacket]; }; First we include a sequence number in each packet so we can determine out of order, lost or duplicate packets. I recommend you run the simulation at the same framerate on both sides (for example 60HZ) and in this case the sequence number can work double duty as the frame number.\nInput is included in each packet because it\u0026rsquo;s needed for extrapolation. Like deterministic lockstep we send multiple redundant inputs so in the case of packet loss it\u0026rsquo;s very unlikely that an input gets dropped. Unlike deterministic lockstep, if don\u0026rsquo;t have the next input we don\u0026rsquo;t stop the simulation and wait for it, we continue extrapolating forward with the last input received.\nNext you can see that we only send a maximum of 64 state updates per-packet. Since we have a total of 901 cubes in the simulation so we need some way to select the n most important state updates to include in each packet. We need some sort of prioritization scheme.\nTo get started each frame walk over all objects in your simulation and calculate their current priority. For example, in the cube simulation I calculate priority for the player cube as 1000000 because I always want it to be included in every packet, and for interacting (red cubes) I give them a higher priority of 100 while at rest objects have priority of 1.\nUnfortunately if you just picked objects according to their current priority each frame you\u0026rsquo;d only ever send red objects while in a katamari ball and white objects on the ground would never get updated. We need to take a slightly different approach, one that prioritizes sending important objects while also distributing updates across all objects in the simulation.\nPriority Accumulator You can do this with a priority accumulator. This is an array of float values, one value per-object, that is remembered from frame to frame. Instead of taking the immediate priority value for the object and sorting on that, each frame we add the current priority for each object to its priority accumulator value then sort objects in order from largest to smallest priority accumulator value. The first n objects in this sorted list are the objects you should send that frame.\nYou could just send state updates for all n objects but typically you have some maximum bandwidth you want to support like 256kbit/sec. Respecting this bandwidth limit is easy. Just calculate how large your packet header is and how many bytes of preamble in the packet (sequence, # of objects in packet and so on) and work out conservatively the number of bytes remaining in your packet while staying under your bandwidth target.\nThen take the n most important objects according to their priority accumulator values and as you construct the packet, walk these objects in order and measure if their state updates will fit in the packet. If you encounter a state update that doesn\u0026rsquo;t fit, skip over it and try the next one. After you serialize the packet, reset the priority accumulator to zero for objects that fit but leave the priority accumulator value alone for objects that didn\u0026rsquo;t. This way objects that don\u0026rsquo;t fit are first in line to be included in the next packet.\nThe desired bandwidth can even be adjusted on the fly. This makes it really easy to adapt state synchronization to changing network conditions, for example if you detect the connection is having difficulty you can reduce the amount of bandwidth sent (congestion avoidance) and the quality of state synchronization scales back automatically. If the network connection seems like it should be able to handle more bandwidth later on then you can raise the bandwidth limit.\nJitter Buffer The priority accumulator covers the sending side, but on the receiver side there is much you need to do when applying these state updates to ensure that you don\u0026rsquo;t see divergence and pops in the extrapolation between object updates.\nThe very first thing you need to consider is that network jitter exists. You don\u0026rsquo;t have any guarantee that packets you sent nicely spaced out 60 times per-second arrive that way on the other side. What happens in the real world is you\u0026rsquo;ll typically receive two packets one frame, 0 packets the next, 1, 2, 0 and so on because packets tend to clump up across frames. To handle this situation you need to implement a jitter buffer for your state update packets. If you fail to do this you\u0026rsquo;ll have a poor quality extrapolation and pops in stacks of objects because objects in different state update packets are slightly out of phase with each other with respect to time.\nAll you do in a jitter buffer is hold packets before delivering them to the application at the correct time as indicated by the sequence number (frame number) in the packet. The delay you need to hold packets for in this buffer is a much smaller amount of time relative to interpolation delay for snapshot interpolation but it\u0026rsquo;s the same basic idea. You just need to delay packets just enough (say 4-5 frames @ 60HZ) so that they come out of the buffer properly spaced apart.\nApplying State Updates Once the packet comes out of the jitter how do you apply state updates? My recommendation is that you should snap the physics state hard. This means you apply the values in the state update directly to the simulation.\nI recommend against trying to apply some smoothing between the state update and the current state at the simulation level. This may sound counterintuitive but the reason for this is that the simulation extrapolates from the state update so you want to make sure it extrapolates from a valid physics state for that object rather than some smoothed, total bullshit made-up one. This is especially important when you are networking large stacks of objects.\nSurprisingly, without any smoothing the result is already pretty good:\nYour browser does not support the video tag. As you can see it\u0026rsquo;s already looking quite good and barely any bandwidth optimization has been performed. Contrast this with the first video for snapshot interpolation which was at 18mbit/sec and you can see that using the simulation to extrapolate between state updates is a great way to use less bandwidth.\nOf course we can do a lot better than this and each optimization we do lets us squeeze more state updates in the same amount of bandwidth. The next obvious thing we can do is to apply all the standard quantization compression techniques such as bounding and quantizing position, linear and angular velocity value and using the smallest three compression as described in snapshot compression.\nBut here it gets a bit more complex. We are extrapolating from those state updates so if we quantize these values over the network then the state that arrives on the right side is slightly different from the left side, leading to a slightly different extrapolation and a pop when the next state update arrives for that object.\nYour browser does not support the video tag. Quantize Both Sides The solution is to quantize the state on both sides. This means that on both sides before each simulation step you quantize the entire simulation state as if it had been transmitted over the network. Once this is done the left and right side are both extrapolating from quantized state and their extrapolations are very similar.\nBecause these quantized values are being fed back into the simulation, you\u0026rsquo;ll find that much more precision is required than snapshot interpolation where they were just visual quantities used for interpolation. In the cube simulation I found it necessary to have 4096 position values per-meter, up from 512 with snapshot interpolation, and a whopping 15 bits per-quaternion component in smallest three (up from 9). Without this extra precision significant popping occurs because the quantization forces physics objects into penetration with each other, fighting against the simulation which tries to keep the objects out of penetration. I also found that softening the constraints and reducing the maximum velocity which the simulation used to push apart penetrating objects also helped reduce the amount of popping.\nYour browser does not support the video tag. With quantization applied to both sides you can see the result is perfect once again. It may look visually about the same as the uncompressed version but in fact we\u0026rsquo;re able to fit many more state updates per-packet into the 256kbit/sec bandwidth limit. This means we are better able to handle packet loss because state updates for each object are sent more rapidly. If a packet is lost, it\u0026rsquo;s less of a problem because state updates for those objects are being continually included in future packets.\nBe aware that when a burst of packet loss occurs like 1/4 a second with no packets getting through, and this is inevitable that eventually something like this will happen, you will probably get a different result on the left and the right sides. We have to plan for this. In spite of all effort that we have made to ensure that the extrapolation is as close as possible (quantizing both sides and so on) pops can and will occur if the network stops delivering packets.\nVisual Smoothing We can cover up these pops with smoothing.\nRemember how I said earlier that you should not apply smoothing at the simulation level because it ruins the extrapolation? What we\u0026rsquo;re going to do for smoothing instead is calculating and maintaining position and orientation error offsets that we reduce over time. Then when we render the cubes in the right side we don\u0026rsquo;t render them at the simulation position and orientation, we render them at the simulation position + error offset, and orientation * orientation error.\nOver time we work to reduce these error offsets back to zero for position error and identity for orientation error. For error reduction I use an exponentially smoothed moving average tending towards zero. So in effect, I multiply the position error offset by some factor each frame (eg. 0.9) until it gets close enough to zero for it to be cleared (thus avoiding denormals). For orientation, I slerp a certain amount (0.1) towards identity each frame, which has the same effect for the orientation error.\nThe trick to making this all work is that when a state update comes in you take the current simulation position and add the position error to that, and subtract that from the new position, giving the new position error offset which gives an identical result to the current (smoothed) visual position.\nThe same process is then applied to the error quaternion (using multiplication by the conjugate instead of subtraction) and this way you effectively calculate on each state update the new position error and orientation error relative to the new state such that the object appears to have not moved at all. Thus state updates are smooth and have no immediate visual effect, and the error reduction smoothes out any error in the extrapolation over time without the player noticing in the common case.\nI find that using a single smoothing factor gives unacceptable results. A factor of 0.95 is perfect for small jitters because it smooths out high frequency jitter really well, but at the same time it is too slow for large position errors, like those that happen after multiple seconds of packet loss:\nYour browser does not support the video tag. The solution I use is two different scale factors at different error distances, and to make sure the transition is smooth I blend between those two factors linearly according to the amount of positional error that needs to be reduced. In this simulation, having 0.95 for small position errors (25cms or less) while having a tighter blend factor of 0.85 for larger distances (1m error or above) gives a good result. The same strategy works well for orientation using the dot product between the orientation error and the identity matrix. I found that in this case a blend of the same factors between dot 0.1 and 0.5 works well.\nThe end result is smooth error reduction for small position and orientation errors combined with a tight error reduction for large pops. As you can see above you don\u0026rsquo;t want to drag out correction of these large pops, they need to be fast and so they\u0026rsquo;re over quickly otherwise they\u0026rsquo;re really disorienting for players, but at the same time you want to have really smooth error reduction when the error is small hence the adaptive error reduction approach works really well.\nYour browser does not support the video tag. Delta Compression Even though I would argue the result above is probably good enough already it is possible to improve the synchronization considerably from this point. For example to support a world with larger objects or more objects being interacted with. So lets work through some of those techniques and push this technique as far as it can go.\nThere is an easy compression that can be performed. Instead of encoding absolute position, if it is within a range of the player cube center, encode position as a relative offset to the player center position. In the common cases where bandwidth is high and state updates need to be more frequent (katamari ball) this provides a large win.\nNext, what if we do want to perform some sort of delta encoding for state synchronization? We can but it\u0026rsquo;s quite different in this case than it is with snapshots because we\u0026rsquo;re not including every cube in every packet, so we can\u0026rsquo;t just track the most recent packet received and say, OK all these state updates in this packet are relative to packet X.\nWhat you actually have to do is per-object update keep track of the packet that includes the base for that update. You also need to keep track of exactly the set of packets received so that the sender knows which packets are valid bases to encode relative to. This is reasonably complicated and requires a bidirectional ack system over UDP. Such a system is designed for exactly this sort of situation where you need to know exactly which packets definitely got through. You can find a tutorial on how to implement this in this article.\nSo assuming that you have an ack system you know with packet sequence numbers get through. What you do then is per-state update write one bit if the update is relative or absolute, if absolute then encode with no base as before, otherwise if relative send the 16 bit sequence number per-state update of the base and then encode relative to the state update data sent in that packet. This adds 1 bit overhead per-update as well as 16 bits to identify the sequence number of the base per-object update. Can we do better?\nYes. In turns out that of course you\u0026rsquo;re going to have to buffer on the send and receive side to implement this relative encoding and you can\u0026rsquo;t buffer forever. In fact, if you think about it you can only buffer up a couple of seconds before it becomes impractical and in the common case of moving objects you\u0026rsquo;re going to be sending the updates for same object frequently (katamari ball) so practically speaking the base sequence will only be from a short time ago.\nSo instead of sending the 16 bit sequence base per-object, send in the header of the packet the most recent acked packet (from the reliability ack system) and per-object encode the offset of the base sequence relative to that value using 5 bits. This way at 60 packets per-second you can identify an state update with a base half a second ago. Any base older than this is unlikely to provide a good delta encoding anyway because it\u0026rsquo;s old, so in that case just drop back to absolute encoding for that update.\nNow lets look at the type of objects that are going to have these absolute encodings rather than relative. They\u0026rsquo;re the objects at rest. What can we do to make them as efficient as possible? In the case of the cube simulation one bad result that can occur is that a cube comes to rest (turns grey) and then has its priority lowered significantly. If that very last update with the position of that object is missed due to packet loss, it can take a long time for that object to have its at rest position updated.\nWe can fix this by tracking objects which have recently come to rest and bumping their priority until an ack comes back for a packet they were sent in. Thus they are sent at an elevated priority compared with normal grey cubes (which are at rest and have not moved) and keep resending at that elevated rate until we know that update has been received, thus \u0026ldquo;committing\u0026rdquo; that grey cube to be at rest at the correct position.\nConclusion And that\u0026rsquo;s really about it for this technique. Without anything fancy it\u0026rsquo;s already pretty good, and on top of that another order of magnitude improvement is available with delta compression, at the cost of significant complexity!\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1420416000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420416000,"objectID":"b583a119a9c40fa7042af92e50e153d9","permalink":"https://gafferongames.com/post/state_synchronization/","publishdate":"2015-01-05T00:00:00Z","relpermalink":"/post/state_synchronization/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we discussed techniques for compressing snapshots.\nIn this article we round out our discussion of networked physics strategies with state synchronization, the third and final strategy in this article series.\nState Synchronization What is state synchronization? The basic idea is that, somewhat like deterministic lockstep, we run the simulation on both sides but, unlike deterministic lockstep, we don\u0026rsquo;t just send input, we send both input and state.","tags":["physics","networking"],"title":"State Synchronization","type":"post"},{"authors":null,"categories":["Networked Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we sent snapshots of the entire simulation 10 times per-second over the network and interpolated between them to reconstruct a view of the simulation on the other side.\nThe problem with a low snapshot rate like 10HZ is that interpolation between snapshots adds interpolation delay on top of network latency. At 10 snapshots per-second, the minimum interpolation delay is 100ms, and a more practical minimum considering network jitter is 150ms. If protection against one or two lost packets in a row is desired, this blows out to 250ms or 350ms delay.\nThis is not an acceptable amount of delay for most games, but when the physics simulation is as unpredictable as ours, the only way to reduce it is to increase the packet send rate. Unfortunately, increasing the send rate also increases bandwidth. So what we\u0026rsquo;re going to do in this article is work through every possible bandwidth optimization (that I can think of at least) until we get bandwidth under control.\nOur target bandwidth is 256 kilobits per-second.\nStarting Point @ 60HZ Life is rarely easy, and the life of a network programmer, even less so. As network programmers we\u0026rsquo;re often tasked with the impossible, so in that spirit, let\u0026rsquo;s increase the snapshot send rate from 10 to 60 snapshots per-second and see exactly how far away we are from our target bandwidth.\nThat\u0026rsquo;s a LOT of bandwidth: 17.37 megabits per-second!\nLet\u0026rsquo;s break it down and see where all the bandwidth is going.\nHere\u0026rsquo;s the per-cube state sent in the snapshot:\nstruct CubeState { bool interacting; vec3f position; vec3f linear_velocity; quat4f orientation; }; And here\u0026rsquo;s the size of each field:\nquat orientation: 128 bits vec3 linear_velocity: 96 bits vec3 position: 96 bits bool interacting: 1 bit This gives a total of 321 bits bits per-cube (or 40.125 bytes per-cube).\nLet\u0026rsquo;s do a quick calculation to see if the bandwidth checks out. The scene has 901 cubes so 901*40.125 = 36152.625 bytes of cube data per-snapshot. 60 snapshots per-second so 36152.625 * 60 = 2169157.5 bytes per-second. Add in packet header estimate: 2169157.5 + 32*60 = 2170957.5. Convert bytes per-second to megabits per-second: 2170957.5 * 8 / ( 1000 * 1000 ) = 17.38mbps.\nEverything checks out. There\u0026rsquo;s no easy way around this, we\u0026rsquo;re sending a hell of a lot of bandwidth, and we have to reduce that to something around 1-2% of it\u0026rsquo;s current bandwidth to hit our target of 256 kilobits per-second.\nIs this even possible? Of course it is! Let\u0026rsquo;s get started :)\nOptimizing Orientation We\u0026rsquo;ll start by optimizing orientation because it\u0026rsquo;s the largest field. (When optimizing bandwidth it\u0026rsquo;s good to work in the order of greatest to least potential gain where possible\u0026hellip;)\nMany people when compressing a quaternion think: \u0026ldquo;I know. I\u0026rsquo;ll just pack it into 8.8.8.8 with one 8 bit signed integer per-component!\u0026rdquo;. Sure, that works, but with a bit of math you can get much better accuracy with fewer bits using a trick called the \u0026ldquo;smallest three\u0026rdquo;.\nHow does the smallest three work? Since we know the quaternion represents a rotation its length must be 1, so x^2+y^2+z^2+w^2 = 1. We can use this identity to drop one component and reconstruct it on the other side. For example, if you send x,y,z you can reconstruct w = sqrt( 1 - x^2 - y^2 - z^2 ). You might think you need to send a sign bit for w in case it is negative, but you don\u0026rsquo;t, because you can make w always positive by negating the entire quaternion if w is negative (in quaternion space (x,y,z,w) and (-x,-y,-z,-w) represent the same rotation.)\nDon\u0026rsquo;t always drop the same component due to numerical precision issues. Instead, find the component with the largest absolute value and encode its index using two bits [0,3] (0=x, 1=y, 2=z, 3=w), then send the index of the largest component and the smallest three components over the network (hence the name). On the other side use the index of the largest bit to know which component you have to reconstruct from the other three.\nOne final improvement. If v is the absolute value of the largest quaternion component, the next largest possible component value occurs when two components have the same absolute value and the other two components are zero. The length of that quaternion (v,v,0,0) is 1, therefore v^2 + v^2 = 1, 2v^2 = 1, v = 1/sqrt(2). This means you can encode the smallest three components in [-0.707107,+0.707107] instead of [-1,+1] giving you more precision with the same number of bits.\nWith this technique I\u0026rsquo;ve found that minimum sufficient precision for my simulation is 9 bits per-smallest component. This gives a result of 2 + 9 + 9 + 9 = 29 bits per-orientation (down from 128 bits).\nYour browser does not support the video tag. This optimization reduces bandwidth by over 5 megabits per-second, and I think if you look at the right side, you\u0026rsquo;d be hard pressed to spot any artifacts from the compression.\nOptimizing Linear Velocity What should we optimize next? It\u0026rsquo;s a tie between linear velocity and position. Both are 96 bits. In my experience position is the harder quantity to compress so let\u0026rsquo;s start here.\nTo compress linear velocity we need to bound its x,y,z components in some range so we don\u0026rsquo;t need to send full float values. I found that a maximum speed of 32 meters per-second is a nice power of two and doesn\u0026rsquo;t negatively affect the player experience in the cube simulation. Since we\u0026rsquo;re really only using the linear velocity as a hint to improve interpolation between position sample points we can be pretty rough with compression. 32 distinct values per-meter per-second provides acceptable precision.\nLinear velocity has been bounded and quantized and is now three integers in the range [-1024,1023]. That breaks down as follows: [-32,+31] (6 bits) for integer component and multiply 5 bits fraction precision. I hate messing around with sign bits so I just add 1024 to get the value in range [0,2047] and send that instead. To decode on receive just subtract 1024 to get back to signed integer range before converting to float.\n11 bits per-component gives 33 bits total per-linear velocity. Just over 1/3 the original uncompressed size!\nWe can do even better than this because most cubes are stationary. To take advantage of this we just write a single bit \u0026ldquo;at rest\u0026rdquo;. If this bit is 1, then velocity is implicitly zero and is not sent. Otherwise, the compressed velocity follows after the bit (33 bits). Cubes at rest now cost just 127 bits, while cubes that are moving cost one bit more than they previously did: 159 + 1 = 160 bits.\nYour browser does not support the video tag. But why are we sending linear velocity at all? In the previous article we decided to send it because it improved the quality of interpolation at 10 snapshots per-second, but now that we\u0026rsquo;re sending 60 snapshots per-second is this still necessary? As you can see below the answer is no.\nYour browser does not support the video tag. Linear interpolation is good enough at 60HZ. This means we can avoid sending linear velocity entirely. Sometimes the best bandwidth optimizations aren\u0026rsquo;t about optimizing what you send, they\u0026rsquo;re about what you don\u0026rsquo;t send.\nOptimizing Position Now we have only position to compress. We\u0026rsquo;ll use the same trick we used for linear velocity: bound and quantize. I chose a position bound of [-256,255] meters in the horizontal plane (xy) and since in the cube simulation the floor is at z=0, I chose a range of [0,32] meters for z.\nNow we need to work out how much precision is required. With experimentation I found that 512 values per-meter (roughly 2mm precision) provides enough precision. This gives position x and y components in [-131072,+131071] and z components in range [0,16383]. That\u0026rsquo;s 18 bits for x, 18 bits for y and 14 bits for z giving a total of 50 bits per-position (originally 96).\nThis reduces our cube state to 80 bits, or just 10 bytes per-cube.\nThis is approximately 1/4 of the original cost. Definite progress!\nYour browser does not support the video tag. Now that we\u0026rsquo;ve compressed position and orientation we\u0026rsquo;ve run out of simple optimizations. Any further reduction in precision results in unacceptable artifacts.\nDelta Compression Can we optimize further? The answer is yes, but only if we embrace a completely new technique: delta compression.\nDelta compression sounds mysterious. Magical. Hard. Actually, it\u0026rsquo;s not hard at all. Here\u0026rsquo;s how it works: the left side sends packets to the right like this: \u0026ldquo;This is snapshot 110 encoded relative to snapshot 100\u0026rdquo;. The snapshot being encoded relative to is called the baseline. How you do this encoding is up to you, there are many fancy tricks, but the basic, big order of magnitude win comes when you say: \u0026ldquo;Cube n in snapshot 110 is the same as the baseline. One bit: Not changed!\u0026rdquo;\nTo implement delta encoding it is of course essential that the sender only encodes snapshots relative to baselines that the other side has received, otherwise they cannot decode the snapshot. Therefore, to handle packet loss the receiver has to continually send \u0026ldquo;ack\u0026rdquo; packets back to the sender saying: \u0026ldquo;the most recent snapshot I have received is snapshot n\u0026rdquo;. The sender takes this most recent ack and if it is more recent than the previous ack updates the baseline snapshot to this value. The next time a packet is sent out the snapshot is encoded relative to this more recent baseline. This process happens continuously such that the steady state becomes the sender encoding snapshots relative to a baseline that is roughly RTT (round trip time) in the past.\nThere is one slight wrinkle: for one round trip time past initial connection the sender doesn\u0026rsquo;t have any baseline to encode against because it hasn\u0026rsquo;t received an ack from the receiver yet. I handle this by adding a single flag to the packet that says: \u0026ldquo;this snapshot is encoded relative to the initial state of the simulation\u0026rdquo; which is known on both sides. Another option if the receiver doesn\u0026rsquo;t know the initial state is to send down the initial state using a non-delta encoded path, eg. as one large data block, and once that data block has been received delta encoded snapshots are sent first relative to the initial baseline in the data block, then eventually converge to the steady state of baselines at RTT.\nYour browser does not support the video tag. As you can see above this is a big win. We can refine this approach and lock in more gains but we\u0026rsquo;re not going to get another order of magnitude improvement past this point. From now on we\u0026rsquo;re going to have to work pretty hard to get a number of small, cumulative gains to reach our goal of 256 kilobits per-second.\nIncremental Improvements First small improvement. Each cube that isn\u0026rsquo;t sent costs 1 bit (not changed). There are 901 cubes so we send 901 bits in each packet even if no cubes have changed. At 60 packets per-second this adds up to 54kbps of bandwidth. Seeing as there are usually significantly less than 901 changed cubes per-snapshot in the common case, we can reduce bandwidth by sending only changed cubes with a cube index [0,900] identifying which cube it is. To do this we need to add a 10 bit index per-cube to identify it.\nThere is a cross-over point where it is actually more expensive to send indices than not-changed bits. With 10 bit indices, the cost of indexing is 10*n bits. Therefore it\u0026rsquo;s more efficient to use indices if we are sending 90 cubes or less (900 bits). We can evaluate this per-snapshot and send a single bit in the header indicating which encoding we are using: 0 = indexing, 1 = changed bits. This way we can use the most efficient encoding for the number of changed cubes in the snapshot.\nThis reduces the steady state bandwidth when all objects are stationary to around 15 kilobits per-second. This bandwidth is composed entirely of our own packet header (uint16 sequence, uint16 base, bool initial) plus IP and UDP headers (28 bytes).\nNext small gain. What if we encoded the cube index relative to the previous cube index? Since we are iterating across and sending changed cube indices in-order: cube 0, cube 10, cube 11, 50, 52, 55 and so on we could easily encode the 2nd and remaining cube indices relative to the previous changed index, e.g.: +10, +1, +39, +2, +3. If we are smart about how we encode this index offset we should be able to, on average, represent a cube index with less than 10 bits.\nThe best encoding depends on the set of objects you interact with. If you spend a lot of time moving horizontally while blowing cubes from the initial cube grid then you hit lots of +1s. If you move vertically from initial state you hit lots of +30s (sqrt(900)). What we need then is a general purpose encoding capable of representing statistically common index offsets with less bits.\nAfter a small amount of experimentation I came up with this simple encoding:\n[1,8] =\u0026gt; 1 + 3 (4 bits) [9,40] =\u0026gt; 1 + 1 + 5 (7 bits) [41,900] =\u0026gt; 1 + 1 + 10 (12 bits) Notice how large relative offsets are actually more expensive than 10 bits. It\u0026rsquo;s a statistical game. The bet is that we\u0026rsquo;re going to get a much larger number of small offsets so that the win there cancels out the increased cost of large offsets. It works. With this encoding I was able to get an average of 5.5 bits per-relative index.\nNow we have a slight problem. We can no longer easily determine whether changed bits or relative indices are the best encoding. The solution I used is to run through a mock encoding of all changed cubes on packet write and count the number of bits required to encode relative indices. If the number of bits required is larger than 901, fallback to changed bits.\nHere is where we are so far, which is a significant improvement:\nYour browser does not support the video tag. Next small improvement. Encoding position relative to (offset from) the baseline position. Here there are a lot of different options. You can just do the obvious thing, eg. 1 bit relative position, and then say 8-10 bits per-component if all components have deltas within the range provided by those bits, otherwise send the absolute position (50 bits).\nThis gives a decent encoding but we can do better. If you think about it then there will be situations where one position component is large but the others are small. It would be nice if we could take advantage of this and send these small components using less bits.\nIt\u0026rsquo;s a statistical game and the best selection of small and large ranges per-component depend on the data set. I couldn\u0026rsquo;t really tell looking at a noisy bandwidth meter if I was making any gains so I captured the position vs. position base data set and wrote it to a text file for analysis.\nI wrote a short ruby script to find the best encoding with a greedy search. The best bit-packed encoding I found for the data set works like this: 1 bit small per delta component followed by 5 bits if small [-16,+15] range, otherwise the delta component is in [-256,+255] range and is sent with 9 bits. If any component delta values are outside the large range, fallback to absolute position. Using this encoding I was able to obtain on average 26.1 bits for changed positions values.\nDelta Encoding Smallest Three Next I figured that relative orientation would be a similar easy big win. Problem is that unlike position where the range of the position offset is quite small relative to the total position space, the change in orientation in 100ms is a much larger percentage of total quaternion space.\nI tried a bunch of stuff without good results. I tried encoding the 4D vector of the delta orientation directly and recomposing the largest component post delta using the same trick as smallest 3. I tried calculating the relative quaternion between orientation and base orientation, and since I knew that w would be large for this (rotation relative to identity) I could avoid sending 2 bits to identify the largest component, but in turn would need to send one bit for the sign of w because I don\u0026rsquo;t want to negate the quaternion. The best compression I could find using this scheme was only 90% of the smallest three. Not very good.\nI was about to give up but I run some analysis over the smallest three representation. I found that 90% of orientations in the smallest three format had the same largest component index as their base orientation 100ms ago. This meant that it could be profitable to delta encode the smallest three format directly. What\u0026rsquo;s more I found that there would be no additional precision loss with this method when reconstructing the orientation from its base. I exported the quaternion values from a typical run as a data set in smallest three format and got to work trying the same multi-level small/large range per-component greedy search that I used for position.\nThe best encoding found was: 5-8, meaning [-16,+15] small and [-128,+127] large. One final thing: as with position the large range can be extended a bit further by knowing that if the component value is not small the value cannot be in the [-16,+15] range. I leave the calculation of how to do this as an exercise for the reader. Be careful not to collapse two values onto zero.\nThe end result is an average of 23.3 bits per-relative quaternion. That\u0026rsquo;s 80.3% of the absolute smallest three.\nThat\u0026rsquo;s just about it but there is one small win left. Doing one final analysis pass over the position and orientation data sets I noticed that 5% of positions are unchanged from the base position after being quantized to 0.5mm resolution, and 5% of orientations in smallest three format are also unchanged from base.\nThese two probabilities are mutually exclusive, because if both are the same then the cube would be unchanged and therefore not sent, meaning a small statistical win exists for 10% of cube state if we send one bit for position changing, and one bit for orientation changing. Yes, 90% of cubes have 2 bits overhead added, but the 10% of cubes that save 20+ bits by sending 2 bits instead of 23.3 bit orientation or 26.1 bits position make up for that providing a small overall win of roughly 2 bits per-cube.\nYour browser does not support the video tag. As you can see the end result is pretty good.\nConclusion And that\u0026rsquo;s about as far as I can take it using traditional hand-rolled bit-packing techniques. You can find source code for my implementation of all compression techniques mentioned in this article here.\nIt\u0026rsquo;s possible to get even better compression using a different approach. Bit-packing is inefficient because not all bit values have equal probability of 0 vs 1. No matter how hard you tune your bit-packer a context aware arithmetic encoding can beat your result by more accurately modeling the probability of values that occur in your data set. This implementation by Fabian Giesen beat my best bit-packed result by 25%.\nIt\u0026rsquo;s also possible to get a much better result for delta encoded orientations using the previous baseline orientation values to estimate angular velocity and predict future orientations rather than delta encoding the smallest three representation directly.\nNEXT ARTICLE: State Synchronization\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1420329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420329600,"objectID":"b519983f87b6ece7e3869b2449cec756","permalink":"https://gafferongames.com/post/snapshot_compression/","publishdate":"2015-01-04T00:00:00Z","relpermalink":"/post/snapshot_compression/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we sent snapshots of the entire simulation 10 times per-second over the network and interpolated between them to reconstruct a view of the simulation on the other side.\nThe problem with a low snapshot rate like 10HZ is that interpolation between snapshots adds interpolation delay on top of network latency. At 10 snapshots per-second, the minimum interpolation delay is 100ms, and a more practical minimum considering network jitter is 150ms.","tags":["physics","networking"],"title":"Snapshot Compression","type":"post"},{"authors":null,"categories":["Networked Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we networked a physics simulation using deterministic lockstep. Now, in this article we\u0026rsquo;re going to network the same simulation with a completely different technique: snapshot interpolation.\nBackground While deterministic lockstep is very efficient in terms of bandwidth, it\u0026rsquo;s not always possible to make your simulation deterministic. Floating point determinism across platforms is hard.\nAlso, as the player counts increase, deterministic lockstep becomes problematic: you can\u0026rsquo;t simulate frame n until you receive input from all players for that frame, so players end up waiting for the most lagged player. Because of this, I recommend deterministic lockstep for 2-4 players at most.\nSo if your simulation is not deterministic or you want higher player counts then you need a different technique. Snapshot interpolation fits the bill nicely. It is in many ways the polar opposite of deterministic lockstep: instead of running two simulations, one on the left and one on the right, and using perfect determinism and synchronized inputs keep them in sync, snapshot interpolation doesn\u0026rsquo;t run any simulation on the right side at all!\nSnapshots Instead, we capture a snapshot of all relevant state from the simulation on the left and transmit it to the right, then on the right side we use those snapshots to reconstruct a visual approximation of the simulation, all without running the simulation itself.\nAs a first pass, let\u0026rsquo;s send across the state required to render each cube:\nstruct CubeState { bool interacting; vec3f position; quat4f orientation; }; I\u0026rsquo;m sure you\u0026rsquo;ve worked out by now that the cost of this technique is increased bandwidth usage. Greatly increased bandwidth usage. Hold on to your neckbeards, because a snapshot contains the visual state for the entire simulation. With a bit of math we can see that each cube serializes down to 225 bits or 28.1 bytes. Since there are 900 cubes in our simulation that means each snapshot is roughly 25 kilobytes. That\u0026rsquo;s pretty big!\nAt this point I would like everybody to relax, take a deep breath, and imagine we live in a world where I can actually send a packet this large 60 times per-second over the internet and not have everything explode. Imagine I have FIOS (I do), or I\u0026rsquo;m sitting over a backbone link to another computer that is also on the backbone. Imagine I live in South Korea. Do whatever you need to do to suspend disbelief, but most of all, don\u0026rsquo;t worry, because I\u0026rsquo;m going to spend the entire next article showing you how to optimize snapshot bandwidth.\nWhen we send snapshot data in packets, we include at the top a 16 bit sequence number. This sequence number starts at zero and increases with each packet sent. We use this sequence number on receive to determine if the snapshot in a packet is newer or older than the most recent snapshot received. If it\u0026rsquo;s older then it\u0026rsquo;s thrown away.\nEach frame we just render the most recent snapshot received on the right:\nLook closely though, and even though we\u0026rsquo;re sending the data as rapidly as possible (one packet per-frame) you can still see hitches on the right side. This is because the internet makes no guarantee that packets sent 60 times per-second arrive nicely spaced 1/60 of a second apart. Packets are jittered. Some frames you receive two snapshot packets. Other frames you receive none.\nJitter and Hitches This is actually a really common thing when you first start networking. You start out playing your game over LAN and notice you can just slam out packets really fast (60pps) and most of the time your game looks great because over the LAN those packets actually do tend to arrive at the same rate they were sent\u0026hellip; and then you start trying to play your game over wireless or the internet and you start seeing hitches. Don\u0026rsquo;t worry. There are ways to handle this!\nFirst, let\u0026rsquo;s look at how much bandwidth we\u0026rsquo;re sending with this naive approach. Each packet is 25312.5 bytes plus 28 bytes for IP + UDP header and 2 bytes for sequence number. That\u0026rsquo;s 25342.5 bytes per-packet and at 60 packets per-second this gives a total of 1520550 bytes per-second or 11.6 megabit/sec. Now there are certainly internet connections out there that can support that amount of traffic\u0026hellip; but since, let\u0026rsquo;s be honest, we\u0026rsquo;re not really getting a lot of benefit blasting packets out 60 times per-second with all the jitter, let\u0026rsquo;s pull it back a bit and send only 10 snapshots per-second:\nYou can see how this looks above. Not so great on the right side but at least we\u0026rsquo;ve reduced bandwidth by a factor of six to around 2 megabit/sec. We\u0026rsquo;re definitely headed in the right direction.\nLinear Interpolation Now for the trick with snapshots. What we do is instead of immediately rendering snapshot data received is that we buffer snapshots for a short amount of time in an interpolation buffer. This interpolation buffer holds on to snapshots for a period of time such that you have not only the snapshot you want to render but also, statistically speaking, you are very likely to have the next snapshot as well. Then as the right side moves forward in time we interpolate between the position and orientation for the two slightly delayed snapshots providing the illusion of smooth movement. In effect, we\u0026rsquo;ve traded a small amount of added latency for smoothness.\nYou may be surprised at just how good it looks with linear interpolation @ 10pps:\nLook closely though and you can see some artifacts on the right side. The first is a subtle position jitter when the player cube is hovering in the air. This is your brain detecting 1st order discontinuity at the sample points of position interpolation. The other artifact occurs when a bunch of cubes are in a katamari ball, you can see a sort of \u0026ldquo;pulsing\u0026rdquo; as the speed of rotation increases and decreases. This occurs because attached cubes interpolate linearly between two sample points rotating around the player cube, effectively interpolating through the player cube as they take the shortest linear path between two points on a circle.\nHermite Interpolation I find these artifacts unacceptable but I don\u0026rsquo;t want to increase the packet send rate to fix them. Let\u0026rsquo;s see what we can do to make it look better at the same send rate instead. One thing we can try is upgrading to a more accurate interpolation scheme for position, one that interpolates between position samples while considering the linear velocity at each sample point.\nThis can be done with an hermite spline (pronounced \u0026ldquo;air-mitt\u0026rdquo;)\nUnlike other splines with control points that affect the curve indirectly, the hermite spline is guaranteed to pass through the start and end points while matching the start and end velocities. This means that velocity is smooth across sample points and cubes in the katamari ball tend to rotate around the cube rather than interpolate through it at speed.\nAbove you can see hermite interpolation for position @ 10pps. Bandwidth has increased slightly because we need to include linear velocity with each cube in the snapshot, but we\u0026rsquo;re able to significantly increase the quality at the same send rate. I can no longer see any artifacts. Go back and compare this with the raw, non-interpolated 10pps version. It really is amazing that we\u0026rsquo;re able to reconstruct the simulation with this level of quality at such a low send rate.\nAs an aside, I found it was not necessary to perform higher order interpolation for orientation quaternions to get smooth interpolation. This is great because I did a lot of research into exactly interpolating between orientation quaternions with a specified angular velocity at sample points and it seemed difficult. All that was needed to achieve an acceptable result was to switch from linear interpolation + normalize (nlerp) to spherical linear interpolation (slerp) to ensure constant angular speed for orientation interpolation.\nI believe this is because cubes in the simulation tend to have mostly constant angular velocity while in the air and large angular velocity changes occur only discontinuously when collisions occur. It could also be because orientation tends to change slowly while in the air vs. position which changes rapidly relative to the number of pixels affected on screen. Either way, it seems that slerp is good enough and that\u0026rsquo;s great because it means we don\u0026rsquo;t need to send angular velocity in the snapshot.\nHandling Real World Conditions Now we have to deal with packet loss. After the discussion of UDP vs. TCP in the previous article I\u0026rsquo;m sure you can see why we would never consider sending snapshots over TCP.\nSnapshots are time critical but unlike inputs in deterministic lockstep snapshots don\u0026rsquo;t need to be reliable. If a snapshot is lost we can just skip past it and interpolate towards a more recent snapshot in the interpolation buffer. We don\u0026rsquo;t ever want to stop and wait for a lost snapshot packet to be resent. This is why you should always use UDP for sending snapshots.\nI\u0026rsquo;ll let you in on a secret. Not only were the linear and hermite interpolation videos above recorded at a send rate of 10 packets per-second, they were also recorded at 5% packet loss with +/- 2 frames of jitter @ 60fps. How I handled packet loss and jitter for those videos is by simply ensuring that snapshots are held in the interpolation buffer for an appropriate amount of time before interpolation.\nMy rule of thumb is that the interpolation buffer should have enough delay so that I can lose two packets in a row and still have something to interpolate towards. Experimentally I\u0026rsquo;ve found that the amount of delay that works best at 2-5% packet loss is 3X the packet send rate. At 10 packets per-second this is 300ms. I also need some extra delay to handle jitter, which in my experience is typically only one or two frames @ 60fps, so the interpolation videos above were recorded with a delay of 350ms.\nAdding 350 milliseconds delay seems like a lot. And it is. But, if you try to skimp you end up hitching for 1/10th of a second each time a packet is lost. One technique that people often use to hide the delay added by the interpolation buffer in other areas (such as FPS, flight simulator, racing games and so on) is to use extrapolation. But in my experience, extrapolation doesn\u0026rsquo;t work very well for rigid bodies because their motion is non-linear and unpredictable. Here you can see an extrapolation of 200ms, reducing overall delay from 350 ms to just 150ms:\nProblem is it\u0026rsquo;s just not very good. The reason is that the extrapolation doesn\u0026rsquo;t know anything about the physics simulation. Extrapolation doesn\u0026rsquo;t know about collision with the floor so cubes extrapolate down through the floor and then spring back up to correct. Prediction doesn\u0026rsquo;t know about the spring force holding the player cube up in the air so it the cube moves slower initially upwards than it should and has to snap to catch up. It also doesn\u0026rsquo;t know anything about collision and how collision response works, so the cube rolling across the floor and other cubes are also mispredicted. Finally, if you watch the katamari ball you\u0026rsquo;ll see that the extrapolation predicts the attached cubes as continuing to move along their tangent velocity when they should rotate with the player cube.\nConclusion You could conceivably spend a great deal of time to improve the quality of this extrapolation and make it aware of various movement modes for the cubes. You could take each cube and make sure that at minimum the cube doesn\u0026rsquo;t go through the floor. You could add some approximate collision detection or response using bounding spheres between cubes. You could even take the cubes in the katamari ball and make them predict motion to rotate around with the player cube.\nBut even if you do all this there will still be misprediction because you simply can\u0026rsquo;t accurately match a physics simulation with an approximation. If your simulation is mostly linear motion, eg. fast moving planes, boats, space ships \u0026ndash; you may find that a simple extrapolation works well for short time periods (50-250ms or so), but in my experience as soon as objects start colliding with other non-stationary objects, extrapolation starts to break down.\nHow can we reduce the amount of delay added for interpolation? 350ms still seems unacceptable and we can\u0026rsquo;t use extrapolation to reduce this delay without adding a lot of inaccuracy. The solution is simple: increase the send rate! If we send 30 snapshots per-second we can get the same amount of packet loss protection with a delay of 150ms. 60 packets per-second needs only 85ms.\nIn order to increase the send rate we\u0026rsquo;re going to need some pretty good bandwidth optimizations. But don\u0026rsquo;t worry, there\u0026rsquo;s a lot we can do to optimize bandwidth. So much so that there was too much stuff to fit in this article and I had to insert an extra unplanned article just to cover all of it!\nNEXT ARTICLE: Snapshot Compression\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1417305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417305600,"objectID":"861f5ff716a89c08c70c318e5fdbd256","permalink":"https://gafferongames.com/post/snapshot_interpolation/","publishdate":"2014-11-30T00:00:00Z","relpermalink":"/post/snapshot_interpolation/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we networked a physics simulation using deterministic lockstep. Now, in this article we\u0026rsquo;re going to network the same simulation with a completely different technique: snapshot interpolation.\nBackground While deterministic lockstep is very efficient in terms of bandwidth, it\u0026rsquo;s not always possible to make your simulation deterministic. Floating point determinism across platforms is hard.\nAlso, as the player counts increase, deterministic lockstep becomes problematic: you can\u0026rsquo;t simulate frame n until you receive input from all players for that frame, so players end up waiting for the most lagged player.","tags":["physics","networking"],"title":"Snapshot Interpolation","type":"post"},{"authors":null,"categories":["Networked Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we explored the physics simulation we\u0026rsquo;re going to network in this article series. In this article specifically, we\u0026rsquo;re going to network this physics simulation using deterministic lockstep.\nDeterministic lockstep is a method of networking a system from one computer to another by sending only the inputs that control that system, rather than the state of that system. In the context of networking a physics simulation, this means we send across a small amount of input, while avoiding sending state like position, orientation, linear velocity and angular velocity per-object.\nThe benefit is that bandwidth is proportional to the size of the input, not the number of objects in the simulation. Yes, with deterministic lockstep you can network a physics simulation of one million objects with the same bandwidth as just one.\nWhile this sounds great in theory, in practice it\u0026rsquo;s difficult to implement deterministic lockstep because most physics simulations are not deterministic. Differences in floating point behavior between compilers, OS\u0026rsquo;s and even instruction sets make it almost impossible to guarantee determinism for floating point calculations.\nDeterminism Determinism means that given the same initial condition and the same set of inputs your simulation gives exactly the same result. And I do mean exactly the same result.\nNot close. Not near enough. Exactly the same. Exact down to the bit-level. So exact, you could take a checksum of your entire physics state at the end of each frame and it would be identical.\nAbove you can see a simulation that is almost deterministic. The simulation on the left is controlled by the player. The simulation on the right has exactly the same inputs applied with a two second delay starting from the same initial condition. Both simulations step forward with the same delta time (a necessary precondition to ensure exactly the same result) and both simulations apply the same inputs. Notice how after the smallest divergence the simulation gets further and further out of sync. This simulation is non-deterministic.\nWhat\u0026rsquo;s going on is that the physics engine I\u0026rsquo;m using (Open Dynamics Engine) uses a random number generator inside its solver to randomize the order of constraint processing to improve stability. It\u0026rsquo;s open source. Take a look and see! Unfortunately this breaks determinism because the simulation on the left processes constraints in a different order to the simulation on the right, leading to slightly different results.\nLuckily all that is required to make ODE deterministic on the same machine, with the same complied binary and on the same OS (is that enough qualifications?) is to set its internal random seed to the current frame number before running the simulation via dSetRandomSeed. Once this is done ODE gives exactly the same result and the left and right simulations stay in sync.\nAnd now a word of warning. Even though the simulation above is deterministic on the same machine, that does not necessarily mean it would also be deterministic across different compilers, a different OS or different machine architectures (eg. PowerPC vs. Intel). In fact, it\u0026rsquo;s probably not even deterministic between debug and release builds due to floating point optimizations.\nFloating point determinism is a complicated subject and there\u0026rsquo;s no silver bullet.\nFor more information please refer to this article.\nNetworking Inputs Now let\u0026rsquo;s get down to implementation.\nOur example physics simulation is driven by keyboard input: arrow keys apply forces to make the player cube move, holding space lifts the cube up and blows other cubes around, and holding \u0026lsquo;z\u0026rsquo; enables katamari mode.\nHow can we network these inputs? Must we send the entire state of the keyboard? No. It\u0026rsquo;s not necessary to send the entire keyboard state, only the state of the keys that affect the simulation. What about key press and release events then? No. This is also not a good strategy. We need to ensure that exactly the same input is applied on the right side, at exactly the same time, so we can\u0026rsquo;t just send \u0026lsquo;key pressed\u0026rsquo;, and \u0026lsquo;key released\u0026rsquo; events over TCP.\nWhat we do instead is represent the input with a struct and at the beginning of each simulation frame on the left side, sample this struct from the keyboard:\nstruct Input { bool left; bool right; bool up; bool down; bool space; bool z; }; Next we send that input from the left simulation to the right simulation in a way that the simulation on the right side knows that the input belongs to frame n.\nAnd here\u0026rsquo;s the key part: the simulation on the right can only simulate frame n when it has the input for that frame. If it doesn\u0026rsquo;t have the input, it has to wait.\nFor example, if you were sending across using TCP you could simply send the inputs and nothing else, and on the other side you could read the packets coming in, and each input received corresponds to one frame for the simulation to step forward. If no input arrives for a given render frame, the right side can\u0026rsquo;t advance forward, it has to wait for the next input to arrive.\nSo let\u0026rsquo;s move forward with TCP, you\u0026rsquo;ve disabled Nagle\u0026rsquo;s Algorithm, and you\u0026rsquo;re sending inputs from the left to the right simulation once per-frame (60 times per-second).\nHere it gets a little complicated. Since we can\u0026rsquo;t simulate forward unless we have the input for the next frame, it\u0026rsquo;s not enough to just take whatever inputs arrive over the network and then run the simulation on inputs as they arrive because the result would be very jittery. Data sent across the network at 60HZ doesn\u0026rsquo;t typically arrive nicely spaced, 1/60th of a second between each packet.\nIf you want this sort of behavior, you have to implement it yourself.\nPlayout Delay Buffer Such a device is called a playout delay buffer.\nUnfortunately, the subject of playout delay buffers is a patent minefield. I would not advise searching for \u0026ldquo;playout delay buffer\u0026rdquo; or \u0026ldquo;adaptive playout delay\u0026rdquo; while at work. But in short, what you want to do is buffer packets for a short amount of time so they appear to be arriving at a steady rate even though in reality they arrive somewhat jittered.\nWhat you\u0026rsquo;re doing here is similar to what Netflix does when you stream a video. You pause a little bit initially so you have a buffer in case some packets arrive late and then once the delay has elapsed video frames are presented spaced the correct time apart. If your buffer isn\u0026rsquo;t large enough then the video playback will be hitchy. With deterministic lockstep your simulation behaves exactly the same way: showing hitches when the buffer isn\u0026rsquo;t large enough to smooth out the jitter. Of course, the cost of increasing the buffer size is additional latency, so you can\u0026rsquo;t just buffer your way out of all problems. At some point the user says enough! That\u0026rsquo;s too much latency added. No sir, I will not play your game with 1 second of extra delay :)\nMy playout delay buffer implementation is really simple. You add inputs to it indexed by frame, and when the very first input is received, it stores the current local time on the receiver machine and from that point on delivers packets assuming they should play at that time + 100ms. You\u0026rsquo;ll likely need to something more complex for a real world situation, perhaps something that handles clock drift, and detecting when the simulation should slightly speed up or slow down to maintain a nice amount of buffering safety (being \u0026ldquo;adaptive\u0026rdquo;) while minimizing overall latency, but this is reasonably complicated and probably worth an article in itself.\nThe goal is that under average conditions the playout delay buffer provides a steady stream of inputs for frame n, n+1, n+2 and so on, nicely spaced 1/60th of a second apart with no drama. In the worst case the time arrives for frame n and the input hasn\u0026rsquo;t arrived yet it returns null and the simulation is forced to wait. If packets get bunched up and delivered late, it\u0026rsquo;s possibly to have multiple inputs ready to dequeue per-frame. In this case I limit to 4 simulated frames per-render frame so the simulation has a chance to catch up, but doesn\u0026rsquo;t simulate for so long that it falls further behind, aka. the \u0026ldquo;spiral of death\u0026rdquo;.\nIs TCP good enough? Using this playout buffer strategy and sending inputs across TCP we ensure that all inputs arrive reliably and in-order. This is convenient, and after all, TCP is designed for exactly this situation: reliable-ordered data.\nIn fact, It\u0026rsquo;s a common thing out there on the Internet for pundits to say stuff like:\nIf you need reliable-ordered, you can\u0026rsquo;t do better than TCP!\nYour game doesn\u0026rsquo;t need UDP (yet)\nBut I\u0026rsquo;m here to tell you this kind of thinking is dead wrong.\nAbove you can see the simulation networked using deterministic lockstep over TCP at 100ms latency and 1% packet loss. If you look closely on the right side you can see hitches every few seconds. What\u0026rsquo;s happening here is that each time a packet is lost, TCP has to wait RTT*2 while it is resent (actually it can be much worse, but I\u0026rsquo;m being generous\u0026hellip;). The hitches happen because with deterministic lockstep the right simulation can\u0026rsquo;t simulate frame n without input n, so it has to pause to wait for input n to be resent!\nThat\u0026rsquo;s not all. It gets significantly worse as latency and packet loss increase. Here is the same simulation networked using deterministic lockstep over TCP at 250ms latency and 5% packet loss:\nNow I will concede that if you have no packet loss and/or a very small amount of latency then you very well may get acceptable results with TCP. But please be aware that if you use TCP it behaves terribly under bad network conditions.\nCan we do better than TCP? Can we beat TCP at its own game. Reliable-ordered delivery?\nThe answer is an emphatic YES. But only if we change the rules of the game.\nHere\u0026rsquo;s the trick. We need to ensure that all inputs arrive reliably and in order. But if we send inputs in UDP packets, some of those packets will be lost. What if, instead of detecting packet loss after the fact and resending lost packets, we redundantly include all inputs in each UDP packet until we know for sure the other side has received them?\nInputs are very small (6 bits). Let\u0026rsquo;s say we\u0026rsquo;re sending 60 inputs per-second (60fps simulation) and round trip time we know is going the be somewhere in 30-250ms range. Let\u0026rsquo;s say just for fun that it could be up to 2 seconds worst case and at this point we\u0026rsquo;ll time out the connection (screw that guy). This means that on average we only need to include between 2-15 frames of input and worst case we\u0026rsquo;ll need 120 inputs. Worst case is 120*6 = 720 bits. That\u0026rsquo;s only 90 bytes of input! That\u0026rsquo;s totally reasonable.\nWe can do even better. It\u0026rsquo;s not common for inputs to change every frame. What if when we send our packet instead we start with the sequence number of the most recent input, and the 6 bits of the first (oldest) input, and the number of un-acked inputs. Then as we iterate across these inputs to write them to the packet we can write a single bit (1) if the next input is different to the previous, and (0) if the input is the same. So if the input is different from the previous frame we write 7 bits (rare). If the input is identical we write just one (common). Where inputs change infrequently this is a big win and in the worst case this really isn\u0026rsquo;t that bad. 120 bits of extra data sent. Just 15 bytes overhead worst case.\nOf course another packet is required from the right simulation to the left so the left side knows which inputs have been received. Each frame the right simulation reads input packets from the network before adding them to the playout delay buffer and keeps track of the most recent input it has received and sends this back to the left as an \u0026ldquo;ack\u0026rdquo; or acknowledgment for inputs.\nWhen the left side receives this ack it discards any inputs older than the most recent received input. This way we have only a small number of inputs in flight proportional to the round trip time between the two simulations.\nFlawless Victory We have beaten TCP by changing the rules of the game.\nInstead of \u0026ldquo;implementing 95% of TCP on top of UDP\u0026rdquo; we have implemented something totally different and better suited to our requirements. A protocol that redundantly sends inputs because we know they are small, so we never have to wait for retransmission.\nSo exactly how much better is this approach than sending inputs over TCP?\nLet\u0026rsquo;s take a look\u0026hellip;\nThe video above shows deterministic lockstep synchronized over UDP using this technique with 2 seconds of latency and 25% packet loss. Imagine how awful TCP would look under these conditions.\nSo in conclusion, even where TCP should have the most advantage, in the only networking model that relies on reliable-ordered data, we can still easily whip its ass with a simple protocol built on top of UDP.\nNEXT ARTICLE: Snapshot Interpolation\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1417219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417219200,"objectID":"389c8d147569f6d63c1bf98d3263a213","permalink":"https://gafferongames.com/post/deterministic_lockstep/","publishdate":"2014-11-29T00:00:00Z","relpermalink":"/post/deterministic_lockstep/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networked Physics.\nIn the previous article we explored the physics simulation we\u0026rsquo;re going to network in this article series. In this article specifically, we\u0026rsquo;re going to network this physics simulation using deterministic lockstep.\nDeterministic lockstep is a method of networking a system from one computer to another by sending only the inputs that control that system, rather than the state of that system.","tags":["physics","networking"],"title":"Deterministic Lockstep","type":"post"},{"authors":null,"categories":["Networked Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to the first article in Networked Physics.\nIn this article series we\u0026rsquo;re going to network a physics simulation three different ways: deterministic lockstep, snapshot interpolation and state synchronization.\nBut before we get to this, let\u0026rsquo;s spend some time exploring the physics simulation we’re going to network in this article series:\nYour browser does not support the video tag. Here I’ve setup a simple simulation of a cube in the open source physics engine ODE. The player moves around by applying forces at its center of mass. The physics simulation takes this linear motion and calculates friction as the cube collides with the ground, inducing a rolling and tumbling motion.\nThis is why I chose a cube instead a sphere. I want this complex, unpredictable motion because rigid bodies in general move in interesting ways according to their shape.\nAn Interactive World Networked physics get interesting when the player interacts with other physically simulated objects, especially when those objects push back and affect the motion of the player.\nSo let\u0026rsquo;s add some more cubes to the simulation:\nYour browser does not support the video tag. When the player interacts with a cube it turns red. When that cube comes to rest it turns back to grey (non-interacting).\nWhile it’s cool to roll around and interact with other cubes, what I really wanted was a way to push lots of cubes around. What I came up with is this:\nYour browser does not support the video tag. As you can see, interactions aren’t just direct. Red cubes pushed around by the player turn other cubes they touch red as well. This way, interactions fan out to cover all affected objects.\nA Complicated Case I also wanted a very complex coupled motion between the player and non-player cubes such they become one system: a group of rigid bodies joined together by constraints.\nTo implement this I thought it would be cool if the player could roll around and create a ball of cubes, like in one of my favorite games Katamari Damacy.\nYour browser does not support the video tag. Cubes within a certain distance of the player have a force applied towards the center of the cube. These cubes remain physically simulated while in the katamari ball, they are not just “stuck” to the player like in the original game.\nThis is a very difficult situation for networked physics!\nNEXT ARTICLE: Deterministic Lockstep\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1417132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417132800,"objectID":"d9d832b1d947b2d56eaf445e2b34d8b5","permalink":"https://gafferongames.com/post/introduction_to_networked_physics/","publishdate":"2014-11-28T00:00:00Z","relpermalink":"/post/introduction_to_networked_physics/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to the first article in Networked Physics.\nIn this article series we\u0026rsquo;re going to network a physics simulation three different ways: deterministic lockstep, snapshot interpolation and state synchronization.\nBut before we get to this, let\u0026rsquo;s spend some time exploring the physics simulation we’re going to network in this article series:\nYour browser does not support the video tag. Here I’ve setup a simple simulation of a cube in the open source physics engine ODE.","tags":["physics","networking"],"title":"Introduction to Networked Physics","type":"post"},{"authors":null,"categories":["Virtual Go"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nSo far in this series, we have mathematically defined the go stone, rendered it, determined how it moves and rotates, and discussed how its shape affects how it responds to collisions.\nNow in this article we reach our first milestone:\nA go stone bouncing and coming to rest on the go board.\nWe\u0026rsquo;re going do this using a technique called impulse-based collision response.\nThe concept is simple. To handle a collision we apply an impulse, an instantaneous change in momentum, at the point of impact to make the go stone bounce.\nLinear Collision Response We now pick up where we left off at the end of the collision detection article.\nWe have a contact point and a contact normal for the collision.\nLet\u0026rsquo;s start by calculating a collision response impulse without rotation.\nFirst, take the dot product of the linear momentum of the go stone with the contact normal. If this value is less than zero, it means the go stone is moving towards the go board, and we need to apply an impulse.\nTo calculate the impulse we need the concept of \u0026rsquo;elasticity\u0026rsquo;. If the collision is perfectly elastic, the go stone bounces off the board without losing any energy:\nIf the collision is inelastic then the go stone loses all its vertical motion post-collision and slides along the surface of the board:\nWhat we really want is something in between:\nTo support this we introduce a new concept called the \u0026lsquo;coefficient of restitution\u0026rsquo;. When this value is 1 the collision is perfectly elastic, when it is 0 the collision is inelastic. At 0.5, it\u0026rsquo;s halfway between.\nThis gives the following formula:\n[latex]j = -( 1 + e ) \\boldsymbol{p} \\cdot \\boldsymbol{n}[/latex]\nWhere:\nj is the magnitude of the collision impulse e is the coefficient of restitution [0,1] p is the linear momentum of the go stone n in the contact normal for the collision Note that the direction of the collision impulse is always along the contact normal, so to apply the impulse just multiply the contact normal by j and add it to the linear momentum vector.\nHere\u0026rsquo;s the code that does this:\nvoid ApplyLinearCollisionImpulse( StaticContact \u0026amp; contact, float e ) { float mass = contact.rigidBody-\u0026gt;mass; float d = dot( contact.rigidBody-\u0026gt;linearMomentum, contact.normal ); float j = max( - ( 1 + e ) * d, 0 ); contact.rigidBody-\u0026gt;linearMomentum += j * contact.normal; } And here\u0026rsquo;s the result:\nNow the stone is definitely bouncing, but in the real world stones don\u0026rsquo;t usually hit the board perfectly flat like this. In the common case, they hit at an angle and the collision makes the stone rotate.\nCollision Response With Rotation To capture this effect we need to calculate collision response with rotation.\nAbove you can see the effect that we want. If a stone were to collide with the board like this, we know from experience that it would rotate in response.\nWe start by calculating the velocity of the stone at the contact point, and take the dot product of this vs. the contact normal to check if the stone is moving towards the board. This is necessary because when the stone is rotating, different points on the stone have different velocities.\nNext, we apply a collision impulse along the contact normal with magnitude j except this impulse is applied at the contact point instead of the center of mass of the stone. This gives the collision response its rotational effect.\nHere is the general equation for the magnitude of this collision impulse.\nYou can find a derivation of this result on wikipedia.\nUnderstandably this is quite complex, but in our case the go board never moves, so we can simplify the equation by assigning zero velocity and infinite mass to the second body. This leads to the following, simpler equation:\ntodo: need a solution to convert across all the latex equations\u0026hellip;\n[latex]j = \\dfrac{ -( 1 + e ) \\boldsymbol{v} \\cdot \\boldsymbol{n} } { m^{-1} + ( \\boldsymbol{I^{-1}} ( \\boldsymbol{r} \\times \\boldsymbol{n} ) \\times \\boldsymbol{r} ) \\cdot \\boldsymbol{n} }[/latex]\nWhere:\nj is the magnitude of the collision impulse e is the coefficient of restitution [0,1] n in the contact normal for the collision v is the the go stone velocity at the contact point r is the contact point minus the center of the go stone I is the inertia tensor of the go stone m is the mass of the go stone Here is the result of our collision response with rotational effects:\nAs you can see, collision response working properly and induces rotation when the go stone hits the board at an angle. It is also able to handle the stone hitting the board while rotating.\nCoulomb Friction We don\u0026rsquo;t often get to see friction-less collisions in the real world so the video above looks a bit strange. To get realistic behavior out of the go stone, we need to add friction.\nWe\u0026rsquo;ll model sliding friction using the Coulomb friction model.\nIn this model, the friction impulse is proportional the magnitude of the normal impulse j and is limited by a friction cone defined by the coefficient of friction u:\nLower friction coefficient values mean less friction, higher values mean more friction. Typical values for the coefficient of friction are in the range [0,1].\nCalculation of the Coulomb friction impulse is performed much like the calculation of the normal impulse except this time the impulse is in the tangent direction against the direction of sliding.\nHere is the formula for calculating the magnitude of the friction impulse:\n[latex]j_t = \\dfrac{ - \\boldsymbol{v} \\cdot \\boldsymbol{t} } { m^{-1} + ( \\boldsymbol{I^{-1}} ( \\boldsymbol{r} \\times \\boldsymbol{t} ) \\times \\boldsymbol{r} ) \\cdot \\boldsymbol{t} }[/latex]\nWhere:\njt is the magnitude of the friction impulse (pre-cone limit) u is the coefficient of friction [0,1] t in the tangent vector in the direction of sliding v is the the go stone velocity at the contact point r is the contact point minus the center of the go stone I is the inertia tensor of the go stone m is the mass of the go stone Which gives the following result:\nWhich looks much more realistic!\nRolling Friction Due to its shape (and the inertia tensor from the previous article), the go stone really prefers to rotate about axes on the xz plane instead of around the y axis.\nI was able to reproduct this effect in the simulation. Adding a torque that spins go stone around the y axis made it stand up and spin like a coin:\nThis is pretty cool and is totally emergent from the shape of the go stone. The only problem is that it spins like this forever.\nWhy is it spinning for so long? Shouldn\u0026rsquo;t coulomb friction handle this for us?\nNo. Coulomb friction only handles friction when the two surfaces are sliding relative to each other. Here at the point of contact, the stone is spinning about that point, not sliding, so from coulomb friction point of view, the contact point is stationary and no friction is applied.\nIt turns out that sliding friction is just one type of friction and there are many others. What we have in this case is a combination of rolling and spinning friction.\nI had very little patience at this point so I came up with my own hack approximation of spinning and rolling friction that gives me the result that I want: vibrant motion at high energies but slightly damped so the stone slows down, collapses from spinning, wobbles a bit and then come to rest.\nMy hack was to apply exponential decay (eg. linearVelocity *= factor [0.9990-0.9999] each frame) to linear and angular velocity. The decay factor was linear interpolated between two key speeds such that there was more damping at low speeds and much less at high speeds. There is no physical basis for this, it\u0026rsquo;s just a hack to get the behavior I want.\nWith a bit of tuning, it seems to work reasonably well:\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1361664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361664000,"objectID":"bd20c9254eacb5ae85d9d020a14949f8","permalink":"https://gafferongames.com/post/collision_response_and_coulomb_friction/","publishdate":"2013-02-24T00:00:00Z","relpermalink":"/post/collision_response_and_coulomb_friction/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nSo far in this series, we have mathematically defined the go stone, rendered it, determined how it moves and rotates, and discussed how its shape affects how it responds to collisions.\nNow in this article we reach our first milestone:\nA go stone bouncing and coming to rest on the go board.","tags":["physics","go/baduk/weiqi"],"title":"Collision Response and Coulomb Friction","type":"post"},{"authors":null,"categories":["Virtual Go"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn the previous article we detected collision between the go stone and the go board. Now we\u0026rsquo;re working up to calculating collision response so the stone bounces and wobbles before coming to rest on the board.\nBut in order to reach this goal we first need to lay some groundwork. It turns out that irregularly shaped objects, like go stones, are easier to rotate about some axes than others and this has a large effect on how they react to collisions.\nThis is the reason go stones wobble in such an interesting way when placed on the go board, and why thick go stones wobble differently to thin ones.\nLet\u0026rsquo;s study this effect so we can reproduce it in Virtual Go.\nRotation in 3D Consider the following case in two dimensions:\nIt\u0026rsquo;s easy because there is only one possible axis for rotation around the center of mass: clockwise or counter-clockwise.\nIt follows that we can represent the orientation of an object in 2D around its center of mass with a single theta value, angular velocity with a scalar radians per-second, and a scalar \u0026lsquo;moment of inertia\u0026rsquo; that works just like an angular equivalent of mass: how hard it is to rotate that object.\nBut when we move to three dimensions suddenly rotation can occur about any axis. Orientation becomes a quaternion, angular velocity a vector, and now for irregular shaped objects like go stones, we need a way to indicate that certain axes of rotation are easier to rotate about than others.\nBut how can we represent an angular mass that depends on the shape of the object and the axis of rotation?\nInertia Tensor The solution is to use an inertia tensor.\nAn inertia tensor is a 3x3 matrix with different rules to a normal matrix. It rotates and translates differently, but otherwise behaves like a 3x3 matrix and is used to transform angular velocity to angular momentum, and the inverse of the inertia tensor transforms angular momentum to angular velocity.\nNow this becomes quite interesting because Newton\u0026rsquo;s laws guarantee that in a perfectly elastic collision angular momentum is conserved but angular velocity is not necessarily.\nWhy is this? Because angular velocity now depends on the axis of rotation, so even if the angular momentum has exactly the same magnitude post-collision the angular velocity can be different if the axis of rotation changes and the inertia tensor is non-uniform.\nBecause of this we\u0026rsquo;ll switch to angular momentum as the primary quantity in our physics simulation and we\u0026rsquo;ll derive angular velocity from it. For consistency we\u0026rsquo;ll also switch from linear velocity to linear momentum.\nCalculating The Inertia Tensor Now we need a way to calculate the inertia tensor of our go stone.\nThe general case is quite complicated because inertia tensors are capable of representing shapes that are non-symmetrical about the axis of rotation.\ntodo: yes, need to sort out the latex equations\u0026hellip;\n[latex]I = \\begin{bmatrix} I_{xx} \u0026amp; I_{xy} \u0026amp; I_{xz} \\ I_{yx} \u0026amp; I_{yy} \u0026amp; I_{yz} \\ I_{zx} \u0026amp; I_{zy} \u0026amp; I_{zz} \\end{bmatrix}[/latex]\nFor example, think of an oddly shaped object attached to a drill bit off-center and wobbling about crazily as the drill spins. Fantastic. But the good news is that we get to dodge this bullet because we are always rotating about the center of mass of the go stone, our inertia tensor is much simpler:\n[latex]I = \\begin{bmatrix} I_{x} \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; I_{y} \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; I_{z} \\end{bmatrix}[/latex]\nAll we need to do in our case is to determine the Ix, Iy and Iz values.\nThey represent how difficult it is to rotate the go stone about the x,y and z axes.\nInterestingly, due to symmetry of the go stone, all axes on the xz plane are identical. So really, we only need to calculate Ix and Iy because Iz = Ix.\nNumerical Integration Let\u0026rsquo;s first calculate the inertia tensor via numerical integration.\nTo do this we just need to know is how difficult it is rotate a point about an axis.\nOnce we know this we can approximate the moment of inertia of a go stone by breaking it up into a discrete number of points and summing up the moments of inertia of all these points.\nIt turns out that the difficulty of rotating a point mass about an axis is proportional to the square of the distance of that point from the axis and the mass of the point. [latex]I = mr^2[/latex]. This is quite interesting because it indicates that the distribution of mass has a significant effect on how difficult it is to rotate an object about an axis.\nOne consequence of this is that a hollow pipe is actually more difficult to rotate than a solid pipe of the same mass. Of course, this is not something we deal with in real life often, because a solid pipe of the same material would be much heavier, and therefore harder to rotate due to increased mass, but if you could find a second material of lower density such that the solid pipe was exactly the same mass as the hollow pipe, you would be able to observe this effect. Obscure.\nIn our case we know the go stone is solid not hollow, and we can go one step further and assume that the go stone has completely uniform density throughout. This means if we know the mass of the go stone we can divide it by the volume of the go stone to find its density. Then we can divide space around the go stone into a grid, and using this density we can assign a mass to each point in the grid proportional to the density of the go stone.\nNow integration is just a triple for loop summing up the moments of inertia for points that are inside the go stone. This gives us an approximation of the inertia tensor for the go stone that becomes more accurate the more points we use.\nInterpreting The Inertia Tensor A size 33 japanese go stone has width 22mm and height 9.2mm:\nUsing our point-based approximation to calculate its inertia tensor gives the following result:\n[latex]I = \\begin{bmatrix} 0.177721 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0.304776 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0.177721 \\end{bmatrix}[/latex]\nAs expected, Ix = Iz due to the symmetry of the go stone.\nThe inertia tensor indicates that its much harder to rotate the go stone about the y axis than axes on the xz plane.\nWhy is this?\nYou can see looking top-down at the go stone when rotating about the y axis a ring of mass around the edge of the stone is multiplied by a large r2 and is therefore difficult to rotate.\nContrast this with the rotation about the z axis, which has a much smaller portion of mass far away from the axis:\nAs you can see the distribution of mass around the axis tends to dominate the inertia tensor due to the r2 term. The same mass, twice the distance from the axis, is four times more difficult to rotate!\nClosed Form Solution Exact equations are known for the moments of inertia of many common objects.\nWith a bit of math we can calculate closed form solutions for the moments of inertia of a go stone.\nTo determine the exact equation for Iy we start with the moment of inertia for a solid disc:\n[latex]I = 1/2mr^2[/latex]\nThen we integrate again, effectively summing up the moments of inertia of an infinite number of thin discs making up the top half of the go stone.\nThis leads to the following integral:\n[latex]\\int_0^{h/2} (r^2-(y+r-h/2)^2)^2,dy[/latex]\nWith a little help from Wolfram Alpha we get the following result:\nfloat CalculateIy( const Biconvex \u0026amp; biconvex ) { const float h = height; const float r = biconvex.GetSphereRadius(); const float h2 = h * h; const float h3 = h2 * h; const float h4 = h3 * h; const float h5 = h4 * h; const float r2 = r * r; const float r3 = r2 * r; const float r4 = r3 * r; return pi * p * ( 1/480.0f * h3 * ( 3*h2 - 30*h*r + 80*r2 ) ); } Plugging in the values for a size 33 stone, we get 0.303588 which is close to the approximate solution 0.304776.\nVerifying exact solutions against numeric ones is a fantastic way to check your calculations.\nCan you derive the equation for Ix?\nNEXT ARTICLE: Collision Response and Coulomb Friction\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1361577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361577600,"objectID":"6dd1c446158b8e75fd1c72314f629054","permalink":"https://gafferongames.com/post/rotation_and_inertia_tensors/","publishdate":"2013-02-23T00:00:00Z","relpermalink":"/post/rotation_and_inertia_tensors/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn the previous article we detected collision between the go stone and the go board. Now we\u0026rsquo;re working up to calculating collision response so the stone bounces and wobbles before coming to rest on the board.\nBut in order to reach this goal we first need to lay some groundwork.","tags":["physics","go/baduk/weiqi"],"title":"Rotation \u0026 Inertia Tensors","type":"post"},{"authors":null,"categories":["Virtual Go"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn this series so far we\u0026rsquo;ve defined the shape of a go stone, rendered it using 3D graphics hardware and simulated how it moves in three dimensions.\nOur next goal is for the go stone to bounce and come to rest on the go board.\nUnderstandably, this is quite complicated, so in this article we\u0026rsquo;ll focus on the first step: detecting collisions between a go stone and the go board.\nVoronoi Regions and The Minkowski Difference First, lets assume that the go board is axis aligned and does not move.\nNext, because go stones are small relative to the go board, we can break down collision detection into regions which are treated differently.\nThe common case is with the primary surface, the actual playing surface of the go board, so lets start by looking top-down at the go board and breaking it up into 2D voronoi regions.\nEach voronoi region corresponds to a subspace where all points (x,z) in that region map to the same nearest feature on the go board. This gives us one region that maps points to the top surface of the go board, four regions that map to the sides, and four corner regions.\nIf we were testing an infinitely small point against the go board, this would be enough, but we are colliding a go stone of a certain width and height.\nOne simple way to incorporate the dimensions of the go stone is to offset the regions from the edge of the go board by the the go stone\u0026rsquo;s bounding sphere radius.\nThis creates something like a poor man\u0026rsquo;s version of a minkowski difference:\nWe can now test the center of the go stone against these regions to quickly to categorize the type of potential collision.\nGo Board Collision Cases Although the go board has nine different regions there only three unique types:\nPrimary Edge Corner Primary is the common case.\nIt\u0026rsquo;s also the easiest to handle. The only possible collision is between the stone and the playing surface of the go board.\nSince the go board rests on the floor and cannot move we do not need to worry about collisions with the bottom surface. This means that we can consider the go board to be infinitely thick. This is extremely useful because it removes the possibility of fast moving go stones tunneling vertically through the board.\nNext is the edge case. This is more complicated because there is more than one way to collide in edge regions. Tests must be done between the go stone and the top plane, the side plane, and the side edge.\nThe corner case is more complicated still. Potential collisions include the top plane, the two side planes, the side edges adjacent to the corner, the vertical corner edge, and the corner point.\nGo Stone Collision Cases When a go stone collides with another object there are three collision cases.\nThe first is a collision on the top surface of the biconvex. This corresponds to a collision with a portion of the bottom sphere that generated the go stone.\nNext is the bottom surface of the biconvex. This corresponds to the top sphere.\nFinally, the collision point can be on the circle ring at the intersection of the two sphere surfaces.\nSeparating Axis Test (SAT) We have 3 ways a stone can collide with any convex object, and 9 different regions that must be treated differently when testing vs. the go board. Within each region we have up to 7 different features on the go board that must be tested against 3 different features on the go stone.\nThis is all rather complicated. How can we simplify it?\nThe solution is to use the separating axis test.\nThe basic idea is that if we can find a plane that separates the stone and the board then they must not be colliding. This gives us a robust way of thinking about collision detection and makes testing for collision between objects more general and less prone to combinatorial explosion.\nCalculating The Support In order to use the separating axis test we must first write a function that determines the support of the go stone.\nThe support is the projection of an object on to an axis. This can be difficult to think about in 3D, but for me it makes it easier to think of the axis not as a line, but as the normal of a plane.\nThen what we are really asking is: given this plane normal, what two planes from either side tightly bound the object like book-ends on a shelf?\nTo calculate the support of a biconvex solid we must consider two cases.\nThe first is when the go stone is vertical relative to the axis. Here it is reasonably easy. To calculate the support you simply calculate the intersection of the supports of the spheres used to generate the go stone. This makes a nice sort of intuitive sense seeing as the go stone is itself the intersection of two spheres.\nUnfortunately, this technique breaks down when the stone is horizontal relative to the axis because it fails to exclude the portion of the spheres that don\u0026rsquo;t contribute to the biconvex solid.\nWhat you need to do instead is to calculate the support of the circle edge.\nThe tricky part is detecting when the transition between these two cases occur. Here\u0026rsquo;s a diagram I created a while back when I first tried to work this out. If you look closely you can see the exact point where my head exploded:\nAnd here\u0026rsquo;s a visualization of the end result:\nNow we are ready to continue with the SAT for detecting collisions.\nPrimary Case With this support we can use a one-sided variant of the SAT to detect collision with the primary surface. We\u0026rsquo;re doing one-sided because we\u0026rsquo;re treating the go board as \u0026lsquo;infinitely thick\u0026rsquo; to avoid tunneling in the common case.\nFirst, we take the normal of the primary surface which is (0,1,0) and find the support for the go stone using this normal as the axis: s1 and s2.\nNext, we calculate the projection of the board surface along the normal: t\nThen, if s1 \u0026lt;= t then the go stone is colliding with the go board:\nUnfortunately, we detect the collision after the go stone has already penetrated the go board. There are many solutions for this problem: continuous collision detection, and speculative contacts being interesting avenues I may explore later on.\nBut for now I just do the simplest and most pragmatic thing I can think of.\nI just push the stone out of the board along the axis.\nAfter I push the stone out, I recalculate the nearest point between the stone and board and use this as the contact point.\nEdge and Corner Cases The primary surface case is easy because only one axis needs to be tested, but in corner and edge regions multiple axes must be tested for collision.\nThis is where the SAT really starts to shine. Now instead of combinatorial explosion testing each of the features of the go stone vs. each of the features on the go board, we flatten both the go stone and the go board into support and test for collision one axis at a time.\nThe separating axis test as applied as follows:\nTest all features in the region and determine if there is any separating axis If a separating axis exists then the go stone is not colliding with the board Otherwise the stone must be colliding with the board If the stone is colliding we must now work out what direction to push the stone out. I thought about this for a while and tried to come up with a simple pattern that worked.\nFirst, I tried pushing the stone out along the axis with the greatest amount of penetration, but this breaks down pretty severely in the case where a go stone approaches the go board from the side:\nNext, I thought that perhaps I could use the previous position of the go stone and try to determine the direction that the stone is approaching from. But then I thought about go stones that were rotating rapidly and how this wouldn\u0026rsquo;t always be correct. Then I started thinking about corner and edge cases, and the longer I thought the more this approach seemed too complicated, like I was trying to invent my own half-assed continuous collision detection method that would probably only work half the time and be almost impossible to test.\nIn the end I settled on the simplest solution I could come up with: push the go stone out along the axis with the least amount of penetration.\nThis seems counter-intuitive at first, but it has some nice parallels with other physical laws. Nature is lazy and always takes the shortest path. Nature does the least amount of work.\nWe should probably do the same :)\nNEXT ARTICLE: Rotation and Inertia Tensors\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1361491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361491200,"objectID":"7289df64380fff06fe9138fd195409c9","permalink":"https://gafferongames.com/post/go_stone_vs_go_board/","publishdate":"2013-02-22T00:00:00Z","relpermalink":"/post/go_stone_vs_go_board/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn this series so far we\u0026rsquo;ve defined the shape of a go stone, rendered it using 3D graphics hardware and simulated how it moves in three dimensions.\nOur next goal is for the go stone to bounce and come to rest on the go board.\nUnderstandably, this is quite complicated, so in this article we\u0026rsquo;ll focus on the first step: detecting collisions between a go stone and the go board.","tags":["physics","go/baduk/weiqi"],"title":"Go Stone vs. Go Board","type":"post"},{"authors":null,"categories":["Virtual Go"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn previous articles we mathematically defined the shape of a go stone and tessellated its shape so it can be drawn with 3D graphics hardware.\nNow we want to make the go stone move, obeying Newton\u0026rsquo;s laws of motion so the simulation is physically accurate. The stone should be accelerated by gravity and fall downwards. I also want the stone to rotate so it tumbles as it falls through the air.\nThe Rigid Body Assumption Try biting down on a go stone and you\u0026rsquo;ll agree: go stones are very, very hard.\nGolf balls are pretty hard too, but if you look at a golf ball being hit by a club in super-slow motion, you\u0026rsquo;ll see that it deforms considerably during impact.\nThe same thing happens to all objects in the real world to some degree. Nothing is truly rigid. No real material is so hard that it never deforms.\nBut this is not the real world. This is Virtual Go :) It\u0026rsquo;s a simulation and here we are free to make whatever assumptions we want. And the smartest simplification we can make at this point is to assume that the go stone is perfectly rigid and does not deform under any circumstance.\nThis is known as the rigid body assumption.\nWorking in Three Dimensions Because the go stones are rigid, all we need to represent their current position is the position of the center. As the center moves, so does the rest of the stone.\nWe\u0026rsquo;ll represent this position using a three dimensional vector P.\nLet\u0026rsquo;s define the axes so we know what the x,y,z components of P mean:\nPositive x is to the right Positive y is up Positive z is into the screen This is what is known as a left-handed coordinate system. So called because I can use the fingers on my left hand to point out each positive axis direction without breaking them.\nI\u0026rsquo;ve chosen a left-handed coordinate system purely on personal preference. Also, I\u0026rsquo;m left-handed and I like my fingers :)\nLinear Motion Now we want to make the stone move.\nTo do this we need the concept of velocity. Velocity is also a vector but it\u0026rsquo;s not a point like P. Think of it more like a direction and a length. The direction of the velocity vector is the direction the stone is moving and the length is the speed it\u0026rsquo;s moving in some unit per-second. Here I\u0026rsquo;ll use centimeters per-second because go stones are small.\nFor example, if we the stone to move to the right at a rate of 5 centimeters per-second then the velocity vector is (5,0,0).\nTo make the stone move, all we have to do is add the velocity to the position once per-second:\nWhile this works, it\u0026rsquo;s not particularly exciting. We\u0026rsquo;d like the stone to move much more smoothly. Instead of updating once per-second, let\u0026rsquo;s update 60 times per-second or 60 fps (frames per-second). Rather than taking one big step, we\u0026rsquo;ll take 60 smaller steps per-second, each step being 1/60 of the velocity.\nYou can generalize this to any framerate with the concept of delta time or \u0026ldquo;dt\u0026rdquo;. To calculate delta time invert frames per second: dt = 1/fps and you have the amount of time per-frame in seconds. Next, multiply velocity by delta time and you have the change in position per-frame.\nconst float fps = 60.0f; const float dt = 1 / fps; while ( !quit ) { stone.rigidBody.position += stone.rigidBody.velocity * dt; RenderStone( stone ); UpdateDisplay(); } This is actually a very simple type of numerical integration.\nGravitational Acceleration Next we want to add gravity.\nTo do this we need to change velocity each frame by some amount downwards due to gravity. Change in velocity is known as acceleration. Gravity provides a constant acceleration of 9.8 meters per-second, per-second, or in our case, 98 centimeters per-second, per-second because we\u0026rsquo;re working in centimeters.\nAcceleration due to gravity is also a vector. Since gravity pulls objects down, the acceleration vector is (0,-98,0). Remember, +y axis is up, so -y is down.\nSo how much does gravity accelerate the go stone in 1/60th of a second? Well, 98 * 1/60 = 1.633\u0026hellip; Hey wait. This is exactly what we did with velocity to get position!\nYes it is. It\u0026rsquo;s exactly the same. Acceleration integrates to velocity just like velocity integrates to position. And both are multiplied by dt to find the amount to add per-frame, where dt = 1/fps.\nHere\u0026rsquo;s the code:\nfloat gravity = 9.8f * 10; float fps = 60.0f; float dt = 1 / fps; while ( !quit ) { stone.rigidBody.velocity += vec3f( 0, -gravity, 0 ) * dt; stone.rigidBody.position += stone.rigidBody.velocity * dt; RenderStone( stone ); UpdateDisplay(); } And here\u0026rsquo;s the result:\nAs you can see, now that we\u0026rsquo;ve added acceleration due to gravity the go stone moves in a parabola just like it does in the real world when it\u0026rsquo;s thrown.\nAngular Motion Now let\u0026rsquo;s make the stone rotate!\nFirst we have to define how we represent the orientation of the stone. For this we\u0026rsquo;ll use a quaternion.\nNext we need the angular equivalent of velocity known as\u0026hellip; wait for it\u0026hellip; angular velocity. This too is a vector aka a direction and a length. It\u0026rsquo;s direction is the axis of rotation and the length is the rate of rotation in radians per-second. One full rotation is 2pi radians or 360 degrees so if the length of the angular velocity vector is 2pi the object rotates around the axis once per-second.\nBecause we\u0026rsquo;re using a left handed coordinate system the direction of rotation is clockwise about the positive axis. You can remember this by sticking your thumb of your left hand in the direction of the axis of rotation and curling your fingers. The direction your fingers curl is the direction of rotation. Notice if you do the same thing with your right hand the rotation is the other way.\nHow do we integrate orientation from angular velocity? Orientation is a quaternion and angular velocity is a vector. We can\u0026rsquo;t just add them together.\nThe solution requires a reasonably solid understanding of quaternion math and how it relates to complex numbers. Long story short, we need to convert our angular velocity into a quaternion form and then we can integrate that just like we integrate any other vector. For a full derivation of this result please refer to this excellent article.\nHere is the code I use to convert angular velocity into quaternion form:\ninline quat4f AngularVelocityToSpin( quat4f orientation, vec3f angularVelocity ) { const float x = angularVelocity.x(); const float y = angularVelocity.y(); const float z = angularVelocity.z(); return 0.5f * quat4f( 0, x, y, z ) * orientation; } And once I have this spin quaternion, I can integrate it to find the change in the orientation quaternion just like any other vector.\nconst float fps = 60.0f; const float dt = 1 / fps; while ( !quit ) { quat4f spin = AngularVelocityToSpin( stone.rigidBody.orientation, stone.rigidBody.angularVelocity ); stone.rigidBody.orientation += spin * iteration_dt; stone.rigidBody.orientation = normalize( stone.rigidBody.orientation ); RenderStone( stone ); UpdateDisplay(); } The only difference is that after integration I renormalize the quaternion to ensure it doesn\u0026rsquo;t drift from unit length, otherwise it stops representing a rotation.\nYep. That go stone is definitely rotating.\nWhy Quaternions? Graphics cards typically represent rotations with matrices, so why are we using quaternions when calculating physics instead of 4x4 matrices? Aren\u0026rsquo;t we bucking the trend a bit here?\nNot really. There are many good reasons to work with quaternions:\nIt\u0026rsquo;s easier to integrate angular velocity using a quaternion than a 3x3 matrix\nNormalizing a quaternion is faster than orthonormalizing a 3x3 matrix\nIt\u0026rsquo;s really easy to interpolate between two quaternions\nWe\u0026rsquo;ll still use matrices but as a secondary quantity. This means that each frame after we integrate we convert the quaternion into a 3x3 rotation matrix and combine it with the position into a 4x4 rigid body matrix and its inverse like this:\nmat4f RigidBodyMatrix( vec3f position, quat4f rotation ) { mat4f matrix; rotation.toMatrix( matrix ); matrix.value.w = simd4f_create( position.x(), position.y(), position.z(), 1 ); return matrix; } mat4f RigidBodyInverse( const mat4f \u0026amp; matrix ) { mat4f inverse = matrix; vec4f translation = matrix.value.w; inverse.value.w = simd4f_create(0,0,0,1); simd4x4f_transpose_inplace( \u0026amp;inverse.value ); vec4f x = matrix.value.x; vec4f y = matrix.value.y; vec4f z = matrix.value.z; inverse.value.w = simd4f_create( -dot( x, translation ), -dot( y, translation ), -dot( z, translation ), 1.0f ); return inverse; } Now whenever we transform vectors want to go in/out of stone body space we\u0026rsquo;ll use this matrix and its inverse. It\u0026rsquo;s the best of both worlds.\nBringing It All Together The best thing about rigid body motion is that you can calculate linear and angular motion separately and combine them together and it just works.\nHere\u0026rsquo;s the final code with linear and angular motion combined:\nconst float gravity = 9.8f * 10; const float fps = 60.0f; const float dt = 1 / fps; while ( !quit ) { stone.rigidBody.velocity += vec3f( 0, -gravity, 0 ) * dt; stone.rigidBody.position += stone.rigidBody.velocity * dt; quat4f spin = AngularVelocityToSpin( stone.rigidBody.orientation, stone.rigidBody.angularVelocity ); stone.rigidBody.orientation += spin * dt; stone.rigidBody.orientation = normalize( stone.rigidBody.orientation ); RenderStone( stone ); UpdateDisplay(); } And here is the end result:\nI think this is fairly convincing. The go stone is moving quite realistically!\nNEXT ARTICLE: Go Stone vs. Go Board\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1361404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361404800,"objectID":"5174e23874b9b4dff7c112e0386fbc3c","permalink":"https://gafferongames.com/post/how_the_go_stone_moves/","publishdate":"2013-02-21T00:00:00Z","relpermalink":"/post/how_the_go_stone_moves/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn previous articles we mathematically defined the shape of a go stone and tessellated its shape so it can be drawn with 3D graphics hardware.\nNow we want to make the go stone move, obeying Newton\u0026rsquo;s laws of motion so the simulation is physically accurate. The stone should be accelerated by gravity and fall downwards.","tags":["physics","go/baduk/weiqi"],"title":"How The Go Stone Moves","type":"post"},{"authors":null,"categories":["Virtual Go"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn this article we want to draw the go stone using OpenGL.\nUnfortunately we can\u0026rsquo;t just tell the graphics card, \u0026ldquo;Hey! Please draw the intersection of two spheres with radius r and d apart with a bevel torus r1 and r2!\u0026rdquo;, because modern 3D graphics cards work by drawing triangles. We have to take our mathematical definition of the go stone and turn it into a set of triangles that the graphics card can render.\nThis is called tessellation and there are several different ways to do it.\nLongitude And Lattitude The first way that I tried was to consider sphere rendering like a globe with longitude/latitude. I started with a ring around the \u0026rsquo;equator\u0026rsquo; of the go stone, stepping these rings up to the top of the sphere like the north pole on a globe.\nUnfortunately, just like longitude/latitude on a globe, tessellating this way leads to very distorted mapping around the pole and a lot of wasted triangles:\nTriangle Subdivision The next method is triangle subdivision. You start with an approximate shape then subdivide each triangle into four smaller triangles recursively like this:\nSince the go stone only needs the top 1/3 or 1/4 of a sphere, I didn\u0026rsquo;t want to subdivide a whole sphere only to throw most of it away. So I designed my own subdivision algorithm to generate only the top section of a sphere.\nAfter some trial and error I found that a pentagon plus a center vertex at the pole of the sphere was a good initial generator that minimized the distortion that occurs during subdivision. The only tricky part is that when subdividing you need to keep track of whether the edge is a sphere edge or a circle edge, as the subdivided vertex must be projected differently.\nWith this technique I was able to generate a much more efficient tessellation:\nTessellating The Bevel Now we need to tesselate the bevel. To do this I take the vertices which form the circle edge at the bottom of the top sphere surface and calculate the angle of each vertex about the y axis. I then use these angles to sweep around the torus ensuring that the torus vertices weld perfectly with the top and bottom sphere sections.\nVertex Welding Due to how recursive subdivision works a lot of duplicate vertices are generated.\nI\u0026rsquo;d rather not have the graphics card waste time transforming the same vertex over and over, so as I add vertices to the mesh I hash vertex positions into a 3D grid (~1mm cells) and reuse an existing vertex if the position and normals match within some small epsilon value.\nWith vertex welding the reduction in vertices is dramatic: 53000 to just 6500.\nFor more information on vertex welding please refer to the discussion in Real-Time Collision Detection by Christer Ericson.\nNEXT ARTICLE: How The Go Stone Moves\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1361318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361318400,"objectID":"635a55f9ba58d7f952715a537b4a79d4","permalink":"https://gafferongames.com/post/tessellating_the_go_stone/","publishdate":"2013-02-20T00:00:00Z","relpermalink":"/post/tessellating_the_go_stone/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIn this article we want to draw the go stone using OpenGL.\nUnfortunately we can\u0026rsquo;t just tell the graphics card, \u0026ldquo;Hey! Please draw the intersection of two spheres with radius r and d apart with a bevel torus r1 and r2!\u0026rdquo;, because modern 3D graphics cards work by drawing triangles.","tags":["physics","go/baduk/weiqi"],"title":"Tessellating The Go Stone","type":"post"},{"authors":null,"categories":["Virtual Go"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIf you play Go, you know that a biconvex go stone has an interesting wobble when it\u0026rsquo;s placed on the board. This wobble is a direct consequence of its unique shape.\nI\u0026rsquo;d like to reproduce this wobble in Virtual Go, so let\u0026rsquo;s to spend some time studying go stone\u0026rsquo;s shape, so we can capture this wobble and simulate it on a computer :)\nSlate And Shell In Japan, Go stones are traditionally made out of slate and clam shell.\nClam shell stones come in several grades of quality. The highest being yuki or \u0026ldquo;snow\u0026rdquo; grade with fine, regularly spaced lines.\nGo stones also come in different sizes. In general, the thicker the stone, the more expensive it is, as only a small portion of the clam shell is suitable for making them.\nAt first glance the go stone looks like an ellipse, but side-on you can see this is not the case. This shape is called a biconvex solid. I find this shape interesting because it is the intersection of two spheres.\nWe can study this shape by looking at the intersection of two circles:\nI quickly noticed that by varying the radius of the circles and the distance between their centers, I could generate go stones of different sizes.\nBut when creating a go stone I don\u0026rsquo;t really want it to be parameterized this way.\nInstead I\u0026rsquo;d like to say, \u0026ldquo;Hey, I would like a stone of this width and height\u0026rdquo; and have a function that calculates the radius of the circles and how far apart they should be to generate that stone.\nTo write this function we first need to do some math:\nFirst notice that the point Q lies on the generating circle, so the line CQ has length r:\ntodo: obviously I have a bunch of work to get the latex equations ported across to Hugo. I\u0026rsquo;m researching different options\u0026hellip;\n[latex size=\u0026ldquo;2\u0026rdquo;]d + h/2 = r[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]d = r - h/2[/latex]\nThe point P is also on the generating circle so the green line CP has length r as well. Using Pythagoras theorem and substituting for d:\n[latex size=\u0026ldquo;2\u0026rdquo;]r^2 = d^2 + (w/2)^2[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]r^2 = ( r - h/2 )^2 + (w/2)^2[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]r^2 = ( h^2/4 - hr + r^2 ) + w^2/4[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]r^2 = h^2/4 - hr + r^2 + w^2/4[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]0 = h^2/4 - hr + 0 + w^2/4[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]hr = h^2/4 + w^2/4[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]r = ( h^2 + w^2 ) / 4h[/latex]\nWhich gives us everything we need to write the function:\nvoid calculateBiconvex( float w, float h, float \u0026amp; r, float \u0026amp; d ) { r = ( w*w + h*h ) / ( 4*h ); d = r - h/2; } Now we can mathematically define a go stone parameterized by its width and height. There is just one problem: the edge is very sharp!\nTo make our stone aesthetically pleasing, lets round the edge with a bevel. Otherwise, you might cut yourself virtually when you play with it:\nLet\u0026rsquo;s parameterize the bevel by its height b:\nIn three dimensions the bevel is actually a torus (donut) around the edge of the go stone. We need to calculate the major and minor radii r1 and r2 of the torus as a function of b and the dimensions of the go stone:\nThe key to solving this is to realize that if the go stone and the bevel are to match perfectly then the tangent of the two circles must be equal at the point P.\nUpdate: A few years later and it occurs to me that it would be even more beautiful if the second derivative matched at this intersection as well. Is this possible in general, or must the generating spheres become to ellipses in order to make this happen? I suspect this is the case. Mathematicians who play Go, let me know your thoughts.\nIf the tangent is equal then the normal must be equal as well. This means that the center of the bevel circle lies at the intersection of the line CP and the x axis.\nWe already know C so if we can find the point P then we can find this intersection point. Once we know the intersection point we can find r1 and r2.\nSince P is at the start of the bevel:\n[latex size=\u0026ldquo;2\u0026rdquo;]P_y = b/2[/latex]\nBecause P lies on the biconvex circle with center C and radius r we can use the equation of the circle to find x as a function of y:\n[latex size=\u0026ldquo;2\u0026rdquo;]x^2 + y^2 = r^2[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]x = \\sqrt{ r^2 - y^2 }[/latex]\nWe need y relative to the circle center C, not in go stone coordinates, so we add d and substitute y\u0026rsquo; for y:\n[latex size=\u0026ldquo;2\u0026rdquo;]y\u0026rsquo; = b/2 + d[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]P_x = \\sqrt{ r^2 - ( b/2 + d )^2 }[/latex]\nWe can now find r1 by similar triangles:\n[latex size=\u0026ldquo;2\u0026rdquo;]r_1/P_x = d / ( d + b/2 )[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]r_1 = P_x d / ( d + b/2 )[/latex]\nand q by Pythagoras theorem:\n[latex size=\u0026ldquo;2\u0026rdquo;]q^2 = d^2 + r_1^2[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]q = \\sqrt{ d^2 + r_1^2 }[/latex]\nBecause line CP has length r and substituting for q:\n[latex size=\u0026ldquo;2\u0026rdquo;]q + r_2 = r[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]r_2 = r - q[/latex] [latex size=\u0026ldquo;2\u0026rdquo;]r_2 = r - \\sqrt{ d^2 + r_1^2 }[/latex]\nNow we have everything we need to write the function:\nvoid calculate_bevel( float r, float d, float b, float \u0026amp; r1, float \u0026amp; r2 ) { const float y = b/2 + d; const float px = sqrt( r*r - y*y ); r1 = px * d / ( d + b/2 ); r2 = r - sqrt( d*d + r1*r1 ); } Now we can calculate the bevel torus to round off any go stone we create as the intersection of two spheres.\nNEXT ARTICLE: Tessellating The Go Stone\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1361232000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361232000,"objectID":"73bf408a142992233babd7a614724739","permalink":"https://gafferongames.com/post/shape_of_the_go_stone/","publishdate":"2013-02-19T00:00:00Z","relpermalink":"/post/shape_of_the_go_stone/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nIf you play Go, you know that a biconvex go stone has an interesting wobble when it\u0026rsquo;s placed on the board. This wobble is a direct consequence of its unique shape.\nI\u0026rsquo;d like to reproduce this wobble in Virtual Go, so let\u0026rsquo;s to spend some time studying go stone\u0026rsquo;s shape, so we can capture this wobble and simulate it on a computer :)","tags":["physics","go/baduk/weiqi"],"title":"Shape of The Go Stone","type":"post"},{"authors":null,"categories":["Virtual Go"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nI\u0026rsquo;m a professional game programmer with 15 years experience in the game industry. Over the years I\u0026rsquo;ve worked for Irrational Games, Team Bondi, Pandemic Studios, Sony Santa Monica and most recently Respawn Entertainment. During my career I\u0026rsquo;m extremely proud to have worked on such games as \u0026lsquo;Freedom Force\u0026rsquo;, \u0026lsquo;L.A. Noire\u0026rsquo;, \u0026lsquo;Journey\u0026rsquo;, \u0026lsquo;God of War: Ascension\u0026rsquo; and \u0026lsquo;Titanfall\u0026rsquo;.\nIn my spare time I\u0026rsquo;m also an avid player of the board game Go.\nA personal project I\u0026rsquo;ve always dreamed of combines the things I love: the game of Go, graphics programming, physics simulation and network programming.\nThe end result I hope to achieve is a beautiful real-time computer rendering of a go board and stones with photorealistic visuals and the laws of physics defining all interactions between the go stones and the board. To Go players reading this, yes, I do aim to reproduce that unique \u0026lsquo;wobble\u0026rsquo; and feel you are familiar with when placing a stone on the board.\nDuring the course of this article series I\u0026rsquo;m going to build this project entirely from scratch and include you in on all the details of building it as a tutorial. I believe in sharing knowledge and my hope is you can follow this project and understand the passion I bring to it and perhaps learn a few things along the way.\nIf you already play Go and want to get right in to the details of building the simulation, I would recommend skipping ahead to the next article in the series: Shape Of The Go Stone.\nOtherwise, if you would like a quick introduction to Go, please read on!\nThe Game of Go Go is a board game that originated in ancient China.\nToday it is played worldwide but has a particularly strong following in China, Japan and Korea. It is not particularly well known in the West, although it has featured in popular culture in the movie \u0026ldquo;A Beautiful Mind\u0026rdquo;.\nGo is played on a grid with black and white stones. It is played by two people, each taking turns to place a stone of their color at one of the intersection points on the grid. Once placed on the board, stones do not move.\nEach stone on the board has a number of liberties equal to the number of lines radiating out from it on the grid. A stone in the middle of the board has four liberties, a stone on the side has three, a stone in the corner has just two.\nIf the opponent is able to surround all the liberties with stones of the opposite color, the stone is removed from the board.\nWhen stones of the same color are placed horizontally or vertically next to each other they become logically connected and form a \u0026ldquo;group\u0026rdquo; with its own set of liberties. For example, a group of two stones in the center has 6 liberties, while the same group on the side has only 4.\nA group may be captured if all of its liberties are blocked with stones of the opposite color. When a group is captured it is removed from the board as a unit.\nOf course it is not so easy to surround your opponents stones because they get to place stones too :)\nFor example, a single black stone in the center with just one liberty remaining is in a situation known as \u0026ldquo;Atari\u0026rdquo;, but black can escape by extending to form a group of two stones. Now the black group has three liberties and can extend to create more liberties faster than they can be taken away.\nIt follows that it\u0026rsquo;s not really possible to capture all of your opponents stones or for them to capture all of yours. Instead, you must coexist on the board with stones of the other color and find a way to surround more points of territory than your opponent.\nIt sounds simple but as you play Go you\u0026rsquo;ll notice beautiful complexity emerging like a fractal: life and death - stones living even though surrounded, liberty races, seki or \u0026ldquo;dual life\u0026rdquo;, the ladder, ko, the snapback, playing under the stones, the monkey jump, the bamboo joint, the tiger mouth.\nSo many beautiful properties from such simple rules. Truly an amazing game!\nPlease visit The Interactive Way To Go if you would like to learn more.\nNEXT ARTICLE: Shape of The Go Stone\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1361145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361145600,"objectID":"a18df56cc21cb903a1d0b0b8227a8d91","permalink":"https://gafferongames.com/post/introduction_to_virtual_go/","publishdate":"2013-02-18T00:00:00Z","relpermalink":"/post/introduction_to_virtual_go/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler. Welcome to Virtual Go, my project to create a physically accurate computer simulation of a Go board and stones.\nI\u0026rsquo;m a professional game programmer with 15 years experience in the game industry. Over the years I\u0026rsquo;ve worked for Irrational Games, Team Bondi, Pandemic Studios, Sony Santa Monica and most recently Respawn Entertainment. During my career I\u0026rsquo;m extremely proud to have worked on such games as \u0026lsquo;Freedom Force\u0026rsquo;, \u0026lsquo;L.","tags":["physics","go/baduk/weiqi"],"title":"Introduction to Virtual Go","type":"post"},{"authors":null,"categories":["Game Networking"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nLately I\u0026rsquo;ve been doing some research into networking game physics simulations via deterministic lockstep methods.\nThe basic idea is that instead of synchronizing the state of physics objects directly by sending the positions, orientations, velocities etc. over the network, one could synchronize the simulation implicitly by sending just the player inputs.\nThis is a very attractive synchronization strategy because the amount of network traffic depends on the size of the player inputs instead of the amount of physics state in the world. In fact, this strategy has been used for many years in RTS games for precisely this reason; with thousands and thousands of units on the map, they simply have too much state to send over the network.\nPerhaps you have a complex physics simulation with lots of rigid body state, or a cloth or soft body simulation which needs to stay perfectly in sync across two machines because it is gameplay affecting, but you cannot afford to send all the state. It is clear that the only possible solution in this situation is to attempt a deterministic networking strategy.\nBut we run into a problem. Physics simulations use floating point calculations, and for one reason or another it is considered very difficult to get exactly the same result from floating point calculations on two different machines. People even report different results on the same machine from run to run, and between debug and release builds. Other folks say that AMDs give different results to Intel machines, and that SSE results are different from x87. What exactly is going on? Are floating point calculations deterministic or not?\nUnfortunately, the answer is not a simple \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no\u0026rdquo; but \u0026ldquo;yes, if\u0026hellip;\u0026rdquo;\nHere is what I have discovered so far:\nIf your physics simulation is itself deterministic, with a bit of work you should be able to get it to play back a replay of recorded inputs on the same machine and get the exact same result.\nIt is possible to get deterministic results for floating calculations across multiple computers provided you use an executable built with the same compiler, run on machines with the same architecture, and perform some platform-specific tricks.\nIt is incredibly naive to write arbitrary floating point code in C or C++ and expect it to give exactly the same result across different compilers or architectures, or even the same results across debug and release builds.\nHowever with a good deal of work you may be able to coax exactly the same floating point results out of different compilers or different machine architectures by using your compilers \u0026ldquo;strict\u0026rdquo; IEEE 754 compliant mode and restricting the set of floating point operations you use. This typically results in significantly lower floating point performance.\nIf you would like to debate these points or add your own nuance, please contact me! I consider this question by no means settled and am very interested in other peoples experiences with deterministic floating point simulations and exactly reproducible floating point calculations. Please contact me especially if you have managed to get binary exact results across different architectures and compilers in real world situations.\nHere are the resources I have discovered in my search so far\u0026hellip;\nThe technology we license to various customers is based on determinism of floating point (in 64-bit mode, even) and has worked that way since the year 2000. As long as you stick to a single compiler, and a single CPU instruction set, it is possible to make floating point fully deterministic. The specifics vary by platform (i e, different between x86, x64 and PPC).\nYou have to make sure that the internal precision is set to 64 bits (not 80, because only Intel implements that), and that the rounding mode is consistent. Furthermore, you have to check this after calls to external DLLs, because many DLLs (Direct3D, printer drivers, sound libraries, etc) will change the precision or rounding mode without setting it back.\nThe ISA is IEEE compliant. If your x87 implementation isn\u0026rsquo;t IEEE, it\u0026rsquo;s not x87.\nAlso, you can\u0026rsquo;t use SSE or SSE2 for floating point, because it\u0026rsquo;s too under-specified to be deterministic.\nJon Watte, GameDev.net forums http://www.gamedev.net/community/forums/topic.asp?topic_id=499435\nI work at Gas Powered Games and i can tell you first hand that floating point math is deterministic. You just need the same instruction set and compiler and of course the user's processor adheres to the IEEE754 standard, which includes all of our PC and 360 customers. The engine that runs DemiGod, Supreme Commander 1 and 2 rely upon the IEEE754 standard. Not to mention probably all other RTS peer to peer games in the market. As soon as you have a peer to peer network game where each client broadcasts what command they are doing on what 'tick' number and rely on the client computer to figure out the simulation/physical details your going to rely on the determinism of the floating point processor. At app startup time we call:\n_controlfp(_PC_24, _MCW_PC) _controlfp(_RC_NEAR, _MCW_RC) Also, every tick we assert that these fpu settings are still set:\ngpAssert( (_controlfp(0, 0) \u0026amp;amp; _MCW_PC) == _PC_24 ); gpAssert( (_controlfp(0, 0) \u0026amp;amp; _MCW_RC) == _RC_NEAR ); There are some MS API functions that can change the fpu model on you so you need to manually enforce the fpu mode after those calls to ensure the fpu stays the same across machines. The assert is there to catch if anyone has buggered the fpu mode.\nFYI We have the compiler floating point model set to Fast /fp:fast ( but its not a requirement )\nWe have never had a problem with the IEEE standard across any PC cpu AMD and Intel with this approach. None of our SupCom or Demigod customers have had problems with their machines either, and we are talking over 1 million customers here (supcom1 + expansion pack). We would have heard if there was a problem with the fpu not having the same results as replays or multiplayer mode wouldn\u0026rsquo;t work at all.\nWe did however have problems when using some physics APIs because their code did not have determinism or reproducibility in mind. For example some physics APIS have solvers that take X number of iterations when solving where X can be lower with faster CPUs.\nElijah, Gas Powered Games\nhttp://www.box2d.org/forum/viewtopic.php?f=3\u0026amp;amp;t=1800\nIf you store replays as controller inputs, they cannot be played back on machines with different CPU architectures, compilers, or optimization settings. In MotoGP, this meant we could not share saved replays between Xbox and PC. It also meant that if we saved a replay from a debug build of the game, it would not work in release builds, or vice versa. This is not always a problem (we never shipped debug builds, after all), but if we ever released a patch, we had to build it using the exact same compiler as the original game. If the compiler had been updated since our original release, and we built a patch using the newer compiler, this could change things around enough that replays saved by the original game would no longer play back correctly. This is madness! Why don\u0026rsquo;t we make all hardware work the same? Well, we could, if we didn\u0026rsquo;t care about performance. We could say \u0026ldquo;hey Mr. Hardware Guy, forget about your crazy fused multiply-add instructions and just give us a basic IEEE implementation\u0026rdquo;, and \u0026ldquo;hey Compiler Dude, please don\u0026rsquo;t bother trying to optimize our code\u0026rdquo;. That way our programs would run consistently slowly everywhere :-)\nShawn Hargreaves, MSDN Blog http://blogs.msdn.com/shawnhar/archive/2009/03/25/is-floating-point-math-deterministic.aspx\n\"Battlezone 2 used a lockstep networking model requiring absolutely identical results on every client, down to the least-significant bit of the mantissa, or the simulations would start to diverge. While this was difficult to achieve, it meant we only needed to send user input across the network; all other game state could be computed locally. During development, we discovered that AMD and Intel processors produced slightly different results for trancendental functions (sin, cos, tan, and their inverses), so we had to wrap them in non-optimized function calls to force the compiler to leave them at single-precision. That was enough to make AMD and Intel processors consistent, but it was definitely a learning experience. Ken Miller, Pandemic Studios\nhttp://www.box2d.org/forum/viewtopic.php?f=4\u0026amp;amp;t=175\n... In FSW1 when desync is detected in player would be instantly killed by \"magic sniper\". :) All that stuff was fixed in FSW2. We just ran precise FP and used Havok FPU libs instead SIMD on PC. Also integer modulo is problem too because C++ standard says it's \"implementation defined\" (in case when multiple compilers/platforms are used). In general I liked tools for lockstep we developed, finding desyncs in code on FSW2 was trivial. Branimir Karadžić, Pandemic Studios http://www.google.com/buzz/100111796601236342885/8hDZ655S6x3/Floating-Point-Determinism-Gaffer-on-Games\nI know three main sources of floating point inconsistency pain: Algebraic compiler optimizations “Complex” instructions like multiply-accumulate or sine x86-specific pain not available on any other platform; not that ~100% of non-embedded devices is a small market share for a pain.\nThe good news is that most pain comes from item 3 which can be more or less solved automatically. For the purpose of decision making (”should we invest energy into FP consistency or is it futile?”), I’d say that it’s not futile and if you can cite actual benefits you’d get from consistency, then it’s worth the (continuous) effort.\nSummary: use SSE2 or SSE, and if you can’t, configure the FP CSR to use 64b intermediates and avoid 32b floats. Even the latter solution works passably in practice, as long as everybody is aware of it.\nYossi Kreinin, Personal Blog\nhttp://www.yosefk.com/blog/consistency-how-to-defeat-the-purpose-of-ieee-floating-point.html\nThe short answer is that FP calculations are entirely deterministic, as per the IEEE Floating Point Standard, but that doesn't mean they're entirely reproducible across machines, compilers, OS's, etc. The long answer to these questions and more can be found in what is probably the best reference on floating point, David Goldberg\u0026rsquo;s What Every Computer Scientist Should Know About Floating Point Arithmetic. Skip to the section on the IEEE standard for the key details.\nFinally, if you are doing the same sequence of floating point calculations on the same initial inputs, then things should be replayable exactly just fine. The exact sequence can change depending on your compiler/os/standard library, so you might get some small errors this way.\nWhere you usually run into problems in floating point is if you have a numerically unstable method and you start with FP inputs that are approximately the same but not quite. If your method\u0026rsquo;s stable, you should be able to guarantee reproducibility within some tolerance. If you want more detail than this, then take a look at Goldberg\u0026rsquo;s FP article linked above or pick up an intro text on numerical analysis.\nTodd Gamblin, Stack Overflow http://stackoverflow.com/questions/968435/what-could-cause-a-deterministic-process-to-generate-floating-point-errors\nThe C++ standard does not specify a binary representation for the floating-point types float, double and long double. Although not required by the standard, the implementation of floating point arithmetic used by most C++ compilers conforms to a standard, IEEE 754-1985, at least for types float and double. This is directly related to the fact that the floating point units of modern CPUs also support this standard. The IEEE 754 standard specifies the binary format for floating point numbers, as well as the semantics for floating point operations. Nevertheless, the degree to which the various compilers implement all the features of IEEE 754 varies. This creates various pitfalls for anyone writing portable floating-point code in C++. Günter Obiltschnig, Cross-Platform Issues with Floating-Point arithmetics in C++ http://www.appinf.com/download/FPIssues.pdf\nFloating-point computations are strongly dependent on the FPU hardware implementation, the compiler and its optimizations, and the system mathematical library (libm). Experiments are usually reproducible only on the same machine with the same system library and the same compiler using the same options. STREFLOP Library http://nicolas.brodu.numerimoire.net/en/programmation/streflop/index.html\nFloating Point (FP) Programming Objectives: • Accuracy - Produce results that are “close” to the correct value\n• Reproducibility - Produce consistent results from one run to the next. From one set of build options to another. From one compiler to another. From one platform to another.\n• Performance – Produce the most efficient code possible.\nThese options usually conflict! Judicious use of compiler options lets you control the tradeoffs.\nIntel C++ Compiler: Floating Point Consistency http://www.nccs.nasa.gov/images/FloatingPoint%5Fconsistency.pdf.\nIf strict reproducibility and consistency are important do not change the floating point environment without also using either fp-model strict (Linux or Mac OS*) or /fp:strict (Windows*) option or pragma fenv_access. Intel C++ Compiler Manual http://cache-www.intel.com/cd/00/00/34/76/347605_347605.pdf\nUnder the fp:strict mode, the compiler never performs any optimizations that perturb the accuracy of floating-point computations. The compiler will always round correctly at assignments, typecasts and function calls, and intermediate rounding will be consistently performed at the same precision as the FPU registers. Floating-point exception semantics and FPU environment sensitivity are enabled by default. Certain optimizations, such as contractions, are disabled because the compiler cannot guarantee correctness in every case. Microsoft Visual C++ Floating-Point Optimization http://msdn.microsoft.com/en-us/library/aa289157(VS.71).aspx#floapoint_topic4\nPlease note that the results of floating point calculations will likely not be exactly the same between PowerPC and Intel, because the PowerPC scalar and vector FPU cores are designed around a fused multiply add operation. The Intel chips have separate multiplier and adder, meaning that those operations must be done separately. This means that for some steps in a calculation, the Intel CPU may incur an extra rounding step, which may introduce 1/2 ulp errors at the multiplication stage in the calculation. Apple Developer Support http://developer.apple.com/hardwaredrivers/ve/sse.html\nFor all of the instructions that are IEEE operations (*,+,-,/,sqrt, compares, regardless of whether they are SSE or x87), they will produce the same results across platforms with the same control settings (same precision control and rounding modes, flush to zero, etc.) and inputs. This is true for both 32-bit and 64-bit processors... On the x87 side, the transcendental instructions like, fsin, fcos, etc. could produce slightly different answers across implementations. They are specified with a relative error that is guaranteed, but not bit-for-bit accuracy. Intel Software Network Support http://software.intel.com/en-us/forums/showthread.php?t=48339\nI'm concerned about the possible differences between hardware implementations of IEEE-754. I already know about the problem of programming languages introducing subtle differences between what is written in the source code and what is actually executed at the assembly level. [Mon08] Now, I'm interested in differences between, say, Intel/SSE and PowerPC at the level of individual instructions. D. Monniaux on IEEE 754 mailing list http://grouper.ieee.org/groups/754/email/msg03864.html\nOne must ... avoid the non-754 instructions that are becoming more prevalent for inverse and inverse sqrt that don't round correctly or even consistently from one implementation to another, as well as the x87 transcendental operations which are necessarily implemented differently by AMD and Intel. David Hough on 754 IEEE mailing list http://grouper.ieee.org/groups/754/email/msg03867.html\nYes, getting reproducible results IS possible. But you CAN'T do it without defining a programming methodology intended to deliver that property. And that has FAR more drastic consequences than any of its proponents admit - in particular, it is effectively incompatible with most forms of parallelism. Nick Maclaren on 754 IEEE mailing list http://grouper.ieee.org/groups/754/email/msg03872.html\nIf we are talking practicabilities, then things are very different, and expecting repeatable results in real programs is crying for the moon. But we have been there before, and let's not go there again. Nick Maclaren on 754 IEEE mailing list http://grouper.ieee.org/groups/754/email/msg03862.html\nThe IEEE 754-1985 allowed many variations in implementations (such as the encoding of some values and the detection of certain exceptions). IEEE 754-2008 has tightened up many of these, but a few variations still remain (especially for binary formats). The reproducibility clause recommends that language standards should provide a means to write reproducible programs (i.e., programs that will produce the same result in all implementations of a language), and describes what needs to be done to achieve reproducible results. Wikipedia Page on IEEE 754-2008 standard http://en.wikipedia.org/wiki/IEEE_754-2008#Reproducibility\nIf one wants semantics almost exactly faithful to strict IEEE-754 single or double precision computations in round-to-nearest mode, including with respect to overflow and underflow conditions, one can use, at the same time, limitation of precision and options and programming style that force operands to be systematically written to memory between floating-point operations. This incurs some performance loss; furthermore, there will still be slight discrepancy due to double rounding on underflow. A simpler solution for current personal computers is simply to force the compiler to use the SSE unit for computations on IEEE-754 types; however, most embedded systems using IA32 microprocessors or microcontrollers do not use processors equipped with this unit.\nDavid Monniaux, The pitfalls of verifying floating-point computations http://hal.archives-ouvertes.fr/docs/00/28/14/29/PDF/floating-point-article.pdf\n6. REPRODUCIBILITY Even under the 1985 version of IEEE-754, if two implementations of the standard executed an operation on the same data, under the same rounding mode and default exception handling, the result of the operation would be identical. The new standard tries to go further to describe when a program will produce identical floating point results on different implementations. The operations described in the standard are all reproducible operations.\nThe recommended operations, such as library functions or reduction operators are not reproducible, because they are not required in all implementations. Likewise dependence on the underflow and inexact flags is not reproducible because two different methods of treating underflow are allowed to preserve conformance between IEEE-754(1985) and IEEE-754(2008). The rounding modes are reproducible attributes. Optional attributes are not reproducible.\nThe use of value-changing optimizations is to be avoided for reproducibility. This includes use of the associative and disributative laws, and automatic generation of fused multiply-add operations when the programmer did not explicitly use that operator.\nPeter Markstein, The New IEEE Standard for Floating Point Arithmetic http://drops.dagstuhl.de/opus/volltexte/2008/1448/pdf/08021.MarksteinPeter.ExtAbstract.1448.pdf\nUnfortunately, the IEEE standard does not guarantee that the same program will deliver identical results on all conforming systems. Most programs will actually produce different results on different systems for a variety of reasons. For one, most programs involve the conversion of numbers between decimal and binary formats, and the IEEE standard does not completely specify the accuracy with which such conversions must be performed. For another, many programs use elementary functions supplied by a system library, and the standard doesn't specify these functions at all. Of course, most programmers know that these features lie beyond the scope of the IEEE standard. Many programmers may not realize that even a program that uses only the numeric formats and operations prescribed by the IEEE standard can compute different results on different systems. In fact, the authors of the standard intended to allow different implementations to obtain different results. Their intent is evident in the definition of the term destination in the IEEE 754 standard: \u0026ldquo;A destination may be either explicitly designated by the user or implicitly supplied by the system (for example, intermediate results in subexpressions or arguments for procedures). Some languages place the results of intermediate calculations in destinations beyond the user\u0026rsquo;s control. Nonetheless, this standard defines the result of an operation in terms of that destination\u0026rsquo;s format and the operands\u0026rsquo; values.\u0026rdquo; (IEEE 754-1985, p. 7) In other words, the IEEE standard requires that each result be rounded correctly to the precision of the destination into which it will be placed, but the standard does not require that the precision of that destination be determined by a user\u0026rsquo;s program. Thus, different systems may deliver their results to destinations with different precisions, causing the same program to produce different results (sometimes dramatically so), even though those systems all conform to the standard.\nDifferences Among IEEE 754 Implementations http://docs.sun.com/source/806-3568/ncg_goldberg.html#3098\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1266969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1266969600,"objectID":"31bf7aded505115541a7063e0eabae8e","permalink":"https://gafferongames.com/post/floating_point_determinism/","publishdate":"2010-02-24T00:00:00Z","relpermalink":"/post/floating_point_determinism/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nLately I\u0026rsquo;ve been doing some research into networking game physics simulations via deterministic lockstep methods.\nThe basic idea is that instead of synchronizing the state of physics objects directly by sending the positions, orientations, velocities etc. over the network, one could synchronize the simulation implicitly by sending just the player inputs.\nThis is a very attractive synchronization strategy because the amount of network traffic depends on the size of the player inputs instead of the amount of physics state in the world.","tags":["networking"],"title":"Floating Point Determinism","type":"post"},{"authors":null,"categories":["Game Networking"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nHave you ever wondered how multiplayer games work?\nFrom the outside it seems magical: two or more players sharing a consistent experience across the network like they actually exist together in the same virtual world.\nBut as programmers we know the truth of what is actually going on underneath is quite different from what you see. It turns out it\u0026rsquo;s all an illusion. A massive sleight-of-hand. What you perceive as a shared reality is only an approximation unique to your own point of view and place in time.\nPeer-to-Peer Lockstep In the beginning games were networked peer-to-peer, with each each computer exchanging information with each other in a fully connected mesh topology. You can still see this model alive today in RTS games, and interestingly for some reason, perhaps because it was the first way - it\u0026rsquo;s still how most people think that game networking works.\nThe basic idea is to abstract the game into a series of turns and a set of command messages when processed at the beginning of each turn direct the evolution of the game state. For example: move unit, attack unit, construct building. All that is needed to network this is to run exactly the same set of commands and turns on each player\u0026rsquo;s machine starting from a common initial state.\nOf course this is an overly simplistic explanation and glosses over many subtle points, but it gets across the basic idea of how networking for RTS games work. You can read more about this networking model here: 1500 Archers on a 28.8: Network Programming in Age of Empires and Beyond.\nIt seems so simple and elegant, but unfortunately there are several limitations.\nFirst, it\u0026rsquo;s exceptionally difficult to ensure that a game is completely deterministic; that each turn plays out identically on each machine. For example, one unit could take slightly a different path on two machines, arriving sooner to a battle and saving the day on one machine, while arriving later on the other and erm. not saving the day. Like a butterfly flapping it\u0026rsquo;s wings and causing a hurricane on the other side of the world, one tiny difference results in complete desynchronization over time.\nThe next limitation is that in order to ensure that the game plays out identically on all machines it is necessary to wait until all player\u0026rsquo;s commands for that turn are received before simulating that turn. This means that each player in the game has latency equal to the most lagged player. RTS games typically hide this by providing audio feedback immediately and/or playing cosmetic animation, but ultimately any truly game affecting action may occur only after this delay has passed.\nThe final limitation occurs because of the way the game synchronizes by sending just the command messages which change the state. In order for this to work it is necessary for all players to start from the same initial state. Typically this means that each player must join up in a lobby before commencing play, although it is technically possible to support late join, this is not common due to the difficulty of capturing and transmitting a completely deterministic starting point in the middle of a live game.\nDespite these limitations this model naturally suits RTS games and it still lives on today in games like \u0026ldquo;Command and Conquer\u0026rdquo;, \u0026ldquo;Age of Empires\u0026rdquo; and \u0026ldquo;Starcraft\u0026rdquo;. The reason being that in RTS games the game state consists of many thousands of units and is simply too large to exchange between players. These games have no choice but to exchange the commands which drive the evolution of the game state.\nBut for other genres, the state of the art has moved on. So that\u0026rsquo;s it for the deterministic peer-to-peer lockstep networking model. Now lets look at the evolution of action games starting with Doom, Quake and Unreal.\nClient/Server In the era of action games, the limitations of peer-to-peer lockstep became apparent in Doom, which despite playing well over the LAN played terribly over the internet for typical users:\nAlthough it is possible to connect two DOOM machines together across the Internet using a modem link, the resulting game will be slow, ranging from the unplayable (e.g. a 14.4Kbps PPP connection) to the marginally playable (e.g. a 28.8Kbps modem running a Compressed SLIP driver). Since these sorts of connections are of only marginal utility, this document will focus only on direct net connections. The problem of course was that Doom was designed for networking over LAN only, and used the peer-to-peer lockstep model described previously for RTS games. Each turn player inputs (key presses etc.) were exchanged with other peers, and before any player could simulate a frame all other player\u0026rsquo;s key presses needed to be received.\nIn other words, before you could turn, move or shoot you had to wait for the inputs from the most lagged modem player. Just imagine the wailing and gnashing of teeth that this would have resulted in for the sort of folks with internet connections that were \u0026ldquo;of only marginal utility\u0026rdquo;. :)\nIn order to move beyond the LAN and the well connected elite at university networks and large companies, it was necessary to change the model. And in 1996, that\u0026rsquo;s exactly what John Carmack and his team did when he released Quake using client/server instead of peer-to-peer.\nNow instead of each player running the same game code and communicating directly with each other, each player was now a \u0026ldquo;client\u0026rdquo; and they all communicated with just one computer called the \u0026ldquo;server\u0026rdquo;. There was no longer any need for the game to be deterministic across all machines, because the game really only existed on the server. Each client effectively acted as a dumb terminal showing an approximation of the game as it played out on the server.\nIn a pure client/server model you run no game code locally, instead sending your inputs such as key presses, mouse movement, clicks to the server. In response the server updates the state of your character in the world and replies with a packet containing the state of your character and other players near you. All the client has to do is interpolate between these updates to provide the illusion of smooth movement and BAM you have a networked game.\nThis was a great step forward. The quality of the game experience now depended on the connection between the client and the server instead of the most lagged peer in the game. It also became possible for players to come and go in the middle of the game, and the number of players increased as client/server reduced the bandwidth required on average per-player.\nBut there were still problems with the pure client/server model:\nWhile I can remember and justify all of my decisions about networking from DOOM through Quake, the bottom line is that I was working with the wrong basic assumptions for doing a good internet game. My original design was targeted at \u0026lt; 200ms connection latencies. People that have a digital connection to the internet through a good provider get a pretty good game experience. Unfortunately, 99% of the world gets on with a slip or ppp connection over a modem, often through a crappy overcrowded ISP. This gives 300+ ms latencies, minimum. Client. User\u0026#39;s modem. ISP\u0026#39;s modem. Server. ISP\u0026#39;s modem. User\u0026#39;s modem. Client. God, that sucks. Ok, I made a bad call. I have a T1 to my house, so I just wasn't familliar with PPP life. I'm addressing it now.\nThe problem was of course latency.\nWhat happened next would change the industry forever.\nClient-Side Prediction In the original Quake you felt the latency between your computer and the server. Press forward and you\u0026rsquo;d wait however long it took for packets to travel to the server and back to you before you\u0026rsquo;d actually start moving. Press fire and you wait for that same delay before shooting.\nIf you\u0026rsquo;ve played any modern FPS like Call of Duty: Modern Warfare, you know this is no longer what happens. So how exactly do modern FPS games remove the latency on your own actions in multiplayer?\nWhen writing about his plans for the soon to be released QuakeWorld, John Carmack said:\nI am now allowing the client to guess at the results of the users movement until the authoritative response from the server comes through. This is a biiiig architectural change. The client now needs to know about solidity of objects, friction, gravity, etc. I am sad to see the elegant client-as-terminal setup go away, but I am practical above idealistic. So now in order to remove the latency, the client runs more code than it previously did. It is no longer a dumb terminal sending inputs to the server and interpolating between state sent back. Instead it is able to predict the movement of your character locally and immediately in response to your input, running a subset of the game code for your player character on the client machine.\nNow as soon as you press forward, there is no wait for a round trip between client and server - your character start moving forward right away.\nThe difficulty of this approach is not in the prediction, for the prediction works just as normal game code does - evolving the state of the game character forward in time according to the player\u0026rsquo;s input. The difficulty is in applying the correction back from the server to resolve cases when the client and server disagree about where the player character should be and what it is doing.\nNow at this point you might wonder. Hey, if you are running code on the client - why not just make the client authoritative over their player character? The client could run the simulation code for their own character and simply tell the server where they are each time they send a packet. The problem with this is that if each player were able to simply tell the server \u0026ldquo;here is my current position\u0026rdquo; it would be trivially easy to hack the client such that a cheater could instantly dodge the RPG about to hit them, or teleport instantly behind you to shoot you in the back.\nSo in FPS games it is absolutely necessary that the server is the authoritative over the state of each player character, in-spite of the fact that each player is locally predicting the motion of their own character to hide latency. As Tim Sweeney writes in The Unreal Networking Architecture: \u0026ldquo;The Server Is The Man\u0026rdquo;.\nHere is where it gets interesting. If the client and the server disagree, the client must accept the update for the position from the server, but due to latency between the client and server this correction is necessarily in the past. For example, if it takes 100ms from client to server and 100ms back, then any server correction for the player character position will appear to be 200ms in the past, relative to the time up to which the client has predicted their own movement.\nIf the client were to simply apply this server correction update verbatim, it would yank the client back in time, completely undoing any client-side prediction. How then to solve this while still allowing the client to predict ahead?\nThe solution is to keep a circular buffer of past character state and input for the local player on the client, then when the client receives a correction from the server, it first discards any buffered state older than the corrected state from the server, and replays the state starting from the corrected state back to the present \u0026ldquo;predicted\u0026rdquo; time on the client using player inputs stored in the circular buffer. In effect the client invisibly \u0026ldquo;rewinds and replays\u0026rdquo; the last n frames of local player character movement while holding the rest of the world fixed.\nThis way the player appears to control their own character without any latency, and provided that the client and server character simulation code is reasonable, giving roughly exactly the same result for the same inputs on the client and server, it is rarely corrected. It is as Tim Sweeney describes:\n... the best of both worlds: In all cases, the server remains completely authoritative. Nearly all the time, the client movement simulation exactly mirrors the client movement carried out by the server, so the client's position is seldom corrected. Only in the rare case, such as a player getting hit by a rocket, or bumping into an enemy, will the client's location need to be corrected. In other words, only when the player\u0026rsquo;s character is affected by something external to the local player\u0026rsquo;s input, which cannot possibly be predicted on the client, will the player\u0026rsquo;s position need to be corrected. That and of course, if the player is attempting to cheat :)\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1266969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1266969600,"objectID":"1feedb722e52a6f1591ea5c24c8cc59b","permalink":"https://gafferongames.com/post/what_every_programmer_needs_to_know_about_game_networking/","publishdate":"2010-02-24T00:00:00Z","relpermalink":"/post/what_every_programmer_needs_to_know_about_game_networking/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nHave you ever wondered how multiplayer games work?\nFrom the outside it seems magical: two or more players sharing a consistent experience across the network like they actually exist together in the same virtual world.\nBut as programmers we know the truth of what is actually going on underneath is quite different from what you see. It turns out it\u0026rsquo;s all an illusion.","tags":["networking"],"title":"What Every Programmer Needs To Know About Game Networking","type":"post"},{"authors":null,"categories":["Game Networking"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn the previous article, we added our own concept of virtual connection on top of UDP. In this article we’re going to add reliability, ordering and congestion avoidance to our virtual UDP connection.\nThe Problem with TCP Those of you familiar with TCP know that it already has its own concept of connection, reliability-ordering and congestion avoidance, so why are we rewriting our own mini version of TCP on top of UDP?\nThe issue is that multiplayer action games rely on a steady stream of packets sent at rates of 10 to 30 packets per second, and for the most part, the data contained is these packets is so time sensitive that only the most recent data is useful. This includes data such as player inputs, the position, orientation and velocity of each player character, and the state of physics objects in the world.\nThe problem with TCP is that it abstracts data delivery as a reliable ordered stream. Because of this, if a packet is lost, TCP has to stop and wait for that packet to be resent. This interrupts the steady stream of packets because more recent packets must wait in a queue until the resent packet arrives, so packets are received in the same order they were sent.\nWhat we need is a different type of reliability. Instead of having all data treated as a reliable ordered stream, we want to send packets at a steady rate and get notified when packets are received by the other computer. This allows time sensitive data to get through without waiting for resent packets, while letting us make our own decision about how to handle packet loss at the application level.\nIt is not possible to implement a reliability system with these properties using TCP, so we have no choice but to roll our own reliability on top of UDP.\nSequence Numbers The goal of our reliability system is simple: we want to know which packets arrive at the other side of the connection.\nFirst we need a way to identify packets.\nWhat if we had added the concept of a \u0026ldquo;packet id\u0026rdquo;? Let\u0026rsquo;s make it an integer value. We could start this at zero then with each packet we send, increase the number by one. The first packet we send would be packet 0, and the 100th packet sent is packet 99.\nThis is actually quite a common technique. It\u0026rsquo;s even used in TCP! These packet ids are called sequence numbers. While we’re not going to implement reliability exactly as TCP does, it makes sense to use the same terminology, so we’ll call them sequence numbers from now on.\nSince UDP does not guarantee the order of packets, the 100th packet received is not necessarily the 100th packet sent. It follows that we need to insert the sequence number somewhere in the packet, so that the computer at the other side of the connection knows which packet it is.\nWe already have a simple packet header for the virtual connection from the previous article, so we\u0026rsquo;ll just add the sequence number in the header like this:\n[uint protocol id] [uint sequence] (packet data...) Now when the other computer receives a packet it knows its sequence number according to the computer that sent it.\nAcks Now that we can identify packets using sequence numbers, the next step is to let the other side of the connection know which packets we receive.\nLogically this is quite simple, we just need to take note of the sequence number of each packet we receive, and send those sequence numbers back to the computer that sent them.\nBecause we are sending packets continuously between both machines, we can just add the ack to the packet header, just like we did with the sequence number:\n[uint protocol id] [uint sequence] [uint ack] (packet data...) Our general approach is as follows:\nEach time we send a packet we increase the local sequence number\nWhen we receieve a packet, we check the sequence number of the packet against the sequence number of the most recently received packet, called the remote sequence number. If the packet is more recent, we update the remote sequence to be equal to the sequence number of the packet.\nWhen we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack.\nThis simple ack system works provided that one packet comes in for each packet we send out.\nBut what if packets clump up such that two packets arrive before we send a packet? We only have space for one ack per-packet, so what do we do?\nNow consider the case where one side of the connection is sending packets at a faster rate. If the client sends 30 packets per-second, and the server only sends 10 packets per-second, we need at least 3 acks included in each packet sent from the server.\nLet\u0026rsquo;s make it even more complex! What if the packet containing the ack is lost? The computer that sent the packet would think the packet got lost but it was actually received!\nIt seems like we need to make our reliability system\u0026hellip; more reliable!\nReliable Acks Here is where we diverge significantly from TCP.\nWhat TCP does is maintain a sliding window where the ack sent is the sequence number of the next packet it expects to receive, in order. If TCP does not receive an ack for a given packet, it stops and resends a packet with that sequence number again. This is exactly the behavior we want to avoid!\nIn our reliability system, we never resend a packet with a given sequence number. We sequence n exactly once, then we send n+1, n+2 and so on. We never stop and resend packet n if it was lost, we leave it up to the application to compose a new packet containing the data that was lost, if necessary, and this packet gets sent with a new sequence number.\nBecause we\u0026rsquo;re doing things differently to TCP, its now possible to have holes in the set of packets we ack, so it is no longer sufficient to just state the sequence number of the most recent packet we have received.\nWe need to include multiple acks per-packet.\nHow many acks do we need?\nAs mentioned previously we have the case where one side of the connection sends packets faster than the other. Let\u0026rsquo;s assume that the worst case is one side sending no less than 10 packets per-second, while the other sends no more than 30. In this case, the average number of acks we\u0026rsquo;ll need per-packet is 3, but if packets clump up a bit, we would need more. Let\u0026rsquo;s say 6-10 worst case.\nWhat about acks that don\u0026rsquo;t get through because the packet containing the ack is lost?\nTo solve this, we\u0026rsquo;re going to use a classic networking strategy of using redundancy to defeat packet loss!\nLet\u0026rsquo;s include 33 acks per-packet, and this isn\u0026rsquo;t just going to be up to 33, but always 33. So for any given ack we redundantly send it up to 32 additional times, just in case one packet with the ack doesn\u0026rsquo;t get through!\nBut how can we possibly fit 33 acks in a packet? At 4 bytes per-ack thats 132 bytes!\nThe trick is to represent the 32 previous acks before \u0026ldquo;ack\u0026rdquo; using a bitfield:\n[uint protocol id] [uint sequence] [uint ack] [uint ack bitfield] \u0026lt;em\u0026gt;(packet data...)\u0026lt;/em\u0026gt; We define \u0026ldquo;ack bitfield\u0026rdquo; such that each bit corresponds to acks of the 32 sequence numbers before \u0026ldquo;ack\u0026rdquo;. So let\u0026rsquo;s say \u0026ldquo;ack\u0026rdquo; is 100. If the first bit of \u0026ldquo;ack bitfield\u0026rdquo; is set, then the packet also includes an ack for packet 99. If the second bit is set, then packet 98 is acked. This goes all the way down to the 32nd bit for packet 68.\nOur adjusted algorithm looks like this:\nEach time we send a packet we increase the local sequence number\nWhen we receive a packet, we check the sequence number of the packet against the remote sequence number. If the packet sequence is more recent, we update the remote sequence number.\nWhen we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack. The ack bitfield is calculated by looking into a queue of up to 33 packets, containing sequence numbers in the range [remote sequence - 32, remote sequence]. We set bit n (in [1,32]) in ack bits to 1 if the sequence number remote sequence - n is in the received queue.\nAdditionally, when a packet is received, ack bitfield is scanned and if bit n is set, then we acknowledge sequence number packet sequence - n, if it has not been acked already.\nWith this improved algorithm, you would have to lose 100% of packets for more than a second to stop an ack getting through. And of course, it easily handles different send rates and clumped up packet receives.\nDetecting Lost Packets Now that we know what packets are received by the other side of the connection, how do we detect packet loss?\nThe trick here is to flip it around and say that if you don\u0026rsquo;t get an ack for a packet within a certain amount of time, then we consider that packet lost.\nGiven that we are sending at no more than 30 packets per second, and we are redundantly sending acks roughly 30 times, if you don\u0026rsquo;t get an ack for a packet within one second, it is very likely that packet was lost.\nSo we are playing a bit of a trick here, while we can know 100% for sure which packets get through, but we can only be reasonably certain of the set of packets that didn\u0026rsquo;t arrive.\nThe implication of this is that any data which you resend using this reliability technique needs to have its own message id so that if you receive it multiple times, you can discard it. This can be done at the application level.\nHandling Sequence Number Wrap-Around No discussion of sequence numbers and acks would be complete without coverage of sequence number wrap around!\nSequence numbers and acks are 32 bit unsigned integers, so they can represent numbers in the range [0,4294967295]. Thats a very high number! So high that if you sent 30 packets per-second, it would take over four and a half years for the sequence number to wrap back around to zero.\nBut perhaps you want to save some bandwidth so you shorten your sequence numbers and acks to 16 bit integers. You save 4 bytes per-packet, but now they wrap around in only half an hour.\nSo how do we handle this wrap around case?\nThe trick is to realize that if the current sequence number is already very high, and the next sequence number that comes in is very low, then you must have wrapped around. So even though the new sequence number is numerically lower than the current sequence value, it actually represents a more recent packet.\nFor example, let\u0026rsquo;s say we encoded sequence numbers in one byte (not recommended btw. :)), then they would wrap around after 255 like this:\n... 252, 253, 254, 255, 0, 1, 2, 3, ... To handle this case we need a new function that is aware of the fact that sequence numbers wrap around to zero after 255, so that 0, 1, 2, 3 are considered more recent than 255. Otherwise, our reliability system stops working after you receive packet 255.\nHere\u0026rsquo;s a function for 16 bit sequence numbers:\ninline bool sequence_greater_than( uint16_t s1, uint16_t s2 ) { return ( ( s1 \u0026gt; s2 ) \u0026amp;\u0026amp; ( s1 - s2 \u0026lt;= 32768 ) ) || ( ( s1 \u0026lt; s2 ) \u0026amp;\u0026amp; ( s2 - s1 \u0026gt; 32768 ) ); } This function works by comparing the two numbers and their difference. If their difference is less than 1/2 the maximum sequence number value, then they must be close together - so we just check if one is greater than the other, as usual. However, if they are far apart, their difference will be greater than 1/2 the max sequence, then we paradoxically consider the sequence number more recent if it is less than the current sequence number.\nThis last bit is what handles the wrap around of sequence numbers transparently, so 0,1,2 are considered more recent than 255.\nMake sure you include this in any sequence number processing you do.\nCongestion Avoidance While we have solved reliability, there is still the question of congestion avoidance. TCP provides congestion avoidance as part of the packet when you get TCP reliability, but UDP has no congestion avoidance whatsoever!\nIf we just send packets without some sort of flow control, we risk flooding the connection and inducing severe latency (2 seconds plus!) as routers between us and the other computer become congested and buffer up packets. This happens because routers try very hard to deliver all the packets we send, and therefore tend to buffer up packets in a queue before they consider dropping them.\nWhile it would be nice if we could tell the routers that our packets are time sensitive and should be dropped instead of buffered if the router is overloaded, we can\u0026rsquo;t really do this without rewriting the software for all routers in the world.\nInstead, we need to focus on what we can actually do which is to avoid flooding the connection in the first place. We try to avoid sending too much bandwidth in the first place, and then if we detect congestion, we attempt to back off and send even less.\nThe way to do this is to implement our own basic congestion avoidance algorithm. And I stress basic! Just like reliability, we have no hope of coming up with something as general and robust as TCP\u0026rsquo;s implementation on the first try, so let\u0026rsquo;s keep it as simple as possible.\nMeasuring Round Trip Time Since the whole point of congestion avoidance is to avoid flooding the connection and increasing round trip time (RTT), it makes sense that the most important metric as to whether or not we are flooding our connection is the RTT itself.\nWe need a way to measure the RTT of our connection.\nHere is the basic technique:\nFor each packet we send, we add an entry to a queue containing the sequence number of the packet and the time it was sent.\nEach time we receive an ack, we look up this entry and note the difference in local time between the time we receive the ack, and the time we sent the packet. This is the RTT time for that packet.\nBecause the arrival of packets varies with network jitter, we need to smooth this value to provide something meaningful, so each time we obtain a new RTT we move a percentage of the distance between our current RTT and the packet RTT. 10% seems to work well for me in practice. This is called an exponentially smoothed moving average, and it has the effect of smoothing out noise in the RTT with a low pass filter.\nTo ensure that the sent queue doesn\u0026rsquo;t grow forever, we discard packets once they have exceeded some maximum expected RTT. As discussed in the previous section on reliability, it is exceptionally likely that any packet not acked within a second was lost, so one second is a good value for this maximum RTT.\nNow that we have RTT, we can use it as a metric to drive our congestion avoidance. If RTT gets too large, we send data less frequently, if its within acceptable ranges, we can try sending data more frequently.\nSimple Binary Congestion Avoidance As discussed before, let\u0026rsquo;s not get greedy, we\u0026rsquo;ll implement a very basic congestion avoidance. This congestion avoidance has two modes. Good and bad. I call it simple binary congestion avoidance.\nLet\u0026rsquo;s assume you send packets of a certain size, say 256 bytes. You would like to send these packets 30 times a second, but if conditions are bad, you can drop down to 10 times a second.\nSo 256 byte packets 30 times a second is around 64kbits/sec, and 10 times a second is roughly 20kbit/sec. There isn\u0026rsquo;t a broadband network connection in the world that can\u0026rsquo;t handle at least 20kbit/sec, so we\u0026rsquo;ll move forward with this assumption. Unlike TCP which is entirely general for any device with any amount of send/recv bandwidth, we\u0026rsquo;re going to assume a minimum supported bandwidth for devices involved in our connections.\nSo the basic idea is this. When network conditions are \u0026ldquo;good\u0026rdquo; we send 30 packets per-second, and when network conditions are \u0026ldquo;bad\u0026rdquo; we drop to 10 packets per-second.\nOf course, you can define \u0026ldquo;good\u0026rdquo; and \u0026ldquo;bad\u0026rdquo; however you like, but I\u0026rsquo;ve gotten good results considering only RTT. For example if RTT exceeds some threshold (say 250ms) then you know you are probably flooding the connection. Of course, this assumes that nobody would normally exceed 250ms under non-flooding conditions, which is reasonable given our broadband requirement.\nHow do you switch between good and bad? The algorithm I like to use operates as follows:\nIf you are currently in good mode, and conditions become bad, immediately drop to bad mode\nIf you are in bad mode, and conditions have been good for a specific length of time \u0026rsquo;t\u0026rsquo;, then return to good mode\nTo avoid rapid toggling between good and bad mode, if you drop from good mode to bad in under 10 seconds, double the amount of time \u0026rsquo;t\u0026rsquo; before bad mode goes back to good. Clamp this at some maximum, say 60 seconds.\nTo avoid punishing good connections when they have short periods of bad behavior, for each 10 seconds the connection is in good mode, halve the time \u0026rsquo;t\u0026rsquo; before bad mode goes back to good. Clamp this at some minimum like 1 second.\nWith this algorithm you will rapidly respond to bad conditions and drop your send rate to 10 packets per-second, avoiding flooding of the connection. You\u0026rsquo;ll also conservatively try out good mode, and persist sending packets at a higher rate of 30 packets per-second, while network conditions are good.\nOf course, you can implement much more sophisticated algorithms. Packet loss % can be taken into account as a metric, even the amount of network jitter (time variance in packet acks), not just RTT.\nYou can also get much more greedy with congestion avoidance, and attempt to discover when you can send data at a much higher bandwidth (eg. LAN), but you have to be very careful! With increased greediness comes more risk that you\u0026rsquo;ll flood the connection.\nConclusion Our new reliability system let\u0026rsquo;s us send a steady stream of packets and notifies us which packets are received. From this we can infer lost packets, and resend data that didn\u0026rsquo;t get through if necessary. We also have a simple congestion avoidance system that drops from 30 packets per-second to 10 times a second so we don\u0026rsquo;t flood the connection.\nNEXT: What Every Programmer Needs to Know About Game Networking\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1224460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1224460800,"objectID":"378d2f3be9528006d1641bd1b8089ad9","permalink":"https://gafferongames.com/post/reliability_ordering_and_congestion_avoidance_over_udp/","publishdate":"2008-10-20T00:00:00Z","relpermalink":"/post/reliability_ordering_and_congestion_avoidance_over_udp/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn the previous article, we added our own concept of virtual connection on top of UDP. In this article we’re going to add reliability, ordering and congestion avoidance to our virtual UDP connection.\nThe Problem with TCP Those of you familiar with TCP know that it already has its own concept of connection, reliability-ordering and congestion avoidance, so why are we rewriting our own mini version of TCP on top of UDP?","tags":["networking"],"title":"Reliability and Congestion Avoidance over UDP","type":"post"},{"authors":null,"categories":["Game Networking"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn the previous article we sent and received packets over UDP. Since UDP is connectionless, one UDP socket can be used to exchange packets with any number of different computers. In multiplayer games however, we usually only want to exchange packets between a small set of connected computers.\nAs the first step towards a general connection system, we\u0026rsquo;ll start with the simplest case possible: creating a virtual connection between two computers on top of UDP.\nBut first, we\u0026rsquo;re going to dig in a bit deeper about how the Internet really works!\nThe Internet NOT a series of tubes In 2006, Senator Ted Stevens made internet history with his famous speech on the net neutrality act:\n\"The internet is not something that you just dump something on. It's not a big truck. It's a series of tubes\" When I first started using the Internet, I was just like Ted. Sitting in the computer lab in University of Sydney in 1995, I was \u0026ldquo;surfing the web\u0026rdquo; with this new thing called Netscape Navigator, and I had absolutely no idea what was going on.\nYou see, I thought each time you connected to a website there was some actual connection going on, like a telephone line. I wondered, how much does it cost each time I connect to a new website? 30 cents? A dollar? Was somebody from the university going to tap me on the shoulder and ask me to pay the long distance charges? :)\nOf course, this all seems silly now.\nThere is no switchboard somewhere that directly connects you via a physical phone line to the other computer you want to talk to, let alone a series of pneumatic tubes like Sen. Stevens would have you believe.\nNo Direct Connections Instead your data is sent over Internet Protocol (IP) via packets that hop from computer to computer.\nA packet may pass through several computers before it reaches its destination. You cannot know the exact set of computers in advance, as it changes dynamically depending on how the network decides to route packets. You could even send two packets A and B to the same address, and they may take different routes.\nOn unix-like systems can inspect the route that packets take by calling \u0026ldquo;traceroute\u0026rdquo; and passing in a destination hostname or IP address.\nOn windows, replace \u0026ldquo;traceroute\u0026rdquo; with \u0026ldquo;tracert\u0026rdquo; to get it to work.\nTry it with a few websites like this:\ntraceroute slashdot.org traceroute amazon.com traceroute google.com traceroute bbc.co.uk traceroute news.com.au Take a look and you should be able to convince yourself pretty quickly that there is no direct connection.\nHow Packets Get Delivered In the first article, I presented a simple analogy for packet delivery, describing it as somewhat like a note being passed from person to person across a crowded room.\nWhile this analogy gets the basic idea across, it is much too simple. The Internet is not a flat network of computers, it is a network of networks. And of course, we don\u0026rsquo;t just need to pass letters around a small room, we need to be able to send them anywhere in the world.\nIt should be pretty clear then that the best analogy is the postal service!\nWhen you want to send a letter to somebody you put your letter in the mailbox and you trust that it will be delivered correctly. It\u0026rsquo;s not really relevant to you how it gets there, as long as it does. Somebody has to physically deliver your letter to its destination of course, so how is this done?\nWell first off, the postman sure as hell doesn\u0026rsquo;t take your letter and deliver it personally! It seems that the postal service is not a series of tubes either. Instead, the postman takes your letter to the local post office for processing.\nIf the letter is addressed locally then the post office just sends it back out, and another postman delivers it directly. But, if the address is is non-local then it gets interesting! The local post office is not able to deliver the letter directly, so it passes it \u0026ldquo;up\u0026rdquo; to the next level of hierarchy, perhaps to a regional post office which services cities nearby, or maybe to a mail center at an airport, if the address is far away. Ideally, the actual transport of the letter would be done using a big truck.\nLets be complicated and assume the letter is sent from Los Angeles to Sydney, Australia. The local post office receives the letter and given that it is addressed internationally, sends it directly to a mail center at LAX. The letter is processed again according to address, and gets routed on the next flight to Sydney.\nThe plane lands at Sydney airport where an entirely different postal system takes over. Now the whole process starts operating in reverse. The letter travels \u0026ldquo;down\u0026rdquo; the hierarchy, from the general, to the specific. From the mail hub at Sydney Airport it gets sent out to a regional center, the regional center delivers it to the local post office, and eventually the letter is hand delivered by a mailman with a funny accent. Crikey! :)\nJust like post offices determine how to deliver letters via their address, networks deliver packets according to their IP address. The low-level details of this delivery and the actual routing of packets from network to network is actually quite complex, but the basic idea is that each router is just another computer, with a routing table describing where packets matching sets of addresses should go, as well as a default gateway address describing where to pass packets for which there is no matching entry in the table. It is routing tables, and the physical connections they represent that define the network of networks that is the Internet.\nThe job of configuring these routing tables is up to network administrators, not programmers like us. But if you want to read more about it, then this article from ars technica provides some fascinating insight into how networks exchange packets between each other via peering and transit relationships. You can also read more details about routing tables in this linux faq, and about the border gateway protocol on wikipedia, which automatically discovers how to route packets between networks, making the internet a truly distributed system capable of dynamically routing around broken connectivity.\nVirtual Connections Now back to connections.\nIf you have used TCP sockets then you know that they sure look like a connection, but since TCP is implemented on top of IP, and IP is just packets hopping from computer to computer, it follows that TCP\u0026rsquo;s concept of connection must be a virtual connection.\nIf TCP can create a virtual connection over IP, it follows that we can do the same over UDP.\nLets define our virtual connection as two computers exchanging UDP packets at some fixed rate like 10 packets per-second. As long as the packets are flowing, we consider the two computers to be virtually connected.\nOur connection has two sides:\nOne computer sits there and listens for another computer to connect to it. We'll call this computer the server. Another computer connects to a server by specifying an IP address and port. We'll call this computer the client. In our case, we only allow one client to connect to the server at any time. We\u0026rsquo;ll generalize our connection system to support multiple simultaneous connections in a later article. Also, we assume that the IP address of the server is on a fixed IP address that the client may directly connect to.\nProtocol ID Since UDP is connectionless our UDP socket can receive packets sent from any computer.\nWe\u0026rsquo;d like to narrow this down so that the server only receives packets sent from the client, and the client only receives packets sent from the server. We can\u0026rsquo;t just filter out packets by address, because the server doesn\u0026rsquo;t know the address of the client in advance. So instead, we prefix each UDP packet with small header containing a 32 bit protocol id as follows:\n[uint protocol id] (packet data...) The protocol id is just some unique number representing our game protocol. Any packet that arrives from our UDP socket first has its first four bytes inspected. If they don\u0026rsquo;t match our protocol id, then the packet is ignored. If the protocol id does match, we strip out the first four bytes of the packet and deliver the rest as payload.\nYou just choose some number that is reasonably unique, perhaps a hash of the name of your game and the protocol version number. But really you can use anything. The whole point is that from the point of view of our connection based protocol, packets with different protocol ids are ignored.\nDetecting Connection Now we need a way to detect connection.\nSure we could do some complex handshaking involving multiple UDP packets sent back and forth. Perhaps a client \u0026ldquo;request connection\u0026rdquo; packet is sent to the server, to which the server responds with a \u0026ldquo;connection accepted\u0026rdquo; sent back to the client, or maybe an \u0026ldquo;i\u0026rsquo;m busy\u0026rdquo; packet if a client tries to connect to server which already has a connected client.\nOr\u0026hellip; we could just setup our server to take the first packet it receives with the correct protocol id, and consider a connection to be established.\nThe client just starts sending packets to the server assuming connection, when the server receives the first packet from the client, it takes note of the IP address and port of the client, and starts sending packets back.\nThe client already knows the address and port of the server, since it was specified on connect. So when the client receives packets, it filters out any that don\u0026rsquo;t come from the server address. Similarly, once the server receives the first packet from the client, it gets the address and port of the client from \u0026ldquo;recvfrom\u0026rdquo;, so it is able to ignore any packets that don\u0026rsquo;t come from the client address.\nWe can get away with this shortcut because we only have two computers involved in the connection. In later articles, we\u0026rsquo;ll extend our connection system to support more than two computers in a client/server or peer-to-peer topology, and at this point we\u0026rsquo;ll upgrade our connection negotiation to something more robust.\nBut for now, why make things more complicated than they need to be?\nDetecting Disconnection How do we detect disconnection?\nWell if a connection is defined as receiving packets, we can define disconnection as not receiving packets.\nTo detect when we are not receiving packets, we keep track of the number of seconds since we last received a packet from the other side of the connection. We do this on both sides.\nEach time we receive a packet from the other side, we reset our accumulator to 0.0, each update we increase the accumulator by the amount of time that has passed.\nIf this accumulator exceeds some value like 10 seconds, the connection \u0026ldquo;times out\u0026rdquo; and we disconnect.\nThis also gracefully handles the case of a second client trying to connect to a server that has already made a connection with another client. Since the server is already connected it ignores packets coming from any address other than the connected client, so the second client receives no packets in response to the packets it sends, so the second client times out and disconnects.\nConclusion And that\u0026rsquo;s all it takes to setup a virtual connection: some way to establish connection, filtering for packets not involved in the connection, and timeouts to detect disconnection.\nOur connection is as real as any TCP connection, and the steady stream of UDP packets it provides is a suitable starting point for a multiplayer action game.\nNow that you have your virtual connection over UDP, you can easily setup a client/server relationship for a two player multiplayer game without TCP.\nNEXT ARTICLE: Reliability and Congestion Avoidance over UDP\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1223424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1223424000,"objectID":"0e05d3eb7aa200d48870d0b2b376f315","permalink":"https://gafferongames.com/post/virtual_connection_over_udp/","publishdate":"2008-10-08T00:00:00Z","relpermalink":"/post/virtual_connection_over_udp/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn the previous article we sent and received packets over UDP. Since UDP is connectionless, one UDP socket can be used to exchange packets with any number of different computers. In multiplayer games however, we usually only want to exchange packets between a small set of connected computers.\nAs the first step towards a general connection system, we\u0026rsquo;ll start with the simplest case possible: creating a virtual connection between two computers on top of UDP.","tags":["networking"],"title":"Virtual Connection over UDP","type":"post"},{"authors":null,"categories":["Game Networking"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn the previous article we discussed options for sending data between computers and decided to use UDP instead of TCP for time critical data.\nIn this article I am going to show you how to send and receive UDP packets.\nBSD sockets For most modern platforms you have some sort of basic socket layer available based on BSD sockets.\nBSD sockets are manipulated using simple functions like \u0026ldquo;socket\u0026rdquo;, \u0026ldquo;bind\u0026rdquo;, \u0026ldquo;sendto\u0026rdquo; and \u0026ldquo;recvfrom\u0026rdquo;. You can of course work directly with these functions if you wish, but it becomes difficult to keep your code platform independent because each platform is slightly different.\nSo although I will first show you BSD socket example code to demonstrate basic socket usage, we won\u0026rsquo;t be using BSD sockets directly for long. Once we\u0026rsquo;ve covered all basic socket functionality we\u0026rsquo;ll abstract everything away into a set of classes, making it easy to you to write platform independent socket code.\nPlatform specifics First let\u0026rsquo;s setup a define so we can detect what our current platform is and handle the slight differences in sockets from one platform to another:\n// platform detection #define PLATFORM_WINDOWS 1 #define PLATFORM_MAC 2 #define PLATFORM_UNIX 3 #if defined(_WIN32) #define PLATFORM PLATFORM_WINDOWS #elif defined(__APPLE__) #define PLATFORM PLATFORM_MAC #else #define PLATFORM PLATFORM_UNIX #endif Now let\u0026rsquo;s include the appropriate headers for sockets. Since the header files are platform specific, we\u0026rsquo;ll use the platform #define to include different sets of files depending on the platform:\n#if PLATFORM == PLATFORM_WINDOWS #include \u0026lt;winsock2.h\u0026gt; #elif PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #endif Sockets are built in to the standard system libraries on unix-based platforms so we don\u0026rsquo;t have to link to any additonal libraries. However, on Windows we need to link to the winsock library to get socket functionality.\nHere is a simple trick to do this without having to change your project or makefile:\n#if PLATFORM == PLATFORM_WINDOWS #pragma comment( lib, \u0026quot;wsock32.lib\u0026quot; ) #endif I like this trick because I\u0026rsquo;m super lazy. You can always link from your project or makefile if you wish.\nInitializing the socket layer Most unix-like platforms (including macosx) don\u0026rsquo;t require any specific steps to initialize the sockets layer, however Windows requires that you jump through some hoops to get your socket code working.\nYou must call \u0026ldquo;WSAStartup\u0026rdquo; to initialize the sockets layer before you call any socket functions, and \u0026ldquo;WSACleanup\u0026rdquo; to shutdown when you are done.\nLet\u0026rsquo;s add two new functions:\nbool InitializeSockets() { #if PLATFORM == PLATFORM_WINDOWS WSADATA WsaData; return WSAStartup( MAKEWORD(2,2), \u0026amp;WsaData ) == NO_ERROR; #else return true; #endif } void ShutdownSockets() { #if PLATFORM == PLATFORM_WINDOWS WSACleanup(); #endif } Now we have a platform independent way to initialize the socket layer.\nCreating a socket It\u0026rsquo;s time to create a UDP socket, here\u0026rsquo;s how to do it:\nint handle = socket( AF_INET, SOCK_DGRAM, IPPROTO_UDP ); if ( handle \u0026lt;= 0 ) { printf( \u0026quot;failed to create socket\\n\u0026quot; ); return false; } Next we bind the UDP socket to a port number (eg. 30000). Each socket must be bound to a unique port, because when a packet arrives the port number determines which socket to deliver to. Don\u0026rsquo;t use ports lower than 1024 because they are reserved for the system. Also try to avoid using ports above 50000 because they used when dynamically assigning ports.\nSpecial case: if you don\u0026rsquo;t care what port your socket gets bound to just pass in \u0026ldquo;0\u0026rdquo; as your port, and the system will select a free port for you.\nsockaddr_in address; address.sin_family = AF_INET; address.sin_addr.s_addr = INADDR_ANY; address.sin_port = htons( (unsigned short) port ); if ( bind( handle, (const sockaddr*) \u0026amp;address, sizeof(sockaddr_in) ) \u0026lt; 0 ) { printf( \u0026quot;failed to bind socket\\n\u0026quot; ); return false; } Now the socket is ready to send and receive packets.\nBut what is this mysterious call to \u0026ldquo;htons\u0026rdquo; in the code above? This is just a helper function that converts a 16 bit integer value from host byte order (little or big-endian) to network byte order (big-endian). This is required whenever you directly set integer members in socket structures.\nYou\u0026rsquo;ll see \u0026ldquo;htons\u0026rdquo; (host to network short) and its 32 bit integer sized cousin \u0026ldquo;htonl\u0026rdquo; (host to network long) used several times throughout this article, so keep an eye out, and you\u0026rsquo;ll know what is going on.\nSetting the socket as non-blocking By default sockets are set in what is called \u0026ldquo;blocking mode\u0026rdquo;.\nThis means that if you try to read a packet using \u0026ldquo;recvfrom\u0026rdquo;, the function will not return until a packet is available to read. This is not at all suitable for our purposes. Video games are realtime programs that simulate at 30 or 60 frames per second, they can\u0026rsquo;t just sit there waiting for a packet to arrive!\nThe solution is to flip your sockets into \u0026ldquo;non-blocking mode\u0026rdquo; after you create them. Once this is done, the \u0026ldquo;recvfrom\u0026rdquo; function returns immediately when no packets are available to read, with a return value indicating that you should try to read packets again later.\nHere\u0026rsquo;s how put a socket in non-blocking mode:\n#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX int nonBlocking = 1; if ( fcntl( handle, F_SETFL, O_NONBLOCK, nonBlocking ) == -1 ) { printf( \u0026quot;failed to set non-blocking\\n\u0026quot; ); return false; } #elif PLATFORM == PLATFORM_WINDOWS DWORD nonBlocking = 1; if ( ioctlsocket( handle, FIONBIO, \u0026amp;nonBlocking ) != 0 ) { printf( \u0026quot;failed to set non-blocking\\n\u0026quot; ); return false; } #endif Windows does not provide the \u0026ldquo;fcntl\u0026rdquo; function, so we use the \u0026ldquo;ioctlsocket\u0026rdquo; function instead.\nSending packets UDP is a connectionless protocol, so each time you send a packet you must specify the destination address. This means you can use one UDP socket to send packets to any number of different IP addresses, there\u0026rsquo;s no single computer at the other end of your UDP socket that you are connected to.\nHere\u0026rsquo;s how to send a packet to a specific address:\nint sent_bytes = sendto( handle, (const char*)packet_data, packet_size, 0, (sockaddr*)\u0026amp;address, sizeof(sockaddr_in) ); if ( sent_bytes != packet_size ) { printf( \u0026quot;failed to send packet\\n\u0026quot; ); return false; } Important! The return value from \u0026ldquo;sendto\u0026rdquo; only indicates if the packet was successfully sent from the local computer. It does not tell you whether or not the packet was received by the destination computer. UDP has no way of knowing whether or not the the packet arrived at its destination!\nIn the code above we pass a \u0026ldquo;sockaddr_in\u0026rdquo; structure as the destination address. How do we setup one of these structures?\nLet\u0026rsquo;s say we want to send to the address 207.45.186.98:30000\nStarting with our address in this form:\nunsigned int a = 207; unsigned int b = 45; unsigned int c = 186; unsigned int d = 98; unsigned short port = 30000; We have a bit of work to do to get it in the form required by \u0026ldquo;sendto\u0026rdquo;:\nunsigned int address = ( a \u0026lt;\u0026lt; 24 ) | ( b \u0026lt;\u0026lt; 16 ) | ( c \u0026lt;\u0026lt; 8 ) | d; sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_addr.s_addr = htonl( address ); addr.sin_port = htons( port ); As you can see, we first combine the a,b,c,d values in range [0,255] into a single unsigned integer, with each byte of the integer now corresponding to the input values. We then initialize a \u0026ldquo;sockaddr_in\u0026rdquo; structure with the integer address and port, making sure to convert our integer address and port values from host byte order to network byte order using \u0026ldquo;htonl\u0026rdquo; and \u0026ldquo;htons\u0026rdquo;.\nSpecial case: if you want to send a packet to yourself, there\u0026rsquo;s no need to query the IP address of your own machine, just pass in the loopback address 127.0.0.1 and the packet will be sent to your local machine.\nReceiving packets Once you have a UDP socket bound to a port, any UDP packets sent to your sockets IP address and port are placed in a queue. To receive packets just loop and call \u0026ldquo;recvfrom\u0026rdquo; until it fails with EWOULDBLOCK indicating there are no more packets to receive.\nSince UDP is connectionless, packets may arrive from any number of different computers. Each time you receive a packet \u0026ldquo;recvfrom\u0026rdquo; gives you the IP address and port of the sender, so you know where the packet came from.\nHere\u0026rsquo;s how to loop and receive all incoming packets:\nwhile ( true ) { unsigned char packet_data[256]; unsigned int max_packet_size = sizeof( packet_data ); #if PLATFORM == PLATFORM_WINDOWS typedef int socklen_t; #endif sockaddr_in from; socklen_t fromLength = sizeof( from ); int bytes = recvfrom( socket, (char*)packet_data, max_packet_size, 0, (sockaddr*)\u0026amp;from, \u0026amp;fromLength ); if ( bytes \u0026lt;= 0 ) break; unsigned int from_address = ntohl( from.sin_addr.s_addr ); unsigned int from_port = ntohs( from.sin_port ); // process received packet } Any packets in the queue larger than your receive buffer will be silently discarded. So if you have a 256 byte buffer to receive packets like the code above, and somebody sends you a 300 byte packet, the 300 byte packet will be dropped. You will not receive just the first 256 bytes of the 300 byte packet.\nSince you are writing your own game network protocol, this is no problem at all in practice, just make sure your receive buffer is big enough to receive the largest packet your code could possibly send.\nDestroying a socket On most unix-like platforms, sockets are file handles so you use the standard file \u0026ldquo;close\u0026rdquo; function to clean up sockets once you are finished with them. However, Windows likes to be a little bit different, so we have to use \u0026ldquo;closesocket\u0026rdquo; instead:\n#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX close( socket ); #elif PLATFORM == PLATFORM_WINDOWS closesocket( socket ); #endif Hooray windows.\nSocket class So we\u0026rsquo;ve covered all the basic operations: creating a socket, binding it to a port, setting it to non-blocking, sending and receiving packets, and destroying the socket.\nBut you\u0026rsquo;ll notice most of these operations are slightly platform dependent, and it\u0026rsquo;s pretty annoying to have to remember to #ifdef and do platform specifics each time you want to perform socket operations.\nWe\u0026rsquo;re going to solve this by wrapping all our socket functionality up into a \u0026ldquo;Socket\u0026rdquo; class. While we\u0026rsquo;re at it, we\u0026rsquo;ll add an \u0026ldquo;Address\u0026rdquo; class to make it easier to specify internet addresses. This avoids having to manually encode or decode a \u0026ldquo;sockaddr_in\u0026rdquo; structure each time we send or receive packets.\nSo let\u0026rsquo;s add a socket class:\nclass Socket { public: Socket(); ~Socket(); bool Open( unsigned short port ); void Close(); bool IsOpen() const; bool Send( const Address \u0026amp; destination, const void * data, int size ); int Receive( Address \u0026amp; sender, void * data, int size ); private: int handle; }; and an address class:\nclass Address { public: Address(); Address( unsigned char a, unsigned char b, unsigned char c, unsigned char d, unsigned short port ); Address( unsigned int address, unsigned short port ); unsigned int GetAddress() const; unsigned char GetA() const; unsigned char GetB() const; unsigned char GetC() const; unsigned char GetD() const; unsigned short GetPort() const; private: unsigned int address; unsigned short port; }; Here\u0026rsquo;s how to to send and receive packets with these classes:\n// create socket const int port = 30000; Socket socket; if ( !socket.Open( port ) ) { printf( \u0026quot;failed to create socket!\\n\u0026quot; ); return false; } // send a packet const char data[] = \u0026quot;hello world!\u0026quot;; socket.Send( Address(127,0,0,1,port), data, sizeof( data ) ); // receive packets while ( true ) { Address sender; unsigned char buffer[256]; int bytes_read = socket.Receive( sender, buffer, sizeof( buffer ) ); if ( !bytes_read ) break; // process packet } As you can see it\u0026rsquo;s much simpler than using BSD sockets directly.\nAs an added bonus the code is the same on all platforms because everything platform specific is handled inside the socket and address classes.\nConclusion You now have a platform independent way to send and receive packets. Enjoy :)\nNEXT ARTICLE: Virtual Connection over UDP\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1222992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1222992000,"objectID":"4a4a50f33e9d4c680630bfd6aa4a6fac","permalink":"https://gafferongames.com/post/sending_and_receiving_packets/","publishdate":"2008-10-03T00:00:00Z","relpermalink":"/post/sending_and_receiving_packets/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn the previous article we discussed options for sending data between computers and decided to use UDP instead of TCP for time critical data.\nIn this article I am going to show you how to send and receive UDP packets.\nBSD sockets For most modern platforms you have some sort of basic socket layer available based on BSD sockets.","tags":["networking"],"title":"Sending and Receiving Packets","type":"post"},{"authors":null,"categories":["Game Networking"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn this article we start with the most basic aspect of network programming: sending and receiving data over the network. This is perhaps the simplest and most basic part of what network programmers do, but still it is quite intricate and non-obvious as to what the best course of action is.\nYou have most likely heard of sockets, and are probably aware that there are two main types: TCP and UDP. When writing a network game, we first need to choose what type of socket to use. Do we use TCP sockets, UDP sockets or a mixture of both? Take care because if you get this wrong it will have terrible effects on your multiplayer game!\nThe choice you make depends entirely on what sort of game you want to network. So from this point on and for the rest of this article series, I assume you want to network an action game. You know, games like Halo, Battlefield 1942, Quake, Unreal, CounterStrike and Team Fortress.\nIn light of the fact that we want to network an action game, we\u0026rsquo;ll take a very close look at the properties of each protocol, and dig a bit into how the internet actually works. Once we have all this information, the correct choice is clear.\nTCP/IP TCP stands for \u0026ldquo;transmission control protocol\u0026rdquo;. IP stands for \u0026ldquo;internet protocol\u0026rdquo;. Together they form the backbone for almost everything you do online, from web browsing to IRC to email, it\u0026rsquo;s all built on top of TCP/IP.\nIf you have ever used a TCP socket, then you know it\u0026rsquo;s a reliable connection based protocol. This means you create a connection between two machines, then you exchange data much like you\u0026rsquo;re writing to a file on one side, and reading from a file on the other.\nTCP connections are reliable and ordered. All data you send is guaranteed to arrive at the other side and in the order you wrote it. It\u0026rsquo;s also a stream protocol, so TCP automatically splits your data into packets and sends them over the network for you.\nIP The simplicity of TCP is in stark contrast to what actually goes on underneath TCP at the IP or \u0026ldquo;internet protocol\u0026rdquo; level.\nHere there is no concept of connection, packets are simply passed from one computer to the next. You can visualize this process being somewhat like a hand-written note passed from one person to the next across a crowded room, eventually, reaching the person it\u0026rsquo;s addressed to, but only after passing through many hands.\nThere is also no guarantee that this note will actually reach the person it is intended for. The sender just passes the note along and hopes for the best, never knowing whether or not the note was received, unless the other person decides to write back!\nOf course IP is in reality a little more complicated than this, since no one computer knows the exact sequence of computers to pass the packet along to so that it reaches its destination quickly. Sometimes IP passes along multiple copies of the same packet and these packets make their way to the destination via different paths, causing packets to arrive out of order and in duplicate.\nThis is because the internet is designed to be self-organizing and self-repairing, able to route around connectivity problems rather than relying on direct connections between computers. It\u0026rsquo;s actually quite cool if you think about what\u0026rsquo;s really going on at the low level. You can read all about this in the classic book TCP/IP Illustrated.\nUDP Instead of treating communications between computers like writing to files, what if we want to send and receive packets directly?\nWe can do this using UDP.\nUDP stands for \u0026ldquo;user datagram protocol\u0026rdquo; and it\u0026rsquo;s another protocol built on top of IP, but unlike TCP, instead of adding lots of features and complexity, UDP is a very thin layer over IP.\nWith UDP we can send a packet to a destination IP address (eg. 112.140.20.10) and port (say 52423), and it gets passed from computer to computer until it arrives at the destination or is lost along the way.\nOn the receiver side, we just sit there listening on a specific port (eg. 52423) and when a packet arrives from any computer (remember there are no connections!), we get notified of the address and port of the computer that sent the packet, the size of the packet, and can read the packet data.\nLike IP, UDP is an unreliable protocol. In practice however, most packets that are sent will get through, but you\u0026rsquo;ll usually have around 1-5% packet loss, and occasionally you\u0026rsquo;ll get periods where no packets get through at all (remember there are lots of computers between you and your destination where things can go wrong\u0026hellip;)\nThere is also no guarantee of ordering of packets with UDP. You could send 5 packets in order 1,2,3,4,5 and they could arrive completely out of order like 3,1,2,5,4. In practice, packets tend to arrive in order most of the time, but you cannot rely on this!\nUDP also provides a 16 bit checksum, which in theory is meant to protect you from receiving invalid or truncated data, but you can\u0026rsquo;t even trust this, since 16 bits is just not enough protection when you are sending UDP packets rapidly over a long period of time. Statistically, you can\u0026rsquo;t even rely on this checksum and must add your own.\nSo in short, when you use UDP you\u0026rsquo;re pretty much on your own!\nTCP vs. UDP We have a decision to make here, do we use TCP sockets or UDP sockets?\nLets look at the properties of each:\nTCP:\nConnection based Guaranteed reliable and ordered Automatically breaks up your data into packets for you Makes sure it doesn't send data too fast for the internet connection to handle (flow control) Easy to use, you just read and write data like its a file UDP:\nNo concept of connection, you have to code this yourself No guarantee of reliability or ordering of packets, they may arrive out of order, be duplicated, or not arrive at all! You have to manually break your data up into packets and send them You have to make sure you don't send data too fast for your internet connection to handle If a packet is lost, you need to devise some way to detect this, and resend that data if necessary You can't even rely on the UDP checksum so you must add your own The decision seems pretty clear then, TCP does everything we want and its super easy to use, while UDP is a huge pain in the ass and we have to code everything ourselves from scratch.\nSo obviously we just use TCP right?\nWrong!\nUsing TCP is the worst possible mistake you can make when developing a multiplayer game! To understand why, you need to see what TCP is actually doing above IP to make everything look so simple.\nHow TCP really works TCP and UDP are both built on top of IP, but they are radically different. UDP behaves very much like the IP protocol underneath it, while TCP abstracts everything so it looks like you are reading and writing to a file, hiding all complexities of packets and unreliability from you.\nSo how does it do this?\nFirstly, TCP is a stream protocol, so you just write bytes to a stream, and TCP makes sure that they get across to the other side. Since IP is built on packets, and TCP is built on top of IP, TCP must therefore break your stream of data up into packets. So, some internal TCP code queues up the data you send, then when enough data is pending the queue, it sends a packet to the other machine.\nThis can be a problem for multiplayer games if you are sending very small packets. What can happen here is that TCP may decide it\u0026rsquo;s not going to send data until you have buffered up enough data to make a reasonably sized packet to send over the network.\nThis is a problem because you want your client player input to get to the server as quickly as possible, if it is delayed or \u0026ldquo;clumped up\u0026rdquo; like TCP can do with small packets, the client\u0026rsquo;s user experience of the multiplayer game will be very poor. Game network updates will arrive late and infrequently, instead of on-time and frequently like we want.\nTCP has an option to fix this behavior called TCP_NODELAY. This option instructs TCP not to wait around until enough data is queued up, but to flush any data you write to it immediately. This is referred to as disabling Nagle\u0026rsquo;s algorithm.\nUnfortunately, even if you set this option TCP still has serious problems for multiplayer games and it all stems from how TCP handles lost and out of order packets to present you with the \u0026ldquo;illusion\u0026rdquo; of a reliable, ordered stream of data.\nHow TCP implements reliability Fundamentally TCP breaks down a stream of data into packets, sends these packets over unreliable IP, then takes the packets received on the other side and reconstructs the stream.\nBut what happens when a packet is lost?\nWhat happens when packets arrive out of order or are duplicated?\nWithout going too much into the details of how TCP works because its super-complicated (please refer to TCP/IP Illustrated) in essence TCP sends out a packet, waits a while until it detects that packet was lost because it didn\u0026rsquo;t receive an ack (or acknowledgement), then resends the lost packet to the other machine. Duplicate packets are discarded on the receiver side, and out of order packets are resequenced so everything is reliable and in order.\nThe problem is that if we were to send our time critical game data over TCP, whenever a packet is dropped it has to stop and wait for that data to be resent. Yes, even if more recent data arrives, that new data gets put in a queue, and you cannot access it until that lost packet has been retransmitted. How long does it take to resend the packet?\nWell, it\u0026rsquo;s going to take at least round trip latency for TCP to work out that data needs to be resent, but commonly it takes 2*RTT, and another one way trip from the sender to the receiver for the resent packet to get there. So if you have a 125ms ping, you\u0026rsquo;ll be waiting roughly 1/5th of a second for the packet data to be resent at best, and in worst case conditions you could be waiting up to half a second or more (consider what happens if the attempt to resend the packet fails to get through?). What happens if TCP decides the packet loss indicates network congestion and it backs off? Yes it actually does this. Fun times!\nNever use TCP for time critical data The problem with using TCP for realtime games like FPS is that unlike web browsers, or email or most other applications, these multiplayer games have a real time requirement on packet delivery.\nWhat this means is that for many parts of a game, for example player input and character positions, it really doesn\u0026rsquo;t matter what happened a second ago, the game only cares about the most recent data.\nTCP was simply not designed with this in mind.\nConsider a very simple example of a multiplayer game, some sort of action game like a shooter. You want to network this in a very simple way. Every frame you send the input from the client to the server (eg. keypresses, mouse input controller input), and each frame the server processes the input from each player, updates the simulation, then sends the current position of game objects back to the client for rendering.\nSo in our simple multiplayer game, whenever a packet is lost, everything has to stop and wait for that packet to be resent. On the client game objects stop receiving updates so they appear to be standing still, and on the server input stops getting through from the client, so the players cannot move or shoot. When the resent packet finally arrives, you receive this stale, out of date information that you don\u0026rsquo;t even care about! Plus, there are packets backed up in queue waiting for the resend which arrive at same time, so you have to process all of these packets in one frame. Everything is clumped up!\nUnfortunately, there is nothing you can do to fix this behavior, it\u0026rsquo;s just the fundamental nature of TCP. This is just what it takes to make the unreliable, packet-based internet look like a reliable-ordered stream.\nThing is we don\u0026rsquo;t want a reliable ordered stream.\nWe want our data to get as quickly as possible from client to server without having to wait for lost data to be resent.\nThis is why you should never use TCP when networking time-critical data!\nWait? Why can\u0026rsquo;t I use both UDP and TCP? For realtime game data like player input and state, only the most recent data is relevant, but for other types of data, say perhaps a sequence of commands sent from one machine to another, reliability and ordering can be very important.\nThe temptation then is to use UDP for player input and state, and TCP for the reliable ordered data. If you\u0026rsquo;re sharp you\u0026rsquo;ve probably even worked out that you may have multiple \u0026ldquo;streams\u0026rdquo; of reliable ordered commands, maybe one about level loading, and another about AI. Perhaps you think to yourself, \u0026ldquo;Well, I\u0026rsquo;d really not want AI commands to stall out if a packet is lost containing a level loading command - they are completely unrelated!\u0026rdquo;. You are right, so you may be tempted to create one TCP socket for each stream of commands.\nOn the surface, this seems like a great idea. The problem is that since TCP and UDP are both built on top of IP, the underlying packets sent by each protocol will affect each other. Exactly how they affect each other is quite complicated and relates to how TCP performs reliability and flow control, but fundamentally you should remember that TCP tends to induce packet loss in UDP packets. For more information, read this paper on the subject.\nAlso, it\u0026rsquo;s pretty complicated to mix UDP and TCP. If you mix UDP and TCP you lose a certain amount of control. Maybe you can implement reliability in a more efficient way that TCP does, better suited to your needs? Even if you need reliable-ordered data, it\u0026rsquo;s possible, provided that data is small relative to the available bandwidth to get that data across faster and more reliably that it would if you sent it over TCP. Plus, if you have to do NAT to enable home internet connections to talk to each other, having to do this NAT once for UDP and once for TCP (not even sure if this is possible\u0026hellip;) is kind of painful.\nConclusion My recommendation is not only that you use UDP, but that you only use UDP for your game protocol. Don\u0026rsquo;t mix TCP and UDP! Instead, learn how to implement the specific features of TCP that you need inside your own custom UDP based protocol.\nOf course, it is no problem to use HTTP to talk to some RESTful services while your game is running. I\u0026rsquo;m not saying you can\u0026rsquo;t do that. A few TCP connections running while your game is running isn\u0026rsquo;t going to bring everything down. The point is, don\u0026rsquo;t split your game protocol across UDP and TCP. Keep your game protocol running over UDP so you are fully in control of the data you send and receive and how reliability, ordering and congestion avoidance are implemented.\nThe rest of this article series show you how to do this, from creating your own virtual connection on top of UDP, to creating your own reliability, flow control and congestion avoidance.\nNEXT ARTICLE: Sending and Receiving Packets\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1222819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1222819200,"objectID":"4fcac8023048fafbf54e4b0235815060","permalink":"https://gafferongames.com/post/udp_vs_tcp/","publishdate":"2008-10-01T00:00:00Z","relpermalink":"/post/udp_vs_tcp/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.\nIn this article we start with the most basic aspect of network programming: sending and receiving data over the network. This is perhaps the simplest and most basic part of what network programmers do, but still it is quite intricate and non-obvious as to what the best course of action is.\nYou have most likely heard of sockets, and are probably aware that there are two main types: TCP and UDP.","tags":["networking"],"title":"UDP vs. TCP","type":"post"},{"authors":null,"categories":["Game Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to use spring-like forces to model basic collision response, joints and motors.\nIn this article we\u0026rsquo;re going to discuss how to network a physics simulation.\nFirst Person Shooters First person shooter physics are usually very simple. The world is static and players are limited to running around and jumping and shooting.\nBecause of cheating, first person shooters typically operate on a client-server model where the server is authoritative over physics. This means that the true physics simulation runs on the server and the clients display an approximation of the server physics to the player.\nThe problem then is how to allow each client to control his own character while displaying a reasonable approximation of the motion of the other players.\nIn order to do this elegantly and simply, we structure the physics simulation as follows:\nCharacter physics are completely driven from input data.\nPhysics state is fully encapsulated in a state structure.\nTo do this we need to gather all the user input that drives the physics simulation into a single structure and the state representing each player character into another.\nHere is an example from a simple run and jump shooter:\nstruct Input { bool left; bool right; bool forward; bool back; bool jump; }; struct State { Vector position; Vector velocity; }; Next we need to make sure that the simulation gives the same result given the same initial state and inputs over time. Or at least, that the results are as close as possible. I\u0026rsquo;m not talking about perfect floating point determinism here, just a reasonable 1/2 second prediction giving approximately the same result.\nNetwork Fundamentals I will briefly discuss actually networking issues in this section before moving on to the important information of what to send over the pipe. It is after all just a pipe after all, networking is nothing special right? Beware! Ignorance of how the pipe works will really bite you. Here are the two networking fundamentals that you absolutely need to know:\nNumber one. If your network programmer is any good at all he will use UDP, which is an unreliable data protocol, and build some sort of application specific networking layer on top of this. The important thing that you as the physics programmer need to know is that you absolutely must design your physics communication over the network so that you can receive the most recent input and state without waiting for lost packets to be resent. This is important because otherwise your physics simulation will stall out under bad networking conditions.\nTwo. You will be very limited in what can be sent across the network due to bandwidth limitations. Compression is a fact of life when sending data across the network. As physics programmer you need to be very careful what data is compressed and how it is done. For the sake of determinism, some data must not be compressed, while other data is safe. Any data that is compressed in a lossy fashion should have the same quantization applied locally where possible, so that the result is the same on both machines. Bottom line you\u0026rsquo;ll need to be involved in this compression in order to make it as efficient as possible without breaking your simulation.\nPhysics Runs On The Server The fundamental primitive we will use when sending data between the client and the server is an unreliable data block, or if you prefer, an unreliable non-blocking remote procedure call (rpc). Non-blocking means that the client sends the rpc to the server then continues immediately executing other code, it does not wait for the rpc to execute on the server! Unreliable means that if you call the rpc is continuously on the the server from a client, some of these calls will not reach the server, and others will arrive in a different order than they were called. We design our communications around this primitive because it suits the transport layer (UDP).\nThe communication between the client and the server is then structured as what I call a \u0026ldquo;stream of input\u0026rdquo; sent via repeated rpc calls. The key to making this input stream tolerant of packet loss and out of order delivery is the inclusion of a floating point time in seconds value with every input rpc sent. The server keeps track of the current time on the server and ignores any input received with a time value less than the current time. This effectively drops any input that is received out of order. Lost packets are ignored.\nThinking in terms of our standard first person shooter, the input we send from client to server is the input structure that we defined earlier:\nstruct Input { bool left; bool right; bool forward; bool back; bool jump; }; class Character { public: void processInput( double time, Input input ); }; Thats the bare minimum data required for sending a simple ground based movement plus jumping across the network. If you are going to allow your clients to shoot you\u0026rsquo;ll need to add mouse input as part of the input structure as well because weapon firing needs to be done server side.\nNotice how I define the rpc as a method inside an object? I assume your network programmer has a channel structure built on top of UDP, eg. some way to indicate that a certain rpc call is directed as a specific object instance on the remote machine.\nSo how does the server process these rpc calls? It basically sits in a loop waiting for input from each of the clients. Each character object has its physics advanced ahead in time individually as input rpcs are received from the client that owns it. This means that the physics state of different client characters are slightly out of phase on the server, some clients being a little bit ahead and others a little bit behind in time. Overall however, the different client characters advance ahead roughly in sync with each other.\nLets see how this rpc call is implemented in code on the server:\nvoid processInput( double time, Input input ) { if ( time \u0026lt; currentTime ) return; float deltaTime = currentTime - time; updatePhysics( currentTime, deltaTime, input ); } The key to the code above is that by advancing the server physics simulation for the client character is performed only as we receive input from that client. This makes sure that the simulation is tolerant of random delays and jitter when sending the input rpc across the network.\nClients Approximate Physics Locally Now for the communication from the server back to the clients. This is where the bulk of the server bandwidth kicks in because the information needs to be broadcast to all the clients.\nWhat happens now is that after every physics update on the server that occurs in response to an input rpc from a client, the server broadcasts out the physics state at the end of that physics update and the current input just received from the rpc.\nThis is sent to all clients in the form of an unreliable rpc:\nvoid clientUpdate( float time, Input input, State state ) { Vector difference = state.position - current.position; float distance = difference.length(); if ( distance \u0026gt; 2.0f ) current.position = state.position; else if ( distance \u0026gt; 0.1 ) current.position += difference * 0.1f; current.velocity = velocity; current.input = input; } What is being done here is this: if the two positions are significantly different (\u0026gt;2m apart) just snap to the corrected position, otherwise if the distance between the server position and the current position on the client is more than 10cms, move 10% of the distance between the current position and the correct position. Otherwise do nothing.\nSince server update rpcs are being broadcast continually from the server to the the clients, moving only a fraction towards the snap position has the effect of smoothing the correction out with what is called an exponentially smoothed moving average.\nThis trades a bit of extra latency for smoothness because only moving some percent towards the snapped position means that the position will be a bit behind where it should really be. You don\u0026rsquo;t get anything for free. I recommend that you perform this smoothing for immediate quantities such as position and orientation, while directly snapping derivative quantities such as velocity, angular velocity because the effect of abruptly changing derivative quantities is not as noticeable.\nOf course, these are just rules of thumb. Make sure you experiment to find out what works best for your simulation.\nClient-Side Prediction So far we have a developed a solution for driving the physics on the server from client input, then broadcasting the physics to each of the clients so they can maintain a local approximation of the physics on the server. This works perfectly however it has one major disadvantage. Latency!\nWhen the user holds down the forward input it is only when that input makes a round trip to the server and back to the client that the client\u0026rsquo;s character starts moving forward locally. Those who remember the original Quake netcode would be familiar with this effect. The solution to this problem was discovered and first applied in the followup QuakeWorld and is called client side prediction. This technique completely eliminates movement lag for the client and has since become a standard technique used in first person shooter netcode.\nClient side prediction works by predicting physics ahead locally using the player\u0026rsquo;s input, simulating ahead without waiting for the server round trip. The server periodically sends corrections to the client which are required to ensure that the client stays in sync with the server physics. At all times the server is authoritative over the physics of the character so even if the client attempts to cheat all they are doing is fooling themselves locally while the server physics remains unaffected. Seeing as all game logic runs on the server according to server physics state, client side movement cheating is basically eliminated.\nThe most complicated part of client side prediction is handling the correction from the server. This is difficult, because the corrections from the server arrive in the past due to client/server communication latency. We need to apply this correction in the past, then calculate the resulting corrected position at present time on the client.\nThe standard technique to do this is to store a circular buffer of saved moves on the client where each move in the buffer corresponds to an input rpc call sent from the client to the server:\nstruct Move { double time; Input input; State state; }; When the client receives a correction it looks through the saved move buffer to compare its physics state at that time with the corrected physics state sent from the server. If the two physics states differ above some threshold then the client rewinds to the corrected physics state and time and replays the stored moves starting from the corrected state in the past, the result of this re-simulation being the corrected physics state at the current time on the client.\nSometimes packet loss or out of order delivery occurs and the server input differs from that stored on the client. In this case the server snaps the client to the correct position automatically via rewind and replay. This snapping is quite noticeable to the player, so we reduce it with the same smoothing technique we used above for the other player characters. This smoothing is done after recalculating the corrected position via rewind and replay.\nConclusion We can easily apply the client side prediction techniques used in first person shooters to network a physics simulation, but only if there is a clear ownership of objects by clients and these object interact mostly with a static world.\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1094256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1094256000,"objectID":"06a3f4e766557bad0f772925c6be4f51","permalink":"https://gafferongames.com/post/networked_physics_2004/","publishdate":"2004-09-04T00:00:00Z","relpermalink":"/post/networked_physics_2004/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to use spring-like forces to model basic collision response, joints and motors.\nIn this article we\u0026rsquo;re going to discuss how to network a physics simulation.\nFirst Person Shooters First person shooter physics are usually very simple. The world is static and players are limited to running around and jumping and shooting.\nBecause of cheating, first person shooters typically operate on a client-server model where the server is authoritative over physics.","tags":["physics","networking"],"title":"Networked Physics (2004)","type":"post"},{"authors":null,"categories":["Game Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to simulate the motion of rigid bodies in 3D. Now we\u0026rsquo;re going to discuss how to implement spring physics.\nThe physics behind springs is simple but extremely versatile and useful. You can use springs to link points together to model rope and string, cloth, and even blobs of jelly. Springs can also be used to implement basic collision response, and to create joints that constrain the motion of rigid bodies.\nThe more physics programming you do, the more springs pop up. Many physical phenomenon boil down to spring-like forces being applied such as buoyancy in water. Springs are everywhere so lets discover how to simulate them!\nSpring and Dampers The formula to use for simulating spring-like behavior is called Hooke\u0026rsquo;s Law.\nF = -kx Where x is the vector displacement of the end of the spring from it\u0026rsquo;s equilibrium position, and k is a constant describing the tightness of the spring. Larger values of k mean that the spring is tighter and will therefore stretch less per unit of force, smaller values mean the spring is looser and will stretch further.\nNewton\u0026rsquo;s third law says that every force has an equal and opposite force. If two objects a and b are connected together by a spring then the spring will apply one force which pulls object a towards object b, and an equal and opposite force pulling object b towards a. However, if you want to attach one object to a fixed point in space its you can apply the force of the object in one direction. This makes sense if you consider that point as having infinite mass.\nSpring forces alone are not much use though. You need to combine them with dampers to have a realistic simulation. Damping simulates energy loss, and it is used in physics simulations to make sure that springs don\u0026rsquo;t oscillate forever but come to rest over time.\nA spring-damper system can be modeled as follows:\nF = - kx - bv Where b is the coefficient of damping and v is the relative velocity between the two points connected by the spring. Larger values for b increase the amount of damping so the object comes to rest more quickly.\nVariations on Springs There are many different variations on spring-damper systems, but what I want to do is explain how to think generally in terms of what the forces are doing so that you can easily design spring and damper systems to achieve whatever effect you want.\nThe first thing is that springs don\u0026rsquo;t only have to act to pull two points together so that they lie on top of each other. For example you can design spring forces that pull together or push apart two points apart so they maintain a desired separation distance from each other:\nF = -k(|x|-d)(x/|x|) - bv Where |x| is the distance between the two points connected to the spring, d is the desired distance of separation, and x / |x| is the unit length direction vector between the two points: a to b, when applying the force to point a and vice versa.\nThe overall effect of the force equation above is to have a force which pushes the two points connected by the spring apart if they are closer than distance d, and bring the two points together if they are further than d apart. Notice how the force becomes exactly zero when the two points are at the target distance? If you tune the k and b parameters correctly you can have a nicely behaving spring that quickly brings the two points together smoothly over time and comes to rest at the solution point.\nBut why apply springs to position only? If you want to accelerate a body over time such that it accelerates to a certain speed then you can calculate a spring force proportional to the difference between the current velocity and the target velocity, combined with a damping proportional to the current velocity so that it reaches its target over time instead of cycling about it. This is usually called a motor in physics simulation.\nWe can even apply the same concept to drive the spinning of an object at a certain speed by applying a spring torque proportional to the difference between the current angular velocity and the desired angular velocity, coupled with a damper force proportional to the current angular velocity.\nAnother commonly implemented spring constraint is to enforce a upright orientation of a body, for example, you could apply a spring torque proportional to the difference between the current orientation and an upright orientation, coupled with a damper proportional to angular velocity. Such a constraint is called a \u0026lsquo;stay upright constraint\u0026rsquo; and its often used for sci-fi hover racing games.\nAs you can see, the applications of springs are limitless. The overall pattern is to design spring forces that will attract your physics simulation into the desired state and fade off to zero when this state is has been reached. In order to make sure that your physics objects don\u0026rsquo;t just oscillate around the solution but actually reach it, it is necessary to apply damping proportional to whatever physics state values are performing the evolution of the simulation towards the solution over time.\nAttachment using a Spring So lets get started with an actual concrete implementation of using springs a simulation. The first thing we will implement is an attachment joint that will allow the user to click and drag a point on the cube to move it around. We will implement this by modeling a tight spring attachment between a target point and an attachment point on the body of the cube. This is effectively a ball and socket joint implemented using only spring forces and is implemented using the standard equation we are used to:\nF = -kx -bv Where x is the vector difference between the current target point and the attachment point on the object, and v is the point velocity at the attachment point on the object. The important thing is that this velocity v being the point velocity means that it incorporates both the linear motion of the object plus any velocity at the attachment point due to angular velocity. As was shown in the previous article we can calculate this point velocity at follows:\nvpoint = vlinear + vangular cross (p - x) Where p is the point on the rigid body and x is the center of mass of the object. Secondly, this spring and damper force is not just applied linearly, but is applied at the attachment point on the object. This means that the spring force will apply both a linear force and a torque component as follows:\nFlinear = F Ftorque = F cross (p - x) The overall effect of this joint then is to bring the target and attachment points together while damping the motion of the object at the attachment point. This allows the object to move as long as remains still at the attachment. In other words the object is only allowed to move by rotating about the attachment point. Our simple ball and socket joint is now complete.\nCollision Response with Springs Next we will implement a basic collision response using springs. The trick here is to apply a spring and damper force that works against what we don\u0026rsquo;t want, eg. objects moving towards each other and penetrating other objects.\nSo we have a collision detected and the usual information is returned to the physics system ready to apply collision response. This information is typically something like:\nA unit length collision normal The penetration depth along the normal The physics state for each of the colliding objects at the time of collision All this information grouped together is called a contact. Processing collision geometry and returning a series of generic contacts is a great idea because it decouples your collision response from the gritty details of collision detection.\nOnce we have all our contact information a simple collision response can be implemented by applying a spring-like force to the colliding objects to keep them apart:\nF = nkd - bn(n.v) Where k and b are the spring-damper coefficients, n is the contact normal and v is the relative velocity between the two objects at the point of collision. Effectively this equation calculates a spring force that pushes out along the contact normal while reducing the relative velocity of the objects towards each other at the contact point.\nVarious different collision responses types can also be achieved using this equation, for example setting b to 0 gives a completely elastic collision response where all energy going into the collision returns in the bounce over time. Setting b to higher value tend to make the collision more inelastic because it removes more energy during the collision. Finally, by increasing and decreasing the spring constant k in concert with b you can make a collision that feels like anything from bouncing off a trampoline (low k and low b), to landing and sinking into quicksand (low k and high b), or landing with a splat on concrete (high k, high b).\nThe Weakness of Springs It seems we can achieve a large variety of different collision effects using only springs and easily make joints and constraints. Its not all good news however because springs come with their own set of weaknesses which I will now explain.\nThe first weakness is that its difficult to tune the spring constants to get exactly the effect you want. For example, if you are attempting to model the real world physics of an object, you will need to experiment to find the spring k and b values that match the simulation. These values are usually dependent on other values of your simulation, such as the gravity constant, meaning that if you adjust gravity you\u0026rsquo;ll need to retune your springs to get the same effect.\nThe next problem is that tighter the spring k you use, the more difficult it becomes to solve the differential equation. Using an RK4 integrator sure help with this, but even with RK4 there is a fundamental limit to how large you can make your spring k before your simulation will explode. At this point you need to either decrease your timestep or reduce your spring k.\nThe final, and major weakness is that springs are reactive not predictive. This is a subtle point but a very important one. A joint or constraint implemented using springs only works by correcting errors after they occur, and collision response using springs requires allowing some amount of penetration before it acts to correct it and so forth. More advanced techniques exist which can solve for the forces required to constrain the physics simulation without inducing error, such as LCP solvers or iterative methods, but they are out of scope of this simple article.\nNEXT ARTICLE: Networked Physics (2004)\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1094169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1094169600,"objectID":"4bf3cfa71409f8fc13ffe1163aae23bf","permalink":"https://gafferongames.com/post/spring_physics/","publishdate":"2004-09-03T00:00:00Z","relpermalink":"/post/spring_physics/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to simulate the motion of rigid bodies in 3D. Now we\u0026rsquo;re going to discuss how to implement spring physics.\nThe physics behind springs is simple but extremely versatile and useful. You can use springs to link points together to model rope and string, cloth, and even blobs of jelly. Springs can also be used to implement basic collision response, and to create joints that constrain the motion of rigid bodies.","tags":["physics"],"title":"Spring Physics","type":"post"},{"authors":null,"categories":["Game Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to integrate our physics simulation forward at fixed delta time increments, regardless of display framerate.\nIn this article we are going to simulate motion in three dimensions.\nRigid Bodies We will concentrate on a type of object called a rigid body. Rigid bodies cannot bend, compress or deform in any way. This makes their motion much easier to calculate.\nTo simulate the motion of rigid bodies, we must study both rigid body kinematics and rigid body dynamics. Kinematics is the study of how an object moves in the absence of forces, while dynamics describes how an object reacts to them. Together they provide all the information you need to simulate the motion of a rigid body in three dimensions.\nAlong the way I will show you how to integrate vector quantities, handle rotations in three dimensions and integrate to find the motion of your object as it moves and spins around the world.\nMoving in the Third Dimension As long as we only have single floating point values for position and velocity our physics simulation is limited to motion in a single dimension, and a point moving from side to side on the screen is pretty boring!\nWe want our object to be able to move in three dimensions: left and right, forward and back, up and down. If we apply the equations of motion to each dimension separately, we can integrate each dimension in turn to find the motion of the object in three dimensions.\nOr\u0026hellip; we could just use vectors.\nVectors are a mathematical type representing an array of numbers. A three dimensional vector has three components x, y and z. Each component corresponds to a dimension. In this article x is left and right, y is up and down, and z is forward and back.\nIn C++ we implement vectors using a struct as follows:\nstruct Vector { float x,y,z; }; Addition of two vectors is defined as adding each component together. Multiplying a vector by a floating point number is the same as just multiplying each component. Lets add overloaded operators to the vector struct so that we can perform these operations in code as if vectors are a native type:\nstruct Vector { float x,y,z; Vector operator + ( const Vector \u0026amp;other ) { Vector result; result.x = x + other.x; result.y = y + other.y; result.z = z + other.z; return result; } Vector operator*( float scalar ) { Vector result; result.x = x * scalar; result.y = y * scalar; result.z = z * scalar; return result; } }; Now instead of maintaining completely seperate equations of motion and integrating seperately for x, y and z, we convert our position, velocity, acceleration and force to vector quantities, then integrate the vectors directly using the equations of motion from the first article:\nF = ma dv/dt = a dx/dt = v Notice how F, a, v and x are written in bold. This is the convention used to distinguish vector quantities from single value (scalar) quantities such as mass m and time t.\nNow that we have the equations of motion in vector form, how do we integrate them? The answer is exactly the same as we integrated single values. This is because we have already added overloaded operators for adding two vectors together, and multiplying a vector by a scalar, and this is all we need to be able to drop in vectors in place of floats and have everything just work.\nFor example, here is a simple Euler integration for vector position from velocity:\nposition = position + velocity * dt; Notice how the overloaded operators make it look exactly the same as an Euler integration for a single value. But what is it really doing? Lets take a look at how we would implement vector integration without the overloaded operators:\nposition.x = position.x + velocity.x * dt; position.y = position.y + velocity.y * dt; position.z = position.z + velocity.z * dt; As you can see, its exactly the same as if we integrated each component of the vector separately! This is the cool thing about vectors. Whether we integrate vectors directly, or integrate each component separately, we are doing exactly the same thing.\nStructuring for RK4 In the example programs from previous articles we drove the simulation from acceleration assuming unit mass. This kept the code nice and simple, but from now on every object will have its own mass in kilograms so the simulation needs be driven by forces instead.\nThere are two ways we can do this. First, we can divide force by mass to get acceleration, then integrate this acceleration to get the velocity, and integrate velocity to get position.\nThe second way is to integrate force directly to get momentum, then convert this momentum to velocity by dividing it by mass, then finally integrate velocity to get position. Remember that momentum is just velocity multiplied by mass:\ndp/dt = F v = p/m dx/dt = v Both methods work, but the second way is more consistent with the way that we must approach rotation later in the article, so we\u0026rsquo;ll use that.\nWhen we switch to momentum we need to make sure that the velocity is recalculated after each integration by dividing momentum by mass. Doing this manually everywhere that momentum is changed would be error prone, so we now separate all our state quantities into primary, secondary and constant values, and add a method called \u0026lsquo;recalculate\u0026rsquo; to the State struct which is responsible for updating all the secondary values from the primary ones:\nstruct State { // primary Vector position; Vector momentum; // secondary Vector velocity; // constant float mass; float inverseMass; void recalculate() { velocity = momentum * inverseMass; } }; struct Derivative { Vector velocity; Vector force; }; If we make sure that recalculate is called whenever any of the primary values change, then our secondary values will always stay in sync. This may seem like overkill just to handle converting momentum to velocity, but as our simulation becomes more complex we will have many more secondary values, so it is important to design a system that handles this.\nSpinning Around So far we have covered linear motion, we can simulate an rigid body so that it moves in 3D space, but it cannot rotate yet.\nThe good news is that rotational equivalents to force, momentum, velocity, position and mass exist, and once we understand how they work, integration of rotational physics state can be performed using our RK4 integrator.\nLet\u0026rsquo;s start off by talking about how rigid bodies rotate. Because our objects are rigid they cannot deform. This means that we can treat the linear and rotational parts of an object\u0026rsquo;s motion as being entirely separate: a linear component (position, velocity, momentum, mass) and a rotational component rotating about the center of mass.\nHow do we represent how the object is rotating? If you think about it a bit, you\u0026rsquo;ll realize that for a rigid body rotation can only ever be around a single axis, so the first thing we need to know is what that axis is. We can represent this axis with a unit length vector. Next we need to know how fast the object is rotating about this axis in radians per second.\nIf we know the center of mass of the object, the axis of rotation, and the speed of rotation then we have the all the information we need to describe how it is rotating.\nThe standard way of representing rotation over time is by combining the axis and the speed of rotation into a single vector called angular velocity. The length of the angular velocity vector is the speed of rotation in radians while the direction of the vector indicates the axis of rotation. For example, an angular velocity of (2Pi,0,0) indicates a rotation about the x axis doing one revolution per second.\nBut what direction is this rotation in? In the example source code I use a right handed coordinate system which is standard when using OpenGL. To find the direction of rotation just take your right hand and point your thumb down the axis, your fingers curl in the direction of rotation. If your 3D engine uses a left handed coordinate system then just use your left hand instead.\nWhy do we combine the axis and rate of rotation into a single vector? Doing so gives us a single vector quantity that is easy to manipulate just like velocity for linear motion. We can easily add and subtract changes to angular velocity to change how the object is rotating just like we can add and subtract from linear velocity. If we stuck with a unit length vector and scalar for rotation speed then it would be much more complicated to apply these changes.\nBut there is one very important difference between linear and angular velocity. Unlike linear velocity, there is no guarantee that angular velocity will remain constant over time in the absence of forces. In other words, angular momentum is conserved while angular velocity is not. This means that we cannot trust angular velocity as a primary value and we need to use angular momentum instead.\nAngular Momentum, Inertia and Torque Just as velocity and momentum are related by mass in linear motion, angular velocity and angular momentum are related by a quantity called the rotational inertia. This tensor is a measurement of how much effort it takes to spin an object around an axis. It depends on both the shape of the object and how much it weighs.\nIn the general case, rotational inertia is represented by a 3x3 matrix called an inertia tensor. Here we make a simplifying assumption by discussing physics in the context of simulating a cube. Because of the symmetries of the cube, we only need a single value for the rotational inertia: 1/6 x size^2 x mass, where size is the length of the sides of the cube.\nJust as we integrate linear momentum from force, we integrate angular momentum directly from the rotational equivalent of force called torque. You can think of torque just like a force, except that when it is applied it induces a rotation around an axis in the direction of torque vector rather than accelerating the object linearly. For example, a torque of (1,0,0) would cause a stationary object to start rotating about the x axis.\nOnce we have angular momentum integrated, we multiply it by the inverse of the rotational inertia to get the angular velocity, and using this angular velocity we integrate to get the rotational equivalent of position called orientation.\nHowever, as we will see, integrating orientation from angular velocity is a bit more complicated!\nOrientation in 3D This complexity is due to the difficulty of representing orientations in three dimensions.\nIn two dimensions orientations are easy, you just keep track of an angle in radians and you are done. In three dimensions it becomes much more complex. It turns out that you must either use 3x3 rotation matrices or quaternions to correctly represent the orientation of an object.\nFor reasons of simplicity and efficiency I\u0026rsquo;m going to use quaternions to represent the orientation instead of matrices. This also gives us an easy way to interpolate between the previous and current physics orientation to get smooth framerate independent animation as per the time stepping scheme outlined in the previous article.\nNow there are plenty of resources on the internet which explain what quaternions are and how unit length quaternions are used to represent rotations in three dimensions. Here is a particularly nice one. What you need to know however is that, effectively, unit quaternions represent an axis of rotation and an amount of rotation about that axis. This may seem similar to our angular velocity, but quaternions are four dimensional vectors instead of three, so mathematically they are actually quite different!\nWe will represent quaternions in code as another struct:\nstruct Quaternion { float w,x,y,z; }; If we define the rotation of a quaternion as being relative to an initial orientation of the object (what we will later call body coordinates) then we can use this quaternion to represent the orientation of the object at any point in time. Now that we have decided on the representation to use for orientation, we need to integrate it over time so that the object rotates according to the angular velocity.\nIntegrating Orientation We are now presented with a problem. Orientation is a quaternion but angular velocity is a vector. How can we integrate orientation from angular velocity when the two quantities are in different mathematical forms?\nThe solution is to convert angular velocity into a quaternion form, then to use this quaternion to integrate orientation. For lack of a better term I will call this time derivative of orientation \u0026ldquo;spin\u0026rdquo;. Exactly how to calculate this spin quaternion is described in detail here.\nHere is the final result:\nd*q*/dt = spin = 0.5 *w* *q* Where q is the current orientation quaternion, and w is the current angular velocity in quaternion form (0,x,y,z) such that x, y, z are the components of the angular velocity vector. Note that the multiplication done between w and q is quaternion multiplication.\nTo implement this in code we add spin as a new secondary quantity calculated from angular velocity in the recalculate method. We also add spin to the derivatives struct as it is the derivative of orientation:\nstruct State { // primary Quaternion orientation; Vector angularMomentum; // secondary Quaternion spin; Vector angularVelocity; // constant float inertia; float inverseInertia; void recalculate() { angularVelocity = angularMomentum * inverseInertia; orientation.normalize(); Quaternion q( 0, angularVelocity.x, angularVelocity.y, angularVelocity.z ) spin = 0.5f * q * orientation; } }; struct Derivatives { Quaternion spin; Vector torque; }; Integrating a quaternion, just like integrating a vector, is as simple as doing the integration for each value separately. The only difference is that after integrating orientation we must renormalize the orientation quaternion to make it unit length, to ensure that it still represents a rotation.\nThis is required because errors in integration accumulate over time and make the quaternion \u0026lsquo;drift\u0026rsquo; away from being unit length. I like to renormalize in the recalculate method for simplicity, but you can get away with doing it less frequently if cpu cycles are tight.\nNow in order to drive the rotation of the object, we need a method that can calculate the torque applied given the current rotational state and time just like the force method we use when integrating linear motion. eg:\nVector torque( const State \u0026amp; state, double t ) { return Vector(1,0,0) - state.angularVelocity * 0.1f; } This function returns an acceleration torque to induce a spin around the x axis, but also applies a damping over time so that at a certain speed the accelerating and damping will cancel each other out. This is done so that the rotation will reach a certain rate and stay constant instead of getting faster and faster over time.\nCombining Linear and Angular Motion Now that we are able to integrate linear and rotational effects, how can they be combined into one simulation? The answer is to just integrate the linear and rotational physics state separately and everything works out. This is because the objects we are simulating are rigid so we can decompose their motion into separate linear and rotational components. As far as integration is concerned, you can treat linear and angular effects as being completely independent of each other.\nNow that we have an object that is translating and rotating through three dimensional space, we need a way to keep track of where it is. We must now introduce the concepts of body coordinates and world coordinates.\nThink of body coordinates in terms of the object in a convenient layout, for example its center of mass would be at the origin (0,0,0) and it would be oriented in the simplest way possible. In the case of the simulation that accompanies this article, in body space the cube is oriented so that it lines up with the x, y and z axes and the center of the cube is at the origin.\nThe important thing to understand is that the object remains stationary in body space, and is transformed into world space using a combination of translation and rotation operations which put it in the correct position and orientation for rendering. When you see the cube animating on screen it is because it is being drawn in world space using the body to world transformation.\nWe have the raw materials to implement this transform from body coordinates into world coordinates in the position vector and the orientation quaternion. The trick to combining the two is to convert each of them into 4x4 matrix form which is capable of representing both rotation and translation. Then we combine the two transformations into a single matrix by multiplication. This combined matrix has the effect of first rotating the cube around the origin to get the correct orientation, then translating the cube to the correct position in world space. See this article for details on how this is done.\nIf we then invert this matrix we get one that has the opposite effect, it transforms points in world coordinates into the body coordinates of the object. Once we have both these matrices we have the ability to convert points from body to world coordinates and back again which is very handy. These two matrices become new secondary values calculated in the \u0026lsquo;recalculate\u0026rsquo; method from the orientation quaternion and position vector.\nForces and Torques We can apply separate forces and torques to an object individually, but we know from real life that if we push an object it usually makes it both move and rotate. So how can we break down a force applied at a point on the object into a linear force which causes a change in momentum, and a torque which changes angular momentum?\nGiven that our object is a rigid body, what actually happens here is that the entire force applied at the point is applied linearly, plus a torque is also generated based on the cross product of the force vector and the point on the object relative to the center of mass of the object:\nFlinear = F Ftorque = F x (p - x) Where F is the force being applied at point p in world coordinates, and x is the center of mass of the object.\nThis seems counterintuitive at first. Why is the force being applied twice? Once to linear and once to rotational motion?\nWhat is happening here is our everyday experience with objects clouding the true behavior of an object under ideal conditions.\nRemember your pushbike when you were a kid? You would have to change your tire and flip the bike upside down. You could spin the tire around by pushing on it. You don\u0026rsquo;t see any linear motion here, just rotation, so what is going on? The answer of course is that the axle of the wheel is counteracting the linear component of the force you applied, leaving only the rotational component. Not convinced? Imagine what would happen if you tried to ride your bike without an axle in your wheel\u0026hellip;\nAnother example: consider a bowling ball lying on a slippery surface such as ice so that no significant friction is present. Now in your mind try to work out a way that you can apply a force at a single point on the surface of the bowling ball such that it will stay completely still while rotating on the spot. There is no way you can do this! Any point where you push would also make the bowling ball move linearly as well as rotate. To apply a pure rotation you\u0026rsquo;d have to push on both sides of the ball, canceling the linear component of your force out leaving only torque.\nSo remember, whenever you apply a force to an object there will always be a linear force component which causes the object to accelerate linearly, as well as, depending on the direction of the force, a rotational component that causes the object to rotate.\nVelocity at a Point The final piece of the puzzle is how to calculate the velocity of a single point in the rigid body. To do this we start with the linear velocity of the object, because all points must move with this velocity to keep it rigid, then add the velocity at the point due to rotation.\nThis velocity due to rotation will not be constant for every point in the body if it is rotating, as each point in the body must be spinning around the axis of rotation. Combining the linear and angular velocities, the total velocity of a point in the rigid body is:\nvpoint = vlinear + vangular cross (p - x) Where p is the point on the rigid body and x is the center of mass of the object.\nConclusion We have covered the techniques required to simulate linear and rotational movement of a rigid body in three dimensions. By combining the linear and rotational physics into a single physics state and integrating, we can simulate the motion of a rigid body in three dimensions.\nNEXT ARTICLE: Spring Physics\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1094083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1094083200,"objectID":"fb7d9a28fea4115352568a2d975b8a57","permalink":"https://gafferongames.com/post/physics_in_3d/","publishdate":"2004-09-02T00:00:00Z","relpermalink":"/post/physics_in_3d/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to integrate our physics simulation forward at fixed delta time increments, regardless of display framerate.\nIn this article we are going to simulate motion in three dimensions.\nRigid Bodies We will concentrate on a type of object called a rigid body. Rigid bodies cannot bend, compress or deform in any way. This makes their motion much easier to calculate.","tags":["physics"],"title":"Physics in 3D","type":"post"},{"authors":null,"categories":["Game Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to integrate the equations of motion using a numerical integrator. Integration sounds complicated, but it\u0026rsquo;s just a way to advance the your physics simulation forward by some small amount of time called \u0026ldquo;delta time\u0026rdquo; (or dt for short).\nBut how to choose this delta time value? This may seem like a trivial subject but in fact there are many different ways to do it, each with their own strengths and weaknesses - so read on!\nFixed delta time The simplest way to step forward is with fixed delta time, like 1/60th of a second:\ndouble t = 0.0; double dt = 1.0 / 60.0; while ( !quit ) { integrate( state, t, dt ); render( state ); t += dt; } In many ways this code is ideal. If you\u0026rsquo;re lucky enough to have your delta time match the display refresh rate, and you can ensure that your update loop takes less than one frame worth of real time, then you already have the perfect solution for updating your physics simulation and you can stop reading this article.\nBut in the real world you may not know the display refresh rate ahead of time. VSYNC could be turned off, or you could be running on a slow computer which cannot update and render your frame fast enough to present it at 60fps.\nIn these cases your simulation will run faster or slower than you intended.\nVariable delta time Fixing this seems simple. Just measure how long the previous frame takes, then feed that value back in as the delta time for the next frame. This makes sense because of course, because if the computer is too slow to update at 60HZ and has to drop down to 30fps, you\u0026rsquo;ll automatically pass in 1/30 as delta time. Same thing for a display refresh rate of 75HZ instead of 60HZ or even the case where VSYNC is turned off on a fast computer:\ndouble t = 0.0; double currentTime = hires_time_in_seconds(); while ( !quit ) { double newTime = hires_time_in_seconds(); double frameTime = newTime - currentTime; currentTime = newTime; integrate( state, t, frameTime ); t += frameTime; render( state ); } But there is a huge problem with this approach which I will now explain. The problem is that the behavior of your physics simulation depends on the delta time you pass in. The effect could be subtle as your game having a slightly different \u0026ldquo;feel\u0026rdquo; depending on framerate or it could be as extreme as your spring simulation exploding to infinity, fast moving objects tunneling through walls and players falling through the floor!\nOne thing is for certain though and that is that it\u0026rsquo;s utterly unrealistic to expect your simulation to correctly handle any delta time passed into it. To understand why, consider what would happen if you passed in 1/10th of a second as delta time? How about one second? 10 seconds? 100? Eventually you\u0026rsquo;ll find a breaking point.\nSemi-fixed timestep It\u0026rsquo;s much more realistic to say that your simulation is well behaved only if delta time is less than or equal to some maximum value. This is usually significantly easier in practice than attempting to make your simulation bulletproof at a wide range of delta time values.\nWith this knowledge at hand, here\u0026rsquo;s a simple trick to ensure that you never pass in a delta time greater than the maximum value, while still running at the correct speed on different machines:\ndouble t = 0.0; double dt = 1 / 60.0; double currentTime = hires_time_in_seconds(); while ( !quit ) { double newTime = hires_time_in_seconds(); double frameTime = newTime - currentTime; currentTime = newTime; while ( frameTime \u0026gt; 0.0 ) { float deltaTime = min( frameTime, dt ); integrate( state, t, deltaTime ); frameTime -= deltaTime; t += deltaTime; } render( state ); } The benefit of this approach is that we now have an upper bound on delta time. It\u0026rsquo;s never larger than this value because if it is we subdivide the timestep. The disadvantage is that we\u0026rsquo;re now taking multiple steps per-display update including one additional step to consume any the remainder of frame time not divisible by dt. This is no problem if you are render bound, but if your simulation is the most expensive part of your frame you could run into the so called \u0026ldquo;spiral of death\u0026rdquo;.\nWhat is the spiral of death? It\u0026rsquo;s what happens when your physics simulation can\u0026rsquo;t keep up with the steps it\u0026rsquo;s asked to take. For example, if your simulation is told: \u0026ldquo;OK, please simulate X seconds worth of physics\u0026rdquo; and if it takes Y seconds of real time to do so where Y \u0026gt; X, then it doesn\u0026rsquo;t take Einstein to realize that over time your simulation falls behind. It\u0026rsquo;s called the spiral of death because being behind causes your update to simulate more steps to catch up, which causes you to fall further behind, which causes you to simulate more steps\u0026hellip;\nSo how do we avoid this? In order to ensure a stable update I recommend leaving some headroom. You really need to ensure that it takes significantly less than X seconds of real time to update X seconds worth of physics simulation. If you can do this then your physics engine can \u0026ldquo;catch up\u0026rdquo; from any temporary spike by simulating more frames. Alternatively you can clamp at a maximum # of steps per-frame and the simulation will appear to slow down under heavy load. Arguably this is better than spiraling to death, especially if the heavy load is just a temporary spike.\nFree the physics Now let\u0026rsquo;s take it one step further. What if you want exact reproducibility from one run to the next given the same inputs? This comes in handy when trying to network your physics simulation using deterministic lockstep, but it\u0026rsquo;s also generally a nice thing to know that your simulation behaves exactly the same from one run to the next without any potential for different behavior depending on the render framerate.\nBut you ask why is it necessary to have fully fixed delta time to do this? Surely the semi-fixed delta time with the small remainder step is \u0026ldquo;good enough\u0026rdquo;? And yes, you are right. It is good enough in most cases but it is not exactly the same due to to the limited precision of floating point arithmetic.\nWhat we want then is the best of both worlds: a fixed delta time value for the simulation plus the ability to render at different framerates. These two things seem completely at odds, and they are - unless we can find a way to decouple the simulation and rendering framerates.\nHere\u0026rsquo;s how to do it. Advance the physics simulation ahead in fixed dt time steps while also making sure that it keeps up with the timer values coming from the renderer so that the simulation advances at the correct rate. For example, if the display framerate is 50fps and the simulation runs at 100fps then we need to take two physics steps every display update. Easy.\nWhat if the display framerate is 200fps? Well in this case it we need to take half a physics step each display update, but we can\u0026rsquo;t do that, we must advance with constant dt. So we take one physics step every two display updates.\nEven trickier, what if the display framerate is 60fps, but we want our simulation to run at 100fps? There is no easy multiple. What if VSYNC is disabled and the display frame rate fluctuates from frame to frame?\nIf you head just exploded don\u0026rsquo;t worry, all that is needed to solve this is to change your point of view. Instead of thinking that you have a certain amount of frame time you must simulate before rendering, flip your viewpoint upside down and think of it like this: the renderer produces time and the simulation consumes it in discrete dt sized steps.\nFor example:\ndouble t = 0.0; const double dt = 0.01; double currentTime = hires_time_in_seconds(); double accumulator = 0.0; while ( !quit ) { double newTime = hires_time_in_seconds(); double frameTime = newTime - currentTime; currentTime = newTime; accumulator += frameTime; while ( accumulator \u0026gt;= dt ) { integrate( state, t, dt ); accumulator -= dt; t += dt; } render( state ); } Notice that unlike the semi-fixed timestep we only ever integrate with steps sized dt so it follows that in the common case we have some unsimulated time left over at the end of each frame. This left over time is passed on to the next frame via the accumulator variable and is not thrown away.\nThe final touch But what do to with this remaining time? It seems incorrect doesn\u0026rsquo;t it?\nTo understand what is going on consider a situation where the display framerate is 60fps and the physics is running at 50fps. There is no nice multiple so the accumulator causes the simulation to alternate between mostly taking one and occasionally two physics steps per-frame when the remainders \u0026ldquo;accumulate\u0026rdquo; above dt.\nNow consider that the majority of render frames will have some small remainder of frame time left in the accumulator that cannot be simulated because it is less than dt. This means we\u0026rsquo;re displaying the state of the physics simulation at a time slightly different from the render time, causing a subtle but visually unpleasant stuttering of the physics simulation on the screen.\nOne solution to this problem is to interpolate between the previous and current physics state based on how much time is left in the accumulator:\ndouble t = 0.0; double dt = 0.01; double currentTime = hires_time_in_seconds(); double accumulator = 0.0; State previous; State current; while ( !quit ) { double newTime = time(); double frameTime = newTime - currentTime; if ( frameTime \u0026gt; 0.25 ) frameTime = 0.25; currentTime = newTime; accumulator += frameTime; while ( accumulator \u0026gt;= dt ) { previousState = currentState; integrate( currentState, t, dt ); t += dt; accumulator -= dt; } const double alpha = accumulator / dt; State state = currentState * alpha + previousState * ( 1.0 - alpha ); render( state ); } This looks complicated but here is a simple way to think about it. Any remainder in the accumulator is effectively a measure of just how much more time is required before another whole physics step can be taken. For example, a remainder of dt/2 means that we are currently halfway between the current physics step and the next. A remainder of dt*0.1 means that the update is 1/10th of the way between the current and the next state.\nWe can use this remainder value to get a blending factor between the previous and current physics state simply by dividing by dt. This gives an alpha value in the range [0,1] which is used to perform a linear interpolation between the two physics states to get the current state to render. This interpolation is easy to do for single values and for vector state values. You can even use it with full 3D rigid body dynamics if you store your orientation as a quaternion and use a spherical linear interpolation (slerp) to blend between the previous and current orientations.\nNEXT ARTICLE: Physics in 3D\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1086825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1086825600,"objectID":"3b603bc3a4332557db8f22fa16ce424f","permalink":"https://gafferongames.com/post/fix_your_timestep/","publishdate":"2004-06-10T00:00:00Z","relpermalink":"/post/fix_your_timestep/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIn the previous article we discussed how to integrate the equations of motion using a numerical integrator. Integration sounds complicated, but it\u0026rsquo;s just a way to advance the your physics simulation forward by some small amount of time called \u0026ldquo;delta time\u0026rdquo; (or dt for short).\nBut how to choose this delta time value? This may seem like a trivial subject but in fact there are many different ways to do it, each with their own strengths and weaknesses - so read on!","tags":["physics"],"title":"Fix Your Timestep!","type":"post"},{"authors":null,"categories":["Game Physics"],"content":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIf you have ever wondered how the physics simulation in a computer game works then this series of articles will explain it for you. I assume you are proficient with C++ and have a basic grasp of physics and mathematics. Nothing else will be required if you pay attention and study the example source code.\nA physics simulation works by making many small predictions based on the laws of physics. These predictions are actually quite simple, and basically boil down to something like \u0026ldquo;the object is here, and is traveling this fast in that direction, so in a short amount of time it should be over there\u0026rdquo;. We perform these predictions using a mathematical technique called integration.\nExactly how to implement this integration is the subject of this article.\nIntegrating the Equations of Motion You may remember from high school or university physics that force equals mass times acceleration.\nF = ma We can switch this around to see that acceleration is force divided by mass. This makes intuitive sense because heavier objects are harder to throw.\na = F/m Acceleration is the rate of change in velocity over time:\ndv/dt = a = F/m Similarly, velocity is the rate of change in position over time:\ndx/dt = v This means if we know the current position and velocity of an object, and the forces that will be applied to it, we can integrate to find its position and velocity at some point in the future.\nNumerical Integration For those who have not formally studied differential equations at university, take heart for you are in almost as good a position as those who have. This is because we\u0026rsquo;re not going to analytically solve the differential equations as you would do in first year mathematics. Instead, we are going to numerically integrate to find the solution.\nHere is how numerical integration works. First, start at an initial position and velocity, then take a small step forward to find the velocity and position at a future time. Then repeat this, moving forward in small time steps, using the result of the previous calculation as the starting point for the next.\nBut how do we find the change in velocity and position at each step?\nThe answer lies in the equations of motion.\nLet\u0026rsquo;s call our current time t, and the time step dt or \u0026lsquo;delta time\u0026rsquo;.\nWe can now put the equations of motion in a form that anyone can understand:\nacceleration = force / mass change in position = velocity * dt change in velocity = acceleration * dt This makes intuitive sense because if you\u0026rsquo;re in a car traveling 60 kilometers per-hour, in one hour you\u0026rsquo;ll be 60 kilometers further down the road. Similarly, a car accelerating 10 kilometers per-hour-per-second will be moving 100 kilometers per-hour faster after 10 seconds.\nOf course this logic only holds when acceleration and velocity are constant. But even when they\u0026rsquo;re not, it\u0026rsquo;s still a pretty decent approximation to start with.\nLet\u0026rsquo;s put this into code. Starting with a stationary object at the origin weighing one kilogram, we apply a constant force of 10 newtons and step forward with time steps of one second:\ndouble t = 0.0; float dt = 1.0f; float velocity = 0.0f; float position = 0.0f; float force = 10.0f; float mass = 1.0f; while ( t \u0026lt;= 10.0 ) { position = position + velocity * dt; velocity = velocity + ( force / mass ) * dt; t += dt; } Here is the result:\nt=0: position = 0 velocity = 0 t=1: position = 0 velocity = 10 t=2: position = 10 velocity = 20 t=3: position = 30 velocity = 30 t=4: position = 60 velocity = 40 t=5: position = 100 velocity = 50 t=6: position = 150 velocity = 60 t=7: position = 210 velocity = 70 t=8: position = 280 velocity = 80 t=9: position = 360 velocity = 90 t=10: position = 450 velocity = 100 As you can see, at at each step we know both the position and velocity of the object. This is numerical integration.\nExplicit Euler What we just did is a type of integration called explicit euler.\nTo save you future embarrassment, I must point out now that Euler is pronounced \u0026ldquo;Oiler\u0026rdquo; not \u0026ldquo;yew-ler\u0026rdquo; as it is the last name of the Swiss mathematician Leonhard Euler who first discovered this technique.\nEuler integration is the most basic numerical integration technique. It is only 100% accurate when the rate of change is constant over the timestep.\nSince acceleration is constant in the example above, the integration of velocity is without error. However, we are also integrating velocity to get position, and velocity is increasing due to acceleration. This means there is error in the integrated position.\nJust how large is this error? Let\u0026rsquo;s find out!\nThere is a closed form solution for how an object moves under constant acceleration. We can use this to compare our numerically integrated position with the exact result:\ns = ut + 0.5at^2 s = 0.0*t + 0.5at^2 s = 0.5(10)(10^2) s = 0.5(10)(100) s = 500 meters After 10 seconds, the object should have moved 500 meters, but explicit euler gives a result of 450 meters. That\u0026rsquo;s 50 meters off after just 10 seconds!\nThis sounds really, really bad, but it\u0026rsquo;s not common for games to step physics forward with such large time steps. In fact, physics usually steps forward at something closer to the display framerate.\nStepping forward with dt = 1/100 yields a much better result:\nt=9.90: position = 489.552155 velocity = 98.999062 t=9.91: position = 490.542145 velocity = 99.099060 t=9.92: position = 491.533142 velocity = 99.199059 t=9.93: position = 492.525146 velocity = 99.299057 t=9.94: position = 493.518127 velocity = 99.399055 t=9.95: position = 494.512115 velocity = 99.499054 t=9.96: position = 495.507111 velocity = 99.599052 t=9.97: position = 496.503113 velocity = 99.699051 t=9.98: position = 497.500092 velocity = 99.799049 t=9.99: position = 498.498077 velocity = 99.899048 t=10.00: position = 499.497070 velocity = 99.999046 As you can see, this is a pretty good result. Certainly good enough for a game.\nWhy explicit euler is not (always) so great With a small enough timestep explicit euler gives decent results for constant acceleration, but what about the case where acceleration isn\u0026rsquo;t constant?\nA good example of non-constant acceleration is a spring damper system.\nIn this system a mass is attached to a spring and its motion is damped by some kind of friction. There is a force proportional to the distance of the object that pulls it towards the origin, and a force proportional to the velocity of the object, but in the opposite direction, which slows it down.\nNow the acceleration is definitely not constant throughout the timestep, but is a continously changing function that is a combination of the position and velocity, which are themselves changing continuously over the timestep.\nThis is an example of a damped harmonic oscillator. It\u0026rsquo;s a well studied problem and there\u0026rsquo;s a closed form solution that we can use to check our numerically integrated result.\nLet\u0026rsquo;s start with an underdamped system where the mass oscillates about the origin while slowing down.\nHere are the input parameters to the mass spring system:\nMass: 1 kilogram Initial position: 1000 meters from origin Hooke\u0026rsquo;s law spring coefficient: k = 15 Hooke\u0026rsquo;s law damping coefficient: b = 0.1 And here is a graph of the exact solution:\nWhen we apply explicit euler to integrate this system, we get the following result, which has been scaled down vertically to fit:\nInstead of damping and converging on the origin, it gains energy over time!\nThis system is unstable when integrated with explicit euler and dt=1/100.\nUnfortunately, since we\u0026rsquo;re already integrating with a small timestep, we don\u0026rsquo;t have a lot of practical options to improve the accuracy. Even if you reduce the timestep, there\u0026rsquo;s always a spring tightness k above which you\u0026rsquo;ll see this behavior.\nSemi-implicit Euler Another integrator to consider is semi-implicit euler.\nMost commercial game physics engines use this integrator.\nSwitching from explicit to semi-implicit euler is as simple as changing:\nposition += velocity * dt; velocity += acceleration * dt; to:\nvelocity += acceleration * dt; position += velocity * dt; Applying the semi-implicit euler integrator with dt = 1/100 to the spring damper system gives a stable result that is very close to the exact solution:\nEven though semi-implicit euler has the same order of accuracy as explicit euler (order 1), we get a much better result when integrating the equations of motion because it is symplectic.\nMany different integration methods exist Implicit euler is an integration technique that is well suited for simulating stiff equations that become unstable with other methods. The drawback is that it requires solving a system of equations per-timestep.\nVerlet integration provides greater accuracy than implicit euler and less memory usage when simulating a large number of particles is. This is a second order integrator which is also symplectic.\nThere are a whole family of integrators called the Runge-Kutta methods. Explicit euler is part of this family, but it also includes higher order integrators, the most classic of these being the Runge Kutta order 4 or simply RK4.\nThis Runge Kutta family of integrators is named for the German physicists who discovered them: Carl Runge and Martin Kutta. This means the \u0026lsquo;g\u0026rsquo; is hard and the \u0026lsquo;u\u0026rsquo; is a short \u0026lsquo;oo\u0026rsquo; sound. I am sorry to inform but this means we are talking about the \u0026lsquo;roon-geh koo-ta\u0026rsquo; methods and not a \u0026lsquo;runge cutter\u0026rsquo;, whatever that is :)\nThe RK4 is a fourth order integrator, which means its accumulated error is on the order of the fourth derivative. This makes it very accurate. Much more accurate than explicit and implicit euler which are only first order.\nBut although it\u0026rsquo;s more accurate, that\u0026rsquo;s not to say RK4 is automatically \u0026ldquo;the best\u0026rdquo; integrator, or that it is better than semi-implicit euler. It\u0026rsquo;s much more complicated than this.\nRegardless, it\u0026rsquo;s an interesting integrator and is well worth studying.\nImplementing RK4 There are many great explanations of the mathematics behind RK4 already. For example: here, here and here. I highly encourage you to follow the derivation and understand how and why it works at a mathematical level. But, seeing as the target audience for this article are programmers, not mathematicians, we\u0026rsquo;re all about implementation, so let\u0026rsquo;s get started.\nLet\u0026rsquo;s define the state of an object as a struct in C++ so we have both position and velocity stored conveniently in one place:\nstruct State { float x; // position float v; // velocity }; We also need a struct to store the derivatives of the state values:\nstruct Derivative { float dx; // dx/dt = velocity float dv; // dv/dt = acceleration }; Next we need a function to advance the physics state ahead from t to t+dt using one set of derivatives, and once there, recalculate the derivatives at this new state:\nDerivative evaluate( const State \u0026amp; initial, double t, float dt, const Derivative \u0026amp; d ) { State state; state.x = initial.x + d.dx*dt; state.v = initial.v + d.dv*dt; Derivative output; output.dx = state.v; output.dv = acceleration( state, t+dt ); return output; } The acceleration function is what drives the entire simulation. Let\u0026rsquo;s set it to the spring damper system and return the acceleration assuming unit mass:\nfloat acceleration( const State \u0026amp; state, double t ) { const float k = 15.0f; const float b = 0.1f; return -k * state.x - b * state.v; } Now we get to the RK4 integration routine itself:\nvoid integrate( State \u0026amp; state, double t, float dt ) { Derivative a,b,c,d; a = evaluate( state, t, 0.0f, Derivative() ); b = evaluate( state, t, dt*0.5f, a ); c = evaluate( state, t, dt*0.5f, b ); d = evaluate( state, t, dt, c ); float dxdt = 1.0f / 6.0f * ( a.dx + 2.0f * ( b.dx + c.dx ) + d.dx ); float dvdt = 1.0f / 6.0f * ( a.dv + 2.0f * ( b.dv + c.dv ) + d.dv ); state.x = state.x + dxdt * dt; state.v = state.v + dvdt * dt; } The RK4 integrator samples the derivative at four points to detect curvative. Notice how derivative a is used when calculating b, b is used when calculating c, and c into d. This feedback of the current derivative into the calculation of the next is what gives the RK4 integrator its accuracy.\nImportantly, each of these derivatives a,b,c and d will be different when the rate of change in these quantities is a function of time or a function of the state itself. For example, in our spring damper system acceleration is a function of the current position and velocity which change throughout the timestep.\nOnce the four derivatives have been evaluated, the best overall derivative is calculated as a weighted sum derived from the taylor series expansion. This combined derivative is used to advance the position and velocity forward, just as we did with the explicit euler integrator.\nSemi-implicit euler vs. RK4 Now let\u0026rsquo;s put the RK4 integrator to the test.\nSince it is a higher order integrator (4th order vs. 1st order) it will be visibly more accurate than semi-implicit euler, right?\nWrong. Both integrators are so close to the exact result that it\u0026rsquo;s impossible to make out any difference at this scale. Both integrators are stable and track the exact solution very well with dt=1/100.\nZooming in confirms that RK4 is more accurate than semi-implicit euler, but is it really worth the complexity and extra runtime cost of RK4? It\u0026rsquo;s hard to say.\nLet\u0026rsquo;s push a bit harder and see if we can find a significant difference between the two integrators. Unfortunately, we can\u0026rsquo;t look at this system for long periods of time because it quickly damps down to zero, so let\u0026rsquo;s switch to a simple harmonic oscillator which oscillates forever without any damping.\nHere\u0026rsquo;s the exact result we\u0026rsquo;re aiming for:\nTo make it harder on the integrators, let\u0026rsquo;s increase delta time to 0.1 seconds.\nNext, we let the integrators run for 90 seconds and zoom in:\nAfter 90 seconds the semi-implicit euler solution (orange) has drifted out of phase with the exact solution because it has a slightly different frequency, while the green line of RK4 matches the frequency, but is losing energy!\nWe can see this more clearly by increasing the time step to 0.25 seconds.\nRK4 maintains the correct frequency but loses energy:\nWhile semi-implicit euler does a better job at conserving energy, on average:\nBut drifts out of phase. What an interesting result! As you can see it\u0026rsquo;s not simply the case that RK4 has a higher order of accuracy and is \u0026ldquo;better\u0026rdquo;. It\u0026rsquo;s much, much more nuanced than this.\nConclusion Which integrator should you use in your game?\nMy recommendation is semi-implicit euler. It\u0026rsquo;s cheap and easy to implement, it\u0026rsquo;s much more stable than explicit euler, and it tends to preserve energy on average even when pushed near its limit.\nIf you really do need more accuracy than semi-implicit euler, I recommend you look into higher order symplectic integrators designed for hamiltonian systems. This way you\u0026rsquo;ll discover more modern higher order integration techniques that are better suited to your simulation than RK4.\nAnd finally, if you are still doing this in your game:\nposition += velocity * dt; velocity += acceleration * dt; Please take a moment to change it to this:\nvelocity += acceleration * dt; position += velocity * dt; You\u0026rsquo;ll be glad you did :)\nNEXT ARTICLE: Fix Your Timestep!\nGlenn Fiedler is the founder and CEO of Network Next.\nNetwork Next is fixing the internet for games by creating a marketplace for premium network transit.\n","date":1086048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1086048000,"objectID":"a31bd41719ec72340ffa5708721e14a8","permalink":"https://gafferongames.com/post/integration_basics/","publishdate":"2004-06-01T00:00:00Z","relpermalink":"/post/integration_basics/","section":"post","summary":"Introduction Hi, I\u0026rsquo;m Glenn Fiedler and welcome to Game Physics.\nIf you have ever wondered how the physics simulation in a computer game works then this series of articles will explain it for you. I assume you are proficient with C++ and have a basic grasp of physics and mathematics. Nothing else will be required if you pay attention and study the example source code.\nA physics simulation works by making many small predictions based on the laws of physics.","tags":["physics"],"title":"Integration Basics","type":"post"}]